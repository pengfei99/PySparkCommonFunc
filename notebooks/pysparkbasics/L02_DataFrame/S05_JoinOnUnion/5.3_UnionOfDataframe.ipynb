{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Union of data frame\n",
    "\n",
    "We have seen how to join two dataframe which has common columns. The join operation merge the columns of dataframe. Now if we have \n",
    "two or more data frames which has the same (or almost the same) schema or structure, how do we merge them (meger rows)? We call the operation that merge dataframe by rows **Union of data frames**.\n",
    "\n",
    "In spark, we have two functions:\n",
    "- union(other): It's called by a data frame, it takes another data frame as argument. It returns a new dataframe which\n",
    "                is the union of the two.\n",
    "- unionByName(other, allowMissingColumns). Idem to union, but since spark 3.1. A new argument allowMissingColumns which\n",
    "                takes a bool value has been added. This allows us to merger data frame with different column numbers.\n",
    "\n",
    "The difference between the two transformations is that \n",
    "- union() resolve column by its position. \n",
    "- unionByName() resolve column by its name. \n",
    "\n",
    "In exp1, \n",
    "In exp3, we tested on different column type, union works. How the output data frame choose column type is unclear. \n",
    "Note there is another transformation called unionAll() which is deprecated since Spark “2.0.0” version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"UnionDataFrame\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "              .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "              .appName(\"UnionDataFrame\") \\\n",
    "              .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "              .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "              .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "              .config(\"spark.executor.instances\", \"4\") \\\n",
    "              .config(\"spark.executor.memory\",\"8g\") \\\n",
    "              .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1') \\\n",
    "              .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dataframes below (e.g. df1, df2). They have almost the same schema, only one column name is different (name vs employee_name). We will try to union df1 and df2 with different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data 1: row number 4\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|name   |department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|James  |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael|Sales     |NY   |86000 |56 |20000|\n",
      "|Robert |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria  |Finance   |CA   |90000 |24 |23000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "             (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "             (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "             (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000)\n",
    "             ]\n",
    "\n",
    "columns1 = [\"name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=columns1)\n",
    "print(\"Source data 1: row number {}\".format(df1.count()))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data 2: row number 5\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [(\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "             (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "             (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "             (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "             (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "             ]\n",
    "columns2 = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df2 = spark.createDataFrame(data=data2, schema=columns2)\n",
    "print(\"Source data 2: row number {}\".format(df2.count()))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 Use union() function \n",
    "\n",
    "In below example, we use the union() function to union the two dataframes that has the same column number. As it unions dataframe by using the position of the column, the column name variation will not cause error. \n",
    "\n",
    "Inner working of spark union(): **spark analyze the column number and type. If those are identical, the difference between column names are ommitted.**\n",
    "\n",
    "Another important note, the union() function just merge the two dataframe without dealing with duplicates.\n",
    "\n",
    "So you need to use distinct() or df.drop_duplicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+\n",
      "|   name|department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note we use df1 as base table, and union df2\n",
    "# The order of the rows in df1, and df2 does not change.\n",
    "df_union=df1.union(df2)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+\n",
      "|   name|department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can notice, after dropDuplicates we do not have duplicates anymore  \n",
    "df_union.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 Some bad examples\n",
    "\n",
    "We have seen a successful union, what happens if we want to union two data frames that have different column numbers?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "|   name|department|state|salary|age|bonus|        msg|\n",
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|Hello_world|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|Hello_world|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|Hello_world|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|Hello_world|\n",
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df1.withColumn(\"msg\",lit(\"Hello_world\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 6 columns;\n'Union false, false\n:- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, Hello_world AS msg#158]\n:  +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n+- Project [name#0 AS name#195, department#1 AS department#196, state#2 AS state#197, salary#3L AS salary#198L, age#4L AS age#199L, bonus#5L AS bonus#200L]\n   +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f279c79e6b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# off course it failed. And the error message is very clear, because two dataframe has different schema(e.g. column number)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \"\"\"\n\u001b[0;32m-> 1828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 6 columns;\n'Union false, false\n:- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, Hello_world AS msg#158]\n:  +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n+- Project [name#0 AS name#195, department#1 AS department#196, state#2 AS state#197, salary#3L AS salary#198L, age#4L AS age#199L, bonus#5L AS bonus#200L]\n   +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n"
     ]
    }
   ],
   "source": [
    "# off course it failed. And the error message is very clear, because two dataframe has different schema(e.g. column number)\n",
    "\n",
    "df3.union(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time if the column number are the same, but the column type are different. Will the union be successful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we cast column age from long type to string type\n",
    "df4=df1.withColumn(\"age\",df1.age.cast(\"string\"))\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+\n",
      "|   name|department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bad_union1=df4.union(df2)\n",
    "df_bad_union1.show()\n",
    "df_bad_union1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above union has successed**\n",
    "Second point, we can notice that **the column type of the result is string**. Is this caused by the spark inner working or just because df4 is the base table of the union? \n",
    "\n",
    "So we change the union base table and try again. The result column type of age is still string. So we can say it's the inner working of spark, which cast long to string automaticlly when there is type difference in the two dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bad_union2=df2.union(df4)\n",
    "df_bad_union2.show()\n",
    "df_bad_union2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 Use unionByName to union dataframe\n",
    "\n",
    "We have seen how to union two dataframes by using their column position. We can also union them by using the column names. In below example, we use unionByName() to union two data frame which has different column number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+------+---+-----+\n",
      "| name|department|state|salary|age|bonus|\n",
      "+-----+----------+-----+------+---+-----+\n",
      "|James|     Sales|   NY| 90000| 34|10000|\n",
      "|Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|  Jen|   Finance|   NY| 79000| 53|15000|\n",
      "| Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-----+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 6 columns and the second table has 7 columns;\n'Union false, false\n:- Project [employee_name#48 AS name#308, department#49, state#50, salary#51L, age#52L, bonus#53L]\n:  +- LogicalRDD [employee_name#48, department#49, state#50, salary#51L, age#52L, bonus#53L], false\n+- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, msg#96]\n   +- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, Hello_world AS msg#96]\n      +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4ec63304631a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_renamed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"employee_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_renamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munionByName\u001b[0;34m(self, other, allowMissingColumns)\u001b[0m\n\u001b[1;32m   1883\u001b[0m            \u001b[0mmissing\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \"\"\"\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowMissingColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 6 columns and the second table has 7 columns;\n'Union false, false\n:- Project [employee_name#48 AS name#308, department#49, state#50, salary#51L, age#52L, bonus#53L]\n:  +- LogicalRDD [employee_name#48, department#49, state#50, salary#51L, age#52L, bonus#53L], false\n+- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, msg#96]\n   +- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, Hello_world AS msg#96]\n      +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n"
     ]
    }
   ],
   "source": [
    "df_renamed=df2.withColumnRenamed(\"employee_name\",\"name\")\n",
    "df_renamed.show()\n",
    "# note below union failed. Because df_renamed has 6 column and df3 has 7 column.\n",
    "df_renamed.unionByName(df3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the union work, we need to use the option **allowMissingColumns=True**, the absent columns will be filled with null after union.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "|   name|department|state|salary|age|bonus|        msg|\n",
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|       null|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|       null|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|       null|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|       null|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|       null|\n",
      "|  James|     Sales|   NY| 90000| 34|10000|Hello_world|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|Hello_world|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|Hello_world|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|Hello_world|\n",
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can notice the row in df_rename has a new column msg filled with null value.\n",
    "# If you are using a spark version prior to 3.1. You don't have access of allowMissingColumns=True. You need\n",
    "# to creat the absent column and filled it with null by yourself.\n",
    "df_renamed.unionByName(df3,allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"name\" among (employee_name, department, state, salary, age, bonus)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-83660f84dd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This time let's try the same column number but different column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munionByName\u001b[0;34m(self, other, allowMissingColumns)\u001b[0m\n\u001b[1;32m   1883\u001b[0m            \u001b[0mmissing\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \"\"\"\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowMissingColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"name\" among (employee_name, department, state, salary, age, bonus)"
     ]
    }
   ],
   "source": [
    "# This time let's try the same column number but different column names\n",
    "# You can notice the message error says it cannot find column \"name\" in df2.\n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4 Last check on the difference of the union and unionByName\n",
    "\n",
    "This time we will use two dataframe has the same column number and type, but the order of the last two columns are different. \n",
    "Then we use union and unionByName to union the two dataframe and check the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds all columns of df2 which does not exist in df1, then create then in df1 and fill it with None\n",
    "def create_missing_column(df1, df2):\n",
    "    for column in [column for column in df2.columns if column not in df1.columns]:\n",
    "        df1 = df1.withColumn(column, lit(None))\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+-----+----+\n",
      "|   name|department|state|salary|age|bonus|  msg|mail|\n",
      "+-------+----------+-----+------+---+-----+-----+----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|hello|null|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|hello|null|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|hello|null|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|hello|null|\n",
      "+-------+----------+-----+------+---+-----+-----+----+\n",
      "\n",
      "+-----+----------+-----+------+---+-----+-----+----+\n",
      "| name|department|state|salary|age|bonus| mail| msg|\n",
      "+-----+----------+-----+------+---+-----+-----+----+\n",
      "|James|     Sales|   NY| 90000| 34|10000|world|null|\n",
      "|Maria|   Finance|   CA| 90000| 24|23000|world|null|\n",
      "|  Jen|   Finance|   NY| 79000| 53|15000|world|null|\n",
      "| Jeff| Marketing|   CA| 80000| 25|18000|world|null|\n",
      "|Kumar| Marketing|   NY| 91000| 50|21000|world|null|\n",
      "+-----+----------+-----+------+---+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_msg has all the common columns and one extra column \"msg\"\n",
    "df_msg = df1.withColumn(\"msg\", lit(\"hello\"))\n",
    "    \n",
    "# df_mail has all the common columns and one extra column \"mail\"\n",
    "df_mail = df2.withColumn(\"mail\", lit(\"world\")).withColumnRenamed(\"employee_name\", \"name\")\n",
    "\n",
    "# we need to create column \"mail\" in df_msg, and column \"msg\" in df_mail\n",
    "df_msg_after_fill = create_missing_column(df_msg, df_mail)\n",
    "df_msg_after_fill.show()\n",
    "\n",
    "df_mail_after_fill = create_missing_column(df_mail, df_msg)\n",
    "df_mail_after_fill.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+-----+----+\n",
      "|   name|department|state|salary|age|bonus|  msg|mail|\n",
      "+-------+----------+-----+------+---+-----+-----+----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|hello|null|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|hello|null|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|hello|null|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|hello|null|\n",
      "|  James|     Sales|   NY| 90000| 34|10000|world|null|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|world|null|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|world|null|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|world|null|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|world|null|\n",
      "+-------+----------+-----+------+---+-----+-----+----+\n",
      "\n",
      "+-------+----------+-----+------+---+-----+-----+-----+\n",
      "|   name|department|state|salary|age|bonus|  msg| mail|\n",
      "+-------+----------+-----+------+---+-----+-----+-----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|hello| null|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|hello| null|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|hello| null|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|hello| null|\n",
      "|  James|     Sales|   NY| 90000| 34|10000| null|world|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000| null|world|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000| null|world|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000| null|world|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000| null|world|\n",
      "+-------+----------+-----+------+---+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# after the fill, we can do the unionByName or union\n",
    "\n",
    "# Check the different result of union and unionByName, it confirms the union resolve column by positon, unionByName\n",
    "# resolve column by name\n",
    "union1_df = df_msg_after_fill.union(df_mail_after_fill)\n",
    "union1_df.show()\n",
    "\n",
    "union2_df = df_msg_after_fill.unionByName(df_mail_after_fill)\n",
    "union2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
