{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Union of data frame\n",
    "\n",
    "We have seen how to join two dataframe which has common columns. The join operation merge the columns of dataframe. Now if we have \n",
    "two or more data frames which has the same (or almost the same) schema or structure, how do we merge them (meger rows)? We call the operation that merge dataframe by rows **Union of data frames**.\n",
    "\n",
    "In spark, we have two functions:\n",
    "- union(other): It's called by a data frame, it takes another data frame as argument. It returns a new dataframe which\n",
    "                is the union of the two.\n",
    "- unionByName(other, allowMissingColumns). Idem to union, but since spark 3.1. A new argument allowMissingColumns which\n",
    "                takes a bool value has been added. This allows us to merger data frame with different column numbers.\n",
    "\n",
    "The difference between the two transformations is that \n",
    "- union() resolve column by its position. \n",
    "- unionByName() resolve column by its name. \n",
    "\n",
    "In exp1, \n",
    "In exp3, we tested on different column type, union works. How the output data frame choose column type is unclear. \n",
    "Note there is another transformation called unionAll() which is deprecated since Spark “2.0.0” version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"UnionDataFrame\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "              .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "              .appName(\"UnionDataFrame\") \\\n",
    "              .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "              .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "              .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "              .config(\"spark.executor.instances\", \"4\") \\\n",
    "              .config(\"spark.executor.memory\",\"8g\") \\\n",
    "              .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1') \\\n",
    "              .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dataframes below (e.g. df1, df2). They have almost the same schema, only one column name is different (name vs employee_name). We will try to union df1 and df2 with different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data 1: row number 4\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|name   |department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|James  |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael|Sales     |NY   |86000 |56 |20000|\n",
      "|Robert |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria  |Finance   |CA   |90000 |24 |23000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "             (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "             (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "             (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000)\n",
    "             ]\n",
    "\n",
    "columns1 = [\"name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=columns1)\n",
    "print(\"Source data 1: row number {}\".format(df1.count()))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data 2: row number 5\n",
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = [(\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "             (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "             (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "             (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "             (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "             ]\n",
    "columns2 = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df2 = spark.createDataFrame(data=data2, schema=columns2)\n",
    "print(\"Source data 2: row number {}\".format(df2.count()))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 Use union() function \n",
    "\n",
    "In below example, we use the union() function to union the two dataframes that has the same column number. As it unions dataframe by using the position of the column, the column name variation will not cause error. \n",
    "\n",
    "Inner working of spark union(): **spark analyze the column number and type. If those are identical, the difference between column names are ommitted.**\n",
    "\n",
    "Another important note, the union() function just merge the two dataframe without dealing with duplicates.\n",
    "\n",
    "So you need to use distinct() or df.drop_duplicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+\n",
      "|   name|department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note we use df1 as base table, and union df2\n",
    "# The order of the rows in df1, and df2 does not change.\n",
    "df_union=df1.union(df2)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+\n",
      "|   name|department|state|salary|age|bonus|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "|  Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|  James|     Sales|   NY| 90000| 34|10000|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|    Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|   Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "+-------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can notice, after dropDuplicates we do not have duplicates anymore  \n",
    "df_union.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 Some bad examples\n",
    "\n",
    "We have seen a successful union, what happens if we want to union two data frames that have different column numbers?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "|   name|department|state|salary|age|bonus|        msg|\n",
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "|  James|     Sales|   NY| 90000| 34|10000|Hello_world|\n",
      "|Michael|     Sales|   NY| 86000| 56|20000|Hello_world|\n",
      "| Robert|     Sales|   CA| 81000| 30|23000|Hello_world|\n",
      "|  Maria|   Finance|   CA| 90000| 24|23000|Hello_world|\n",
      "+-------+----------+-----+------+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df1.withColumn(\"msg\",lit(\"Hello_world\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 6 columns;\n'Union false, false\n:- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, Hello_world AS msg#195]\n:  +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n+- Project [name#0 AS name#232, department#1 AS department#233, state#2 AS state#234, salary#3L AS salary#235L, age#4L AS age#236L, bonus#5L AS bonus#237L]\n   +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ddb16033f193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \"\"\"\n\u001b[0;32m-> 1828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 6 columns;\n'Union false, false\n:- Project [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L, Hello_world AS msg#195]\n:  +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n+- Project [name#0 AS name#232, department#1 AS department#233, state#2 AS state#234, salary#3L AS salary#235L, age#4L AS age#236L, bonus#5L AS bonus#237L]\n   +- LogicalRDD [name#0, department#1, state#2, salary#3L, age#4L, bonus#5L], false\n"
     ]
    }
   ],
   "source": [
    "# off course it failed. And the error message is very clear, because two dataframe has different schema(e.g. column number)\n",
    "\n",
    "df3.union(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time if the column number are the same, but the column type are different. Will the union be successful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we cast column age from long type to string type\n",
    "df4=df1.withColumn(\"age\",df1.age.cast(\"string\"))\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad_union=df4.union(df2)\n",
    "df_bad_union.show()\n",
    "df_bad_union.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above union has successed**\n",
    "Second point, we can notice that **the column type of the result is string** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
