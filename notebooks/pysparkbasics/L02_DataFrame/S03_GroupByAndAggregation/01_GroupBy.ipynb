{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Spark GroupBy\n",
    "\n",
    "Similar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into\n",
    "groups on DataFrame and perform aggregate functions on the grouped data.\n",
    "roupBy(col1 : scala.Predef.String, cols : scala.Predef.String*) :\n",
    "      org.apache.spark.sql.RelationalGroupedDataset\n",
    "Note that, it can take one or more column names and returns GroupedData object which can use below aggregation functions.\n",
    "- count() - Returns the count of rows for each group.\n",
    "- mean() - Returns the mean of values for each group.\n",
    "- max() - Returns the maximum of values for each group.\n",
    "- min() - Returns the minimum of values for each group.\n",
    "- sum() - Returns the total for values for each group.\n",
    "- avg() - Returns the average for values for each group.\n",
    "- agg() - Using agg() function, we can calculate more than one aggregate at a time.\n",
    "- pivot() - This function is used to Pivot the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"pySparkGroupBy\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"SparkArrowCompression\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "            (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "            (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "            (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "            (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "            (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "            (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "            (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "            (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "            ]\n",
    "\n",
    "schema = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Groupby single column \n",
    "\n",
    "In this example, we groupby a single column and aggregate single/multi column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby department and count the rows in each grouped department. If each row represent a distinct employee, then we have the count of employee\n",
    "# of each department\n",
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+\n",
      "|state|avg(salary)|avg(age)|\n",
      "+-----+-----------+--------+\n",
      "|   CA|    87500.0|   29.75|\n",
      "|   NY|    85800.0|    45.8|\n",
      "+-----+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the average salary and age of each state\n",
    "df.groupBy(\"state\").avg(\"salary\",\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 GroupBy multiple columns\n",
    "We have seen how to groupBy one column. We can also groupBy multiple columns.\n",
    "\n",
    "Suppose we do a groupBy on two column A, B. If A has m distinct value, and B has n distinct value. Then after groupby, we will have m*n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|max(salary)|max(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|   Finance|   NY|      83000|     19000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "|   Finance|   CA|      99000|     24000|\n",
      "|     Sales|   NY|      90000|     20000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this example, we groupBy rows by their department(Sales, Finance, Marketing) and state(CA, NY)\n",
    "# we will have 6 possible combination, for each combination we will use the aggregation function to calculate the max.\n",
    "df.groupBy(\"department\", \"state\").max(\"salary\", \"bonus\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Apply multiple aggregation function on one groupBy \n",
    "\n",
    "The default min, max, can only show one type of stats. If you want to show multiple column with different stats, you \n",
    "need to use agg().  \n",
    "\n",
    "Note here, avg and mean returned the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------------+-----------------+\n",
      "|department|min_salary|max_salary|       avg_salary|      mean_salary|\n",
      "+----------+----------+----------+-----------------+-----------------+\n",
      "|     Sales|     81000|     90000|85666.66666666667|85666.66666666667|\n",
      "|   Finance|     79000|     99000|          87750.0|          87750.0|\n",
      "| Marketing|     80000|     91000|          85500.0|          85500.0|\n",
      "+----------+----------+----------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(\n",
    "  f.min(\"salary\").alias(\"min_salary\"), \\\n",
    "  f.max(\"salary\").alias(\"max_salary\"), \\\n",
    "  f.avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "  f.mean(\"salary\").alias(\"mean_salary\")  \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use other aggregation functions inside agg, the full list of predefine aggregation function is here \n",
    "https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/ \n",
    "\n",
    "Below shows collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\").agg(f.collect_list())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
