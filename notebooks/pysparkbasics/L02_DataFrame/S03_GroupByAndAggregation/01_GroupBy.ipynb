{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Spark GroupBy\n",
    "\n",
    "Similar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into\n",
    "groups on DataFrame and perform aggregate functions on the grouped data.\n",
    "roupBy(col1 : scala.Predef.String, cols : scala.Predef.String*) :\n",
    "      org.apache.spark.sql.RelationalGroupedDataset\n",
    "Note that, it can take one or more column names and returns GroupedData object which can use below aggregation functions.\n",
    "- count() - Returns the count of rows for each group.\n",
    "- mean() - Returns the mean of values for each group.\n",
    "- max() - Returns the maximum of values for each group.\n",
    "- min() - Returns the minimum of values for each group.\n",
    "- sum() - Returns the total for values for each group.\n",
    "- avg() - Returns the average for values for each group.\n",
    "- agg() - Using agg() function, we can calculate more than one aggregate at a time.\n",
    "- pivot() - This function is used to Pivot the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"pySparkGroupBy\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"SparkArrowCompression\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "            (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "            (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "            (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "            (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "            (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "            (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "            (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "            (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "            ]\n",
    "\n",
    "schema = [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Groupby single column \n",
    "\n",
    "In this example, we groupby a single column and aggregate single/multi column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby department and count the rows in each grouped department. If each row represent a distinct employee, then we have the count of employee\n",
    "# of each department\n",
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+\n",
      "|state|avg(salary)|avg(age)|\n",
      "+-----+-----------+--------+\n",
      "|   CA|    87500.0|   29.75|\n",
      "|   NY|    85800.0|    45.8|\n",
      "+-----+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the average salary and age of each state\n",
    "df.groupBy(\"state\").avg(\"salary\",\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 GroupBy multiple columns\n",
    "We have seen how to groupBy one column. We can also groupBy multiple columns.\n",
    "\n",
    "Suppose we do a groupBy on two column A, B. If A has m distinct value, and B has n distinct value. Then after groupby, we will have m*n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|max(salary)|max(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|   Finance|   NY|      83000|     19000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "|   Finance|   CA|      99000|     24000|\n",
      "|     Sales|   NY|      90000|     20000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this example, we groupBy rows by their department(Sales, Finance, Marketing) and state(CA, NY)\n",
    "# we will have 6 possible combination, for each combination we will use the aggregation function to calculate the max.\n",
    "df.groupBy(\"department\", \"state\").max(\"salary\", \"bonus\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Apply multiple aggregation function on one groupBy \n",
    "\n",
    "The default min, max, can only show one type of stats. If you want to show multiple column with different stats, you \n",
    "need to use agg().  \n",
    "\n",
    "Note here, avg and mean returned the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------------+-----------------+\n",
      "|department|min_salary|max_salary|       avg_salary|      mean_salary|\n",
      "+----------+----------+----------+-----------------+-----------------+\n",
      "|     Sales|     81000|     90000|85666.66666666667|85666.66666666667|\n",
      "|   Finance|     79000|     99000|          87750.0|          87750.0|\n",
      "| Marketing|     80000|     91000|          85500.0|          85500.0|\n",
      "+----------+----------+----------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(\n",
    "  f.min(\"salary\").alias(\"min_salary\"), \\\n",
    "  f.max(\"salary\").alias(\"max_salary\"), \\\n",
    "  f.avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "  f.mean(\"salary\").alias(\"mean_salary\")  \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use other aggregation functions inside agg, the full list of predefine aggregation function is here \n",
    "https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/ \n",
    "\n",
    "Below shows collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------+\n",
      "|department|collect_list(employee_name)|\n",
      "+----------+---------------------------+\n",
      "|Sales     |[James, Michael, Robert]   |\n",
      "|Finance   |[Maria, Raman, Scott, Jen] |\n",
      "|Marketing |[Jeff, Kumar]              |\n",
      "+----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(f.collect_list(\"employee_name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.1.4 Cube\n",
    "\n",
    "We have seen in a GROUP BY, every row is included only once in its corresponding summary.\n",
    "\n",
    "With GROUP BY CUBE(..) every row is included in summary of each combination of levels it represents, wildcards included. Logically, the shown above is equivalent to something like this (assuming we could use NULL placeholders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|      null|    9|\n",
      "| Marketing|    2|\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the difference with groupBy(\"department\")\n",
    "# as we cube only one column, so the possible combination is 3, plus wildcards. So we have 4 rows in total. \n",
    "# The wildcards represnet the all departments\n",
    "\n",
    "df.cube(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count: 12\n",
      "+----------+-----+-----+\n",
      "|department|state|count|\n",
      "+----------+-----+-----+\n",
      "|   Finance| null|    4|\n",
      "| Marketing| null|    2|\n",
      "|   Finance|   CA|    2|\n",
      "|   Finance|   NY|    2|\n",
      "|     Sales|   CA|    1|\n",
      "|     Sales|   NY|    2|\n",
      "|      null| null|    9|\n",
      "|      null|   NY|    5|\n",
      "| Marketing|   CA|    1|\n",
      "|      null|   CA|    4|\n",
      "| Marketing|   NY|    1|\n",
      "|     Sales| null|    3|\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we cube two columns department(3 distinc value + wildcard),state(2 distinc value + wildcard), we should have 4*3 (12) row in total  \n",
    "df1=df.cube(\"department\",\"state\").count()\n",
    "print(f\"Total row count: {df1.count()}\")\n",
    "# (Finance null) means Finance department of all state. \n",
    "# (null   NY) means all department of NY state\n",
    "#(null null) means all department and all state\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.1.5 Rollup\n",
    "rollup computes hierarchical subtotals from left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count 10\n",
      "+----------+-----+-----+\n",
      "|department|state|count|\n",
      "+----------+-----+-----+\n",
      "|   Finance| null|    4|\n",
      "| Marketing| null|    2|\n",
      "|   Finance|   CA|    2|\n",
      "|   Finance|   NY|    2|\n",
      "|     Sales|   CA|    1|\n",
      "|     Sales|   NY|    2|\n",
      "|      null| null|    9|\n",
      "| Marketing|   CA|    1|\n",
      "| Marketing|   NY|    1|\n",
      "|     Sales| null|    3|\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We rollup(\"department\",\"state\"). The first column department has 3 distinct value, \n",
    "# the seconde column department has 2 distinct value. For the second column, we need to add wildcard\n",
    "# so we have 3*3+global wildcart=10 rows in the rollup result.\n",
    "# Similar to cube, \n",
    "# (Finance null) means Finance department of all state. \n",
    "# (null null) means all department and all state\n",
    "\n",
    "# You can notice we don't have rows like  (null   NY) anymore. Because the order of input columns play as hierarchy.\n",
    "\n",
    "df2=df.rollup(\"department\", \"state\").count()\n",
    "print(f\"Total row count {df2.count()}\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
