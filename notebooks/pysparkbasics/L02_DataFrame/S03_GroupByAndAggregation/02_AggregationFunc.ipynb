{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Aggregation functions\n",
    "\n",
    "**Aggregate functions** operate on a group of rows and calculate a single return value for every group.\n",
    "\n",
    "In previous GroupBy section, we have seen some aggregation example. In this section, we will examine all existing aggregation functions in spark. The full list in alphabetic order:\n",
    "\n",
    "- approx_count_distinct, count, countDistinct\n",
    "- avg/mean, max, min, \n",
    "- collect_list, collect_set\n",
    "- grouping : Check if a column is created by aggregation function or not, returns 1 for aggregated, 0 for not aggregated\n",
    "- first,last\n",
    "- sum, sumDistinct\n",
    "- kurtosis, skewness\n",
    "- stddev, stddev_samp, stddev_pop\n",
    "- variance, var_samp, var_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import count, countDistinct, approx_count_distinct, avg, min, max, mean, collect_list, \\\n",
    "    collect_set, grouping, first, last, sum, sumDistinct, skewness, kurtosis, stddev, stddev_samp, stddev_pop, \\\n",
    "    variance, var_samp, var_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"pySparkGroupBy\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"SparkArrowCompression\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------+----------+------+\n",
      "|   name|department|salary|\n",
      "+-------+----------+------+\n",
      "|  Alice|     Sales|  3000|\n",
      "|Michael|     Sales|  4600|\n",
      "| Robert|        IT|  4100|\n",
      "|  Maria|   Finance|  3000|\n",
      "|   Haha|        IT|  3000|\n",
      "|  Scott|   Finance|  3300|\n",
      "|    Jen|   Finance|  3900|\n",
      "|   Jeff| Marketing|  3000|\n",
      "|  Kumar| Marketing|  2000|\n",
      "|   Haha|     Sales|  4100|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", \"Sales\", 3000),\n",
    "            (\"Michael\", \"Sales\", 4600),\n",
    "            (\"Robert\", \"IT\", 4100),\n",
    "            (\"Maria\", \"Finance\", 3000),\n",
    "            (\"Haha\", \"IT\", 3000),\n",
    "            (\"Scott\", \"Finance\", 3300),\n",
    "            (\"Jen\", \"Finance\", 3900),\n",
    "            (\"Jeff\", \"Marketing\", 3000),\n",
    "            (\"Kumar\", \"Marketing\", 2000),\n",
    "            (\"Haha\", \"Sales\", 4100)\n",
    "            ]\n",
    "schema = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Count, countDistinct, approx_count_distinct\n",
    "The classic count() will just count row numbers of a group.\n",
    "\n",
    "The **approx_count_distinct** is implemented to avoid count(distinct()) operations. The approx_count_distinct uses an algorithm called **HyperLogLog**. This algorithm can estimate the number of distinct values of greater than 1,000,000,000, where the accuracy of the calculated approximate distinct count value is within 2% of the actual distinct count value. It can do this while using much less memory.\n",
    "\n",
    "Because **count(distinct())** requires more and more memory as the number of distinct values increases. \n",
    "\n",
    "This tutorial shows how the approx_count_distinct function is implemented\n",
    "https://mungingdata.com/apache-spark/hyperloglog-count-distinct/#:~:text=approx_count_distinct%20uses%20the%20HyperLogLog%20algorithm,count()%20will%20run%20slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(salary)|\n",
      "+-------------+\n",
      "|           10|\n",
      "+-------------+\n",
      "\n",
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    3|\n",
      "| Marketing|    2|\n",
      "|        IT|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation function can be used after groupBy or on the whole data frame.\n",
    "df.select(count(\"salary\")).show()\n",
    "\n",
    "# below is\n",
    "df.groupBy(\"department\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT salary)|\n",
      "+----------------------+\n",
      "|                     6|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|                                10|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# countDistinct can take multiple column as argument. If input column number is greater than 1, then\n",
    "# the value combination (col1, col2, ...) must be distinct.\n",
    "\n",
    "# salary distinct value is 6, but (department, salary) distinct value is 10\n",
    "df.select(countDistinct(\"salary\")).show()\n",
    "df.select(countDistinct(\"department\", \"salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|approx_count_distinct(salary)|\n",
      "+-----------------------------+\n",
      "|                            6|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# approx count uses less resources\n",
    "df.select(approx_count_distinct(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Get basic stats of a column by using avg/mean, min, max\n",
    "\n",
    "Note mean is the alias of avg. It's not the median function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     3400.0|\n",
      "+-----------+\n",
      "\n",
      "+---------+\n",
      "|avg(name)|\n",
      "+---------+\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show avg example\n",
    "# avg can only apply on digit column, if type is mismatch, it returns null\n",
    "df.select(avg(\"salary\")).show()\n",
    "df.select(avg(\"name\")).show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     3400.0|\n",
      "+-----------+\n",
      "\n",
      "+---------+\n",
      "|avg(name)|\n",
      "+---------+\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show mean example\n",
    "# mean is the alias of avg. so it works like avg. It's not the median.\n",
    "df.select(mean(\"salary\")).show()\n",
    "df.select(mean(\"name\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|       2000|\n",
      "+-----------+\n",
      "\n",
      "+---------+\n",
      "|min(name)|\n",
      "+---------+\n",
      "|    Alice|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show min example\n",
    "# min can apply on digit and string column, it uses default sorting order (ascending) to find min\n",
    "df.select(min(\"salary\")).show()\n",
    "df.select(min(\"name\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|       4600|\n",
      "+-----------+\n",
      "\n",
      "+---------+\n",
      "|max(name)|\n",
      "+---------+\n",
      "|    Scott|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show max example\n",
    "# max can apply on digit and string column, it uses default sorting order (ascending) to find max\n",
    "df.select(max(\"salary\")).show()\n",
    "df.select(max(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 collect_list, collect_set \n",
    "\n",
    "- collect_list() function returns a list that contains all values from an input column with duplicates.\n",
    "- collect_set() function returns a list that contains all values from an input column without duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n",
      "+----------+----------------------+\n",
      "|department|collect_list(name)    |\n",
      "+----------+----------------------+\n",
      "|Sales     |[Alice, Michael, Haha]|\n",
      "|Finance   |[Maria, Scott, Jen]   |\n",
      "|Marketing |[Jeff, Kumar]         |\n",
      "|IT        |[Robert, Haha]        |\n",
      "+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show collect_list example\n",
    "\n",
    "# collect the salary of employee, note we have duplicates\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "# collect the name of each department\n",
    "# note collect_list can't be used directly after groupBy, it must be in agg()\n",
    "df.groupBy(\"department\").agg(collect_list(\"name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show collect_set example\n",
    "# note we dont have duplicates\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 Groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupBy(\"department\").avg(\"salary\")\n",
    "df1.show()\n",
    "# can't make grouping works \n",
    "# df1.select(grouping(\"avg(salary)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.5 \n",
    "\n",
    "- first(colName): it returns the first element of a group in a column. When ignoreNulls is set to true, it returns the first non-null element.\n",
    "- last(colName): it returns the last element of a group in a column. When ignoreNulls is set to true, it returns the last non-null element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|first(salary)|last(name)|\n",
      "+-------------+----------+\n",
      "|         3000|      Haha|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the first and last of all rows\n",
    "df.select(first(\"salary\"),last(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+\n",
      "|department|first(salary)|last(name)|\n",
      "+----------+-------------+----------+\n",
      "|     Sales|         3000|      Haha|\n",
      "|   Finance|         3000|       Jen|\n",
      "| Marketing|         3000|     Kumar|\n",
      "|        IT|         4100|      Haha|\n",
      "+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the first, last of each sub group\n",
    "df.groupBy(\"department\").agg(first(\"salary\"),last(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.6 sum and sumDistinct\n",
    "- sum(colName): returns the sum of all values in a column.\n",
    "- sumDistinct(colName): returns the sum of all distinct values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|salary_sum|salary_distinct_sum|\n",
      "+----------+-------------------+\n",
      "|     34000|              20900|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the sum and sumDistinct of each group\n",
    "df.select(sum(\"salary\").alias(\"salary_sum\"),sumDistinct(\"salary\").alias(\"salary_distinct_sum\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+-------------------+\n",
      "|department|collect_list(salary)|salary_sum|salary_distinct_sum|\n",
      "+----------+--------------------+----------+-------------------+\n",
      "|     Sales|  [4600, 4100, 3000]|     11700|              11700|\n",
      "|   Finance|  [3900, 3000, 3300]|     10200|              10200|\n",
      "| Marketing|        [2000, 3000]|      5000|               5000|\n",
      "|        IT|        [3000, 4100]|      7100|               7100|\n",
      "+----------+--------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(collect_list(\"salary\"),sum(\"salary\").alias(\"salary_sum\"),sumDistinct(\"salary\").alias(\"salary_distinct_sum\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.7 skewness, kurtosis\n",
    "\n",
    "When we do descriptive analysis, we want to know the skewness of a distribution. If a distribution is not skewed, which\n",
    "is a normal distribution, then we want to know the Kurtosis(峰度) of the distribution.\n",
    "\n",
    "Central tendency\n",
    "This is nothing but Mean, its the average value of a distribution. to calculate the central tendency we can use Imputer or Spark SQL's stats function.\n",
    "\n",
    "Dispersion\n",
    "This Nothing but Variance,Is a measure that how far the data set is spread out, So calculate the Central tendency and dispersion refer this tutorial.\n",
    "\n",
    "- skewness(colName): Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric \n",
    "if it looks the same to the left and right of the centre point. \n",
    "\n",
    "- kurtosis(colName): Kurtosis measures whether your dataset is heavy-tailed or light-tailed compared to a normal distribution. \n",
    "Data sets with high kurtosis have heavy tails and more outliers and data sets with low kurtosis tend to have \n",
    "light tails and fewer outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|    skewness(salary)|   kurtosis(salary)|\n",
      "+--------------------+-------------------+\n",
      "|-0.12041791181069564|-0.6467803030303032|\n",
      "+--------------------+-------------------+\n",
      "\n",
      "+----------+--------------------+-------------------+----------------+\n",
      "|department|collect_list(salary)|   skewness(salary)|kurtosis(salary)|\n",
      "+----------+--------------------+-------------------+----------------+\n",
      "|     Sales|  [3000, 4600, 4100]|-0.4220804429649461|            -1.5|\n",
      "|   Finance|  [3000, 3300, 3900]|0.38180177416060623|            -1.5|\n",
      "| Marketing|        [3000, 2000]|                0.0|            -2.0|\n",
      "|        IT|        [4100, 3000]|                0.0|            -2.0|\n",
      "+----------+--------------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if the skew value is positive, it means, the distribution is skew to the right \n",
    "df.select(skewness(\"salary\"), kurtosis(\"salary\")).show()\n",
    "df.groupBy(\"department\").agg(collect_list(\"salary\"), skewness(\"salary\"),kurtosis(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|  20| 100|\n",
      "|  50| 100|\n",
      "| 100| 100|\n",
      "| 100| 100|\n",
      "| 100| 100|\n",
      "| 100| 100|\n",
      "| 100| 150|\n",
      "| 100| 180|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below example, col1 is skew to the left, col2 is skew to right\n",
    "data2 = [(20,100),(50,100),(100,100),(100,100),(100,100),(100,100),(100, 150),(100,180)]\n",
    "df_skew= spark.createDataFrame(data=data2, schema=[\"col1\",\"col2\"])\n",
    "df_skew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     skewness(col1)|     kurtosis(col1)|\n",
      "+-------------------+-------------------+\n",
      "|-1.3746740084540479|0.16603074794216655|\n",
      "+-------------------+-------------------+\n",
      "\n",
      "+------------------+-----------------+\n",
      "|    skewness(col2)|   kurtosis(col2)|\n",
      "+------------------+-----------------+\n",
      "|1.3746740084540479|0.166030747942167|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col1 skew to the left, returns negative skewness value\n",
    "df_skew.select(skewness(\"col1\"), kurtosis(\"col1\")).show()\n",
    "\n",
    "# col2 skew to the right, returns positive skewness value\n",
    "df_skew.select(skewness(\"col2\"), kurtosis(\"col2\")).show()\n",
    "\n",
    "# you can notice they have the same kurtorsis value, it means the distance between tail and median is the same. They are both not heavy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 100| 100|\n",
      "| 100| 100|\n",
      "| 100| 110|\n",
      "| 100| 120|\n",
      "| 100| 130|\n",
      "| 100| 140|\n",
      "| 100| 150|\n",
      "| 110| 180|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data3 = [(100,100),(100,100),(100,110),(100,120),(100,130),(100,140),(100, 150),(110,180)]\n",
    "df_kurt= spark.createDataFrame(data=data3, schema=[\"col1\",\"col2\"])\n",
    "df_kurt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|    skewness(col1)|    kurtosis(col1)|\n",
      "+------------------+------------------+\n",
      "|2.2677868380553634|3.1428571428571432|\n",
      "+------------------+------------------+\n",
      "\n",
      "+------------------+-------------------+\n",
      "|    skewness(col2)|     kurtosis(col2)|\n",
      "+------------------+-------------------+\n",
      "|0.6682892518272334|-0.5349496168871446|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col1 skew to the right, returns positive skewness value, is not heavily tailed, so kurt is positive\n",
    "df_kurt.select(skewness(\"col1\"), kurtosis(\"col1\")).show()\n",
    "\n",
    "# col2 skew to the right, returns positive skewness value. is heavily tailed, so kurt is negative\n",
    "df_kurt.select(skewness(\"col2\"), kurtosis(\"col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.8 Standard deviation\n",
    "\n",
    "Standard deviation is a measure of the amount of variation or dispersion of a set of values.\n",
    "- A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) \n",
    "     of the set\n",
    "- A high standard deviation indicates that the values are spread out over a wider range.\n",
    "\n",
    "Spark provides three methods:\n",
    "- stddev(): alias for stddev_samp.\n",
    "- stddev_samp(): returns the unbiased sample standard deviation of the expression in a group.\n",
    "- stddev_pop(): returns population standard deviation of the expression in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|  765.9416862050705|  765.9416862050705|  726.636084983398|\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), stddev_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+------------------+\n",
      "|department|collect_list(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+----------+--------------------+-------------------+------------------+\n",
      "|     Sales|  [3000, 4600, 4100]|   818.535277187245| 668.3312551921141|\n",
      "|   Finance|  [3000, 3300, 3900]|   458.257569495584|374.16573867739413|\n",
      "| Marketing|        [3000, 2000]|  707.1067811865476|             500.0|\n",
      "|        IT|        [4100, 3000]|  777.8174593052023|             550.0|\n",
      "+----------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(collect_list(\"salary\"), stddev(\"salary\"),stddev_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.9 Variance\n",
    "\n",
    "Variance is also a measure of the amount of variation or dispersion of a set of values. The difference with standard\n",
    "deviation is that square functions are used during the calculation because they weight outliers more heavily than \n",
    "points that are near to the mean. This prevents that differences above the mean neutralize those below the mean.\n",
    "\n",
    "But the square function makes the Variance change the unit of measurement of the original data. For example, a column\n",
    "contains centimeter values. Your variance would be in squared centimeters and therefore not the best measurement. \n",
    "\n",
    "Spark provides three methods:\n",
    "- variance(): alias for var_samp.\n",
    "- var_samp(): returns the unbiased sample variance of the values in a group.\n",
    "- var_pop(): returns the population variance of the values in a group. \n",
    "\n",
    "High variance value means data are spreaded to a wider range.\n",
    "Low variance value means data are close to mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+---------------+\n",
      "| var_samp(salary)| var_samp(salary)|var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|       528000.0|\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(variance(\"salary\"), var_samp(\"salary\"), var_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------+-----------------+\n",
      "|department|collect_list(salary)|var_samp(salary)|  var_pop(salary)|\n",
      "+----------+--------------------+----------------+-----------------+\n",
      "|     Sales|  [3000, 4600, 4100]|        670000.0|446666.6666666667|\n",
      "|   Finance|  [3000, 3300, 3900]|        210000.0|         140000.0|\n",
      "| Marketing|        [3000, 2000]|        500000.0|         250000.0|\n",
      "|        IT|        [4100, 3000]|        605000.0|         302500.0|\n",
      "+----------+--------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(collect_list(\"salary\"), variance(\"salary\"),var_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
