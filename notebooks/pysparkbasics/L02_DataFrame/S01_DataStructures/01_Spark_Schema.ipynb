{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2.1 Spark Schema\n",
    "\n",
    "`Spark schema` is the structure of the DataFrame or Dataset, we can define it using StructType class which is a collection of StructField that define the **column name(String), column type (DataType), nullable column (Boolean) and metadata (MetaData)**\n",
    "\n",
    "The column type can be primitive type that provided by spark. For more details, please check all available data type [list](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/13 01:48:42 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/02/13 01:48:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/13 01:48:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .appName(\"Dataframe_schema\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"Dataframe_schema\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config(\"spark.kubernetes.driver.pod.name\", os.environ[\"POD_NAME\"]) \\\n",
    "                      .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') \\\n",
    "                      .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data=[(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Bn\",\"\",\"F\",-1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.1 Define a simple schema\n",
    "\n",
    "In below example, we define a simple schema by using `StructType` which contains a list of `StructField`. In StructField, we define the column name(String), column type (DataType), nullable column (Boolean) and `metadata (MetaData) (only exist in scala implementation, not for python)`\n",
    "\n",
    "Below is an example how to define metadata in StructField in scala\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.types.MetadataBuilder\n",
    "val metadata = new MetadataBuilder()\n",
    "  .putString(\"comment\", \"this is a comment\")\n",
    "  .build\n",
    "import org.apache.spark.sql.types.{LongType, StructField}\n",
    "val f = new StructField(name = \"id\", dataType = LongType, nullable = false, metadata)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"firstname\",StringType(),True),\n",
    "    StructField(\"middlename\",StringType(),True),\n",
    "    StructField(\"lastname\",StringType(),True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|      Bn|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema of a dataframe\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.2 Define a nested structure schema\n",
    "\n",
    "Spark DataFrame often need to work with the nested struct columns. In below example, we group column first_name, middle_name, last_name, and create a nested column name that contains three fields.\n",
    "\n",
    "For defining `nested_struct_data`, we group first_name, middle_name, last_name as a new **Row called name inside the main Row employee**\n",
    "\n",
    "For defining `nested_schema`, we define first a StructType name, then in the main schema, we create a StructField that has type name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "nested_struct_data=[\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Bn\"),\"\",\"F\",-1)    \n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# first we build the nested structType name\n",
    "name=StructType([\n",
    "    StructField(\"firstname\",StringType(),True),\n",
    "    StructField(\"middlename\",StringType(),True),\n",
    "    StructField(\"lastname\",StringType(),True)])\n",
    "\n",
    "# we define column name has nested structType\n",
    "nested_schema=StructType([\n",
    "    StructField(\"name\",name,False),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    {James, , Smith}|36636|     M|  3100|\n",
      "|   {Michael, Rose, }|40288|     M|  4300|\n",
      "|{Robert, , Williams}|42114|     M|  1400|\n",
      "|{Maria, Anne, Jones}|39192|     F|  5500|\n",
      "|     {Jen, Mary, Bn}|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_df=spark.createDataFrame(data=nested_struct_data,schema=nested_schema)\n",
    "nested_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.3 Write schema to json file\n",
    "\n",
    "In above examples, we use StructType and StructField to define schema by hand, if the dataframe has many columns, this can be time-consuming to do so. We can write/read schema of a dataframe by using json.\n",
    "\n",
    "Let's start with how to export a schema. We have seen in previous sections, we can print the schema by using printSchema() method. If we want to get StructType, We can use Dataframe.schema.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.StructType'>\n"
     ]
    }
   ],
   "source": [
    "out_schema=nested_df.schema\n",
    "print(type(out_schema))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice the type of out_schema is `pyspark.sql.types.StructType`. We can also get column list(list of StructField) by using Dataframe.schema.fields"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "out_fields=nested_df.schema.fields\n",
    "print(type(out_fields))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField(name,StructType(List(StructField(firstname,StringType,true),StructField(middlename,StringType,true),StructField(lastname,StringType,true))),false)\n",
      "StructField(id,StringType,true)\n",
      "StructField(gender,StringType,true)\n",
      "StructField(salary,IntegerType,true)\n"
     ]
    }
   ],
   "source": [
    "for field in out_fields:\n",
    "    print(field)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have all the information we need, we need to convert the schema(StructType) to json format. And pyspark already provide this function. In below example, we\n",
    "get the schema in json string, then write it in a json file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"name\",\"nullable\":false,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"firstname\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"middlename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lastname\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"gender\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"salary\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "json_schema=nested_df.schema.json()\n",
    "print(json_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "json_file_path=\"data/schema.json\"\n",
    "with open(json_file_path, \"w\") as f:\n",
    "   f.write(json_schema)\n",
    "   f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.4 Read schema from a json file\n",
    "\n",
    "In previous section we output a schema into json file, now let's build a schema based on the json file.\n",
    "\n",
    "First step, we read the json string from the json file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"fields\": [\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"name\",\n",
      "      \"nullable\": false,\n",
      "      \"type\": {\n",
      "        \"fields\": [\n",
      "          {\n",
      "            \"metadata\": {},\n",
      "            \"name\": \"firstname\",\n",
      "            \"nullable\": true,\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          {\n",
      "            \"metadata\": {},\n",
      "            \"name\": \"middlename\",\n",
      "            \"nullable\": true,\n",
      "            \"type\": \"string\"\n",
      "          },\n",
      "          {\n",
      "            \"metadata\": {},\n",
      "            \"name\": \"lastname\",\n",
      "            \"nullable\": true,\n",
      "            \"type\": \"string\"\n",
      "          }\n",
      "        ],\n",
      "        \"type\": \"struct\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"id\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"gender\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    {\n",
      "      \"metadata\": {},\n",
      "      \"name\": \"salary\",\n",
      "      \"nullable\": true,\n",
      "      \"type\": \"integer\"\n",
      "    }\n",
      "  ],\n",
      "  \"type\": \"struct\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(json_file_path,\"r\") as f:\n",
    "    json_string_schema=f.read()\n",
    "\n",
    "print(json_string_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# second step, we convert the json string to json object\n",
    "loaded_json_schema=json.loads(json_string_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# third step, we build a schema by using the json object\n",
    "loaded_json_schema=StructType.fromJson(loaded_json_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(name,StructType(List(StructField(firstname,StringType,true),StructField(middlename,StringType,true),StructField(lastname,StringType,true))),false),StructField(id,StringType,true),StructField(gender,StringType,true),StructField(salary,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(loaded_json_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|                name|   id|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    {James, , Smith}|36636|     M|  3100|\n",
      "|   {Michael, Rose, }|40288|     M|  4300|\n",
      "|{Robert, , Williams}|42114|     M|  1400|\n",
      "|{Maria, Anne, Jones}|39192|     F|  5500|\n",
      "|     {Jen, Mary, Bn}|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_json_schema=spark.createDataFrame(data=nested_struct_data,schema=loaded_json_schema)\n",
    "df_with_json_schema.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_json_schema.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1.5 Write schema to DDL String\n",
    "\n",
    "Spark also provides api to export/import schema by using DDL string. But pyspark does not yet implement this feature yet. For scala api, check the below code example\n",
    "\n",
    "```scala\n",
    "\n",
    "import sparkSession.implicits._\n",
    "\n",
    "// generate ddl string from a schema\n",
    "val ddlSchemaStr =nested_df.schema.toDDL()\n",
    "// It should return the below string\n",
    "// \"`fullName` STRUCT<`first`: STRING, `last`: STRING, `middle`: STRING>,`age` INT,`gender` STRING\"\n",
    "\n",
    "// generate a schema from ddl string\n",
    "val ddlSchema = StructType.fromDDL(ddlSchemaStr)\n",
    "ddlSchema.printTreeString()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'StructType' has no attribute 'fromDDL'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3385/2294634333.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mddlSchemaStr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"`fullName` STRUCT<`first`: STRING, `last`: STRING, `middle`: STRING>,`age` INT,`gender` STRING\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mddlSchema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromDDL\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mddlSchemaStr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mddlSchema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintTreeString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: type object 'StructType' has no attribute 'fromDDL'"
     ]
    }
   ],
   "source": [
    "ddlSchemaStr = \"`fullName` STRUCT<`first`: STRING, `last`: STRING, `middle`: STRING>,`age` INT,`gender` STRING\"\n",
    "# StructType only provide fromJson(), fromDDL() is not implemented\n",
    "ddlSchema = StructType.fromDDL(ddlSchemaStr)\n",
    "ddlSchema.printTreeString()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}