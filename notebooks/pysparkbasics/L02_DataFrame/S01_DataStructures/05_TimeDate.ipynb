{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Date and time in Spark data frames\n",
    "\n",
    "Date and time column are essential for time series data. In spark, we can express date and time with **four different data types**: \n",
    "- String : Most commun format, no compability issues. But need to be parsed to long or timestamp types to do arithmetic operations\n",
    "- Long : in spark, it represents **unix_timestamp** implementation. it uses seconds since the UNIX epoch\n",
    "- timestamp :  spark has its own implementation to represent timestamp. Represents values comprising values of fields year, month, day, hour, minute, and second, with the **session local time-zone**. The timestamp value represents an absolute point in time.\n",
    "- date: Represents values comprising values of fields year, month and day, without a time-zone.\n",
    "\n",
    "Below, we will show how to do convertion between them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit, unix_timestamp, col, from_unixtime, to_timestamp, date_format, to_date,current_date\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/22 14:36:59 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/07/22 14:36:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/22 14:36:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"DateAndTime\").getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "data = [(\"2019-01-01 12:01:19\", \"01-01-2019 12:01:19.888\", \"01-01-2019\"),\n",
    "            (\"2018-07-01 12:01:19\", \"07-01-2018 12:01:19.666\", \"07-01-2018\"),\n",
    "            (\"2017-12-01 12:01:19\", \"12-01-2017 12:01:19.111\", \"12-01-2017\")]\n",
    "columns = [\"timestamp_1\", \"timestamp_2\", \"timestamp_3\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+\n",
      "|        timestamp_1|         timestamp_2|timestamp_3|\n",
      "+-------------------+--------------------+-----------+\n",
      "|2019-01-01 12:01:19|01-01-2019 12:01:...| 01-01-2019|\n",
      "|2018-07-01 12:01:19|07-01-2018 12:01:...| 07-01-2018|\n",
      "|2017-12-01 12:01:19|12-01-2017 12:01:...| 12-01-2017|\n",
      "+-------------------+--------------------+-----------+\n",
      "\n",
      "root\n",
      " |-- timestamp_1: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Convert string date, timestamp to long (seconds since the UNIX epoch)\n",
    "\n",
    "To convert string to long, we can use unix_timestamp() function, it takes a string date, and a date format, if the date format is not provided, the default format (yyyy-MM-dd HH:mm:ss) will be used. Then the string date are converted to Unix timestamp (in seconds) by using the current timezone of the system. \n",
    "\n",
    "Note you can set the timezone of your spark session by using **spark.conf.set(\"spark.sql.session.timeZone\", \"<time_zone>\")**, for example spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\").\n",
    "\n",
    "1: def unix_timestamp(): returns the current time in seconds (LongType)\n",
    "2) def unix_timestamp(date: Column): take the string column date as input, output it in seconds, it will use default date format (yyyy-MM-dd HH:mm:ss) to parse the string. \n",
    "3) def unix_timestamp(date: Column, format: String): we can specify explicitly the format of the date string, if the default format does not fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_3: string (nullable = true)\n",
      " |-- unix_timestamp: long (nullable = true)\n",
      "\n",
      "+-------------------+--------------------+-----------+--------------+\n",
      "|        timestamp_1|         timestamp_2|timestamp_3|unix_timestamp|\n",
      "+-------------------+--------------------+-----------+--------------+\n",
      "|2019-07-01 12:01:19|07-01-2019 12:01:...| 07-01-2019|    1658493752|\n",
      "|2018-07-01 12:01:19|07-01-2018 12:01:...| 07-01-2018|    1658493752|\n",
      "|2017-07-01 12:01:19|07-01-2017 12:01:...| 07-01-2017|    1658493752|\n",
      "+-------------------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We add a new column that is filled with the current time\n",
    "# note the column type\n",
    "df1 = df.withColumn(\"unix_timestamp\", lit(unix_timestamp()))\n",
    "df1.printSchema()\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp1: covert date string to seconds with some erreurs\n",
      "root\n",
      " |-- timestamp_1: long (nullable = true)\n",
      " |-- timestamp_2: long (nullable = true)\n",
      " |-- timestamp_3: long (nullable = true)\n",
      "\n",
      "+-----------+-----------+-----------+\n",
      "|timestamp_1|timestamp_2|timestamp_3|\n",
      "+-----------+-----------+-----------+\n",
      "| 1561975279| 1561975279|       null|\n",
      "| 1530439279| 1530439279|       null|\n",
      "| 1498903279| 1498903279|       null|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note the output dataframe has timestamp 1 and timestamp 2 correct. But timestamp 3 wrong. \n",
    "# we did not give format for timestamp_1 and timestamp_3, it works for timestamp_1 because it uses the default format is yyyy-MM-dd HH:mm:ss\n",
    "# timestamp_3 does not work, because it does not use the default date format.\n",
    "# for timestamp_2, we specify a date format, if we don't give the format, it returns null, because the string does not have\n",
    "# the default date format\n",
    "# If we give a format that is wrong, the spark job will fail. For example, try to remove SSS from timestamp_2 format, and see what happens\n",
    "\n",
    "df2 = df.withColumn(\"timestamp_1\", unix_timestamp(\"timestamp_1\")) \\\n",
    "        .withColumn(\"timestamp_2\", unix_timestamp(\"timestamp_2\", \"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
    "        .withColumn(\"timestamp_3\", unix_timestamp(\"timestamp_3\"))\n",
    "print(\"Exp1: covert date string to seconds with some erreurs\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp1: covert date string to seconds with success\n",
      "root\n",
      " |-- timestamp_1: long (nullable = true)\n",
      " |-- timestamp_2: long (nullable = true)\n",
      " |-- timestamp_3: long (nullable = true)\n",
      " |-- timestamp_4: long (nullable = true)\n",
      "\n",
      "+-----------+-----------+-----------+-----------+\n",
      "|timestamp_1|timestamp_2|timestamp_3|timestamp_4|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "| 1561975279| 1561975279| 1561932000| 1658493780|\n",
      "| 1530439279| 1530439279| 1530396000| 1658493780|\n",
      "| 1498903279| 1498903279| 1498860000| 1658493780|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# below, we give each one a correct date format.\n",
    "df3 = df.withColumn(\"timestamp_1\", unix_timestamp(\"timestamp_1\")) \\\n",
    "        .withColumn(\"timestamp_2\", unix_timestamp(\"timestamp_2\", \"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
    "        .withColumn(\"timestamp_3\", unix_timestamp(\"timestamp_3\", \"MM-dd-yyyy\")) \\\n",
    "        .withColumn(\"timestamp_4\", lit(unix_timestamp()))\n",
    "print(\"Exp1: covert date string to seconds with success\")\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Convert long(seconds since the UNIX epoch) back to string date\n",
    "\n",
    "We can use from_unixtime(unix_time: Column, format: String) to convert long back to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_3: string (nullable = true)\n",
      " |-- timestamp_4: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-----------+-------------------+\n",
      "|timestamp_1        |timestamp_2        |timestamp_3|timestamp_4        |\n",
      "+-------------------+-------------------+-----------+-------------------+\n",
      "|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |2022-07-22 14:43:12|\n",
      "|2018-07-01 12:01:19|07-01-2018 12:01:19|07-01-2018 |2022-07-22 14:43:12|\n",
      "|2017-07-01 12:01:19|07-01-2017 12:01:19|07-01-2017 |2022-07-22 14:43:12|\n",
      "+-------------------+-------------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert unix timestamp in second to string date\n",
    "df4 = df3.select(\n",
    "        from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n",
    "        from_unixtime(col(\"timestamp_2\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n",
    "        from_unixtime(col(\"timestamp_3\"), \"MM-dd-yyyy\").alias(\"timestamp_3\"),\n",
    "        from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\")\n",
    "    )\n",
    "df4.printSchema()\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Convert string to spark timestamp\n",
    "\n",
    "Spark provides its own timestamp type. Instead of long, we can also use spark timestamp to store time data.\n",
    "\n",
    "To convert a string date to spark timestamp, we can use:\n",
    "- to_timestamp(col, format): the default format is yyyy-MM-dd HH:mm:ss. If it failed, return null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: timestamp (nullable = true)\n",
      " |-- timestamp_2: timestamp (nullable = true)\n",
      " |-- timestamp_3: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+-----------------------+-------------------+\n",
      "|timestamp_1        |timestamp_2            |timestamp_3        |\n",
      "+-------------------+-----------------------+-------------------+\n",
      "|2019-07-01 12:01:19|2019-07-01 12:01:19.888|2019-07-01 00:00:00|\n",
      "|2018-07-01 12:01:19|2018-07-01 12:01:19.666|2018-07-01 00:00:00|\n",
      "|2017-07-01 12:01:19|2017-07-01 12:01:19.111|2017-07-01 00:00:00|\n",
      "+-------------------+-----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_timestamp() works similar to unix_timestamp(). For timestamp_1, it has the default format, so no need to specify the format\n",
    "# For 2 and 3, we need to specify the date format. Otherwise, it returns null\n",
    "\n",
    "# note the output column type\n",
    "\n",
    "df5 = df.withColumn(\"timestamp_1\", to_timestamp(\"timestamp_1\")) \\\n",
    "        .withColumn(\"timestamp_2\", to_timestamp(\"timestamp_2\", \"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
    "        .withColumn(\"timestamp_3\", to_timestamp(\"timestamp_3\", \"MM-dd-yyyy\"))\n",
    "\n",
    "df5.printSchema()\n",
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also use to_timestamp in a sql \n",
    "spark.sql(\"select to_timestamp('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Convert spark timestamp back to string \n",
    "\n",
    "To convert timestamp back to string, we have two options \n",
    "- cast(\"String\"): It converts a timestamp column to default date format yyyy-MM-dd HH:mm:ss\n",
    "- date_format(column,format): It converts a timestamp column to any Java Date formats specified in DateTimeFormatter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Use cast(\"String\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----------------------+\n",
      "|timestamp_2            |timestamp_2            |\n",
      "+-----------------------+-----------------------+\n",
      "|2019-07-01 12:01:19.888|2019-07-01 12:01:19.888|\n",
      "|2018-07-01 12:01:19.666|2018-07-01 12:01:19.666|\n",
      "|2017-07-01 12:01:19.111|2017-07-01 12:01:19.111|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert spark timestamp back to string by using cast()\n",
    "# note they all use the default date format.\n",
    "df6 = df5.select(col(\"timestamp_2\").cast(\"string\"), \\\n",
    "                col(\"timestamp_2\").cast(\"string\"))\n",
    "df6.printSchema()\n",
    "df6.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Use date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: timestamp (nullable = true)\n",
      " |-- timestamp_2: timestamp (nullable = true)\n",
      " |-- timestamp_3: timestamp (nullable = true)\n",
      " |-- str_date_yyyy_MM_dd: string (nullable = true)\n",
      " |-- str_date_MM/dd/yyyy_hh:mm: string (nullable = true)\n",
      " |-- str_date_yyyy_MMM_dd: string (nullable = true)\n",
      " |-- str_date_yyyy_MMMM_dd_E: string (nullable = true)\n",
      "\n",
      "+-------------------+-----------------------+-------------------+-------------------+-------------------------+--------------------+-----------------------+\n",
      "|timestamp_1        |timestamp_2            |timestamp_3        |str_date_yyyy_MM_dd|str_date_MM/dd/yyyy_hh:mm|str_date_yyyy_MMM_dd|str_date_yyyy_MMMM_dd_E|\n",
      "+-------------------+-----------------------+-------------------+-------------------+-------------------------+--------------------+-----------------------+\n",
      "|2019-07-01 12:01:19|2019-07-01 12:01:19.888|2019-07-01 00:00:00|2019 07 01         |07/01/2019 12:01         |2019 Jul 01         |2019 July 01 Mon       |\n",
      "|2018-07-01 12:01:19|2018-07-01 12:01:19.666|2018-07-01 00:00:00|2018 07 01         |07/01/2018 12:01         |2018 Jul 01         |2018 July 01 Sun       |\n",
      "|2017-07-01 12:01:19|2017-07-01 12:01:19.111|2017-07-01 00:00:00|2017 07 01         |07/01/2017 12:01         |2017 Jul 01         |2017 July 01 Sat       |\n",
      "+-------------------+-----------------------+-------------------+-------------------+-------------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use date_format() to convert spark timestamp to various string date\n",
    "df7 = df5.withColumn(\"str_date_yyyy_MM_dd\", date_format(col(\"timestamp_2\"), \"yyyy MM dd\")) \\\n",
    "        .withColumn(\"str_date_MM/dd/yyyy_hh:mm\", date_format(col(\"timestamp_2\"), \"MM/dd/yyyy hh:mm\")) \\\n",
    "        .withColumn(\"str_date_yyyy_MMM_dd\", date_format(col(\"timestamp_2\"), \"yyyy MMM dd\")) \\\n",
    "        .withColumn(\"str_date_yyyy_MMMM_dd_E\", date_format(col(\"timestamp_2\"), \"yyyy MMMM dd E\"))\n",
    "    \n",
    "df7.printSchema()\n",
    "df7.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Convert string to date type\n",
    "\n",
    "We can use `to_date()` function to convert string to date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|     input|      date|\n",
      "+----------+----------+\n",
      "|02-03-2013|2013-02-03|\n",
      "|05-06-2023|2023-05-06|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1=[[\"02-03-2013\"], \n",
    "       [\"05-06-2023\"]]\n",
    "columns1=[\"input\"]\n",
    "df8 = spark.createDataFrame(data=data1,schema=columns1 )\n",
    "df9 = df8.withColumn(\"date\", to_date(\"input\", \"MM-dd-yyyy\"))\n",
    "\n",
    "df9.printSchema()\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.6 Spark Date functions\n",
    "\n",
    "The operation you can call on date in `string` or `timestamp` type are quite limited. But with **Date** type, you can call many prebuilt operations. However, you lose the time precision (hh:mm:ss).\n",
    "\n",
    "Below is a list of prebuilt functions:\n",
    "\n",
    "- date_format(date, format):\tConverts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.\n",
    "- to_date(col): Convert string type containing date value to date format.\n",
    "- current_date(): Returns the current date as a date column.\n",
    "\n",
    "- year(col): Extract the year of a given date as integer.\n",
    "- quarter(col): Extract the quarter of a given date as integer.\n",
    "- month(col): Extract the month of a given date as integer.\n",
    "- hour(col): Extract the hours of a given date as integer.\n",
    "- minute(col): Extract the minutes of a given date as integer.\n",
    "- second(col): Extract the seconds of a given date as integer.\n",
    "- dayofyear(col): Extract the day of the year of a given date as integer.\n",
    "- dayofmonth(col): Extract the day of the month of a given date as integer.\n",
    "- dayofweek(col): Extract the day of the week of a given date as integer.\n",
    "- weekofyear(col): Extract the week number of a given date as integer.\n",
    "\n",
    "\n",
    "\n",
    "- date_add(start, days): Add days to the date.\n",
    "- add_months(start, months): Add months to date.\n",
    "- datediff(end, start):\tReturns difference between two dates in days.\n",
    "- date_sub(start, days): Subtract the days from date field.\n",
    "- last_day(date): Returns the last day of the month which the given date belongs to.\n",
    "- months_between(date1, date2): Returns number of months between two dates.\n",
    "- next_day(date, dayOfWeek): Returns the first date which is later than the value of the date column.\n",
    "- trunc(date, format): Returns date truncated to the unit specified by the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "date_func_df=df.select(\"timestamp_1\")\\\n",
    "    .withColumn(\"raw_date\",to_date(\"timestamp_1\",\"yyyy-MM-dd hh:mm:ss\"))\\\n",
    "    .withColumn(\"current_date\", current_date())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------+\n",
      "|        timestamp_1|  raw_date|current_date|\n",
      "+-------------------+----------+------------+\n",
      "|2019-01-01 12:01:19|2019-01-01|  2022-07-22|\n",
      "|2018-07-01 12:01:19|2018-07-01|  2022-07-22|\n",
      "|2017-12-01 12:01:19|2017-12-01|  2022-07-22|\n",
      "+-------------------+----------+------------+\n",
      "\n",
      "root\n",
      " |-- timestamp_1: string (nullable = true)\n",
      " |-- raw_date: date (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_func_df.show()\n",
    "date_func_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,quarter,month,hour,minute,second,dayofyear,dayofmonth,dayofweek,weekofyear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.6.1 Date extraction\n",
    "\n",
    "In feature engineering phase, we often need to extract certain part of the date (e.g. year, month, day). Because date is too complex for model to understand.\n",
    "\n",
    "We have below functions to do extraction:\n",
    "- year(col): Extract the year of a given date as integer.\n",
    "- quarter(col): Extract the quarter of a given date as integer.\n",
    "- month(col): Extract the month of a given date as integer.\n",
    "- hour(col): Extract the hours of a given date as integer.\n",
    "- minute(col): Extract the minutes of a given date as integer.\n",
    "- second(col): Extract the seconds of a given date as integer.\n",
    "\n",
    "- dayofyear(col): Extract the day of the year of a given date as integer.\n",
    "- dayofmonth(col): Extract the day of the month of a given date as integer.\n",
    "- dayofweek(col): Extract the day of the week of a given date as integer.\n",
    "- weekofyear(col): Extract the week number of a given date as integer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# define target date column name\n",
    "target_col_name=\"raw_date\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|  raw_date|year(raw_date)|\n",
      "+----------+--------------+\n",
      "|2019-01-01|          2019|\n",
      "|2018-07-01|          2018|\n",
      "|2017-12-01|          2017|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the year\n",
    "\n",
    "date_func_df.select(target_col_name,year(target_col_name)).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|  raw_date|quarter(raw_date)|\n",
      "+----------+-----------------+\n",
      "|2019-01-01|                1|\n",
      "|2018-07-01|                3|\n",
      "|2017-12-01|                4|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the quarter\n",
    "\n",
    "date_func_df.select(target_col_name,quarter(target_col_name)).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|  raw_date|month(raw_date)|\n",
      "+----------+---------------+\n",
      "|2019-01-01|              1|\n",
      "|2018-07-01|              7|\n",
      "|2017-12-01|             12|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the month\n",
    "\n",
    "date_func_df.select(target_col_name,month(target_col_name)).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------------+----------------+\n",
      "|  raw_date|hour(raw_date)|minute(raw_date)|second(raw_date)|\n",
      "+----------+--------------+----------------+----------------+\n",
      "|2019-01-01|             0|               0|               0|\n",
      "|2018-07-01|             0|               0|               0|\n",
      "|2017-12-01|             0|               0|               0|\n",
      "+----------+--------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the hour,minute,second\n",
    "\n",
    "date_func_df.select(target_col_name,hour(target_col_name),minute(target_col_name),second(target_col_name)).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+-------------------+--------------------+\n",
      "|  raw_date|dayofyear(raw_date)|dayofmonth(raw_date)|dayofweek(raw_date)|weekofyear(raw_date)|\n",
      "+----------+-------------------+--------------------+-------------------+--------------------+\n",
      "|2019-01-01|                  1|                   1|                  3|                   1|\n",
      "|2018-07-01|                182|                   1|                  1|                  26|\n",
      "|2017-12-01|                335|                   1|                  6|                  48|\n",
      "+----------+-------------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_func_df.select(target_col_name,dayofyear(target_col_name),dayofmonth(target_col_name),dayofweek(target_col_name),weekofyear(target_col_name)).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.6.1 Date arithmetic operation\n",
    "\n",
    "Below functions allows us to do arithmetic operations on date columns\n",
    "\n",
    "- date_add(start, days): Add days to the date.\n",
    "- add_months(start, months): Add months to date.\n",
    "- datediff(end, start):\tReturns difference between two dates in days.\n",
    "- date_sub(start, days): Subtract the days from date field.\n",
    "- last_day(date): Returns the last day of the month which the given date belongs to.\n",
    "- months_between(date1, date2): Returns number of months between two dates.\n",
    "- next_day(date, dayOfWeek): Returns the first date which is later than the value of the date column.\n",
    "- trunc(date, format): Returns date truncated to the unit specified by the format."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}