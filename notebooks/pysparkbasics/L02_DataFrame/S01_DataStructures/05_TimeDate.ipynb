{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Date and time in Spark data frames\n",
    "\n",
    "Date and time column are essential for time series data. In spark, we can express date and time with **four different data types**: \n",
    "- String : Most commun format, no compability issues. But need to be parsed to long or timestamp types to do arithmetic operations\n",
    "- Long : in spark, it represents **unix_timestamp** implementation. it uses seconds since the UNIX epoch\n",
    "- timestamp :  spark has its own implementation to represent timestamp. Represents values comprising values of fields year, month, day, hour, minute, and second, with the **session local time-zone**. The timestamp value represents an absolute point in time.\n",
    "- date: Represents values comprising values of fields year, month and day, without a time-zone.\n",
    "\n",
    "Below, we will show how to do convertion between them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit, unix_timestamp, col, from_unixtime, to_timestamp, date_format, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"DateAndTime\").getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession \\\n",
    "    .builder.master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "    .appName(\"SparkArrowCompression\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(\"2019-07-01 12:01:19\", \"07-01-2019 12:01:19.888\", \"07-01-2019\"),\n",
    "            (\"2018-07-01 12:01:19\", \"07-01-2018 12:01:19.666\", \"07-01-2018\"),\n",
    "            (\"2017-07-01 12:01:19\", \"07-01-2017 12:01:19.111\", \"07-01-2017\")]\n",
    "columns = [\"timestamp_1\", \"timestamp_2\", \"timestamp_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+\n",
      "|        timestamp_1|         timestamp_2|timestamp_3|\n",
      "+-------------------+--------------------+-----------+\n",
      "|2019-07-01 12:01:19|07-01-2019 12:01:...| 07-01-2019|\n",
      "|2018-07-01 12:01:19|07-01-2018 12:01:...| 07-01-2018|\n",
      "|2017-07-01 12:01:19|07-01-2017 12:01:...| 07-01-2017|\n",
      "+-------------------+--------------------+-----------+\n",
      "\n",
      "root\n",
      " |-- timestamp_1: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Convert string date, timestamp to long (seconds since the UNIX epoch)\n",
    "\n",
    "To convert string to long, we can use unix_timestamp() function, it takes a string date, and a date format, if the date format is not provided, the default format (yyyy-MM-dd HH:mm:ss) will be used. Then the string date are converted to Unix timestamp (in seconds) by using the current timezone of the system. \n",
    "\n",
    "Note you can set the timezone of your spark session by using **spark.conf.set(\"spark.sql.session.timeZone\", \"<time_zone>\")**, for example spark.conf.set(\"spark.sql.session.timeZone\", \"US/Pacific\").\n",
    "\n",
    "1: def unix_timestamp(): returns the current time in seconds (LongType)\n",
    "2) def unix_timestamp(date: Column): take the string column date as input, output it in seconds, it will use default date format (yyyy-MM-dd HH:mm:ss) to parse the string. \n",
    "3) def unix_timestamp(date: Column, format: String): we can specify explicitly the format of the date string, if the default format does not fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_3: string (nullable = true)\n",
      " |-- timestamp_4: long (nullable = true)\n",
      "\n",
      "+-------------------+--------------------+-----------+-----------+\n",
      "|        timestamp_1|         timestamp_2|timestamp_3|timestamp_4|\n",
      "+-------------------+--------------------+-----------+-----------+\n",
      "|2019-07-01 12:01:19|07-01-2019 12:01:...| 07-01-2019| 1632594547|\n",
      "|2018-07-01 12:01:19|07-01-2018 12:01:...| 07-01-2018| 1632594547|\n",
      "|2017-07-01 12:01:19|07-01-2017 12:01:...| 07-01-2017| 1632594547|\n",
      "+-------------------+--------------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We add a new column that is filled with the current time\n",
    "# note the column type\n",
    "df1 = df.withColumn(\"timestamp_4\", lit(unix_timestamp()))\n",
    "df1.printSchema()\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp1: covert date string to seconds with some erreurs\n",
      "root\n",
      " |-- timestamp_1: long (nullable = true)\n",
      " |-- timestamp_2: long (nullable = true)\n",
      " |-- timestamp_3: long (nullable = true)\n",
      "\n",
      "+-----------+-----------+-----------+\n",
      "|timestamp_1|timestamp_2|timestamp_3|\n",
      "+-----------+-----------+-----------+\n",
      "| 1561982479| 1561982479|       null|\n",
      "| 1530446479| 1530446479|       null|\n",
      "| 1498910479| 1498910479|       null|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note the output dataframe has timestamp 1 and timestamp 2 correct. But timestamp 3 wrong. \n",
    "# we did not give format for timestamp_1 and timestamp_3, it works for timestamp_1 because it uses the default format is yyyy-MM-dd HH:mm:ss\n",
    "# timestamp_3 does not work, because it does not use the default date format.\n",
    "# for timestamp_2, we specify a date format, if we don't give the format, it returns null, because the string does not have\n",
    "# the default date format\n",
    "# If we give a format that is wrong, the spark job will fail. For example, try to remove SSS from timestamp_2 format, and see what happens\n",
    "\n",
    "df2 = df.withColumn(\"timestamp_1\", unix_timestamp(\"timestamp_1\")) \\\n",
    "        .withColumn(\"timestamp_2\", unix_timestamp(\"timestamp_2\", \"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
    "        .withColumn(\"timestamp_3\", unix_timestamp(\"timestamp_3\"))\n",
    "print(\"Exp1: covert date string to seconds with some erreurs\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp1: covert date string to seconds with success\n",
      "root\n",
      " |-- timestamp_1: long (nullable = true)\n",
      " |-- timestamp_2: long (nullable = true)\n",
      " |-- timestamp_3: long (nullable = true)\n",
      " |-- timestamp_4: long (nullable = true)\n",
      "\n",
      "+-----------+-----------+-----------+-----------+\n",
      "|timestamp_1|timestamp_2|timestamp_3|timestamp_4|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "| 1561982479| 1561982479| 1561939200| 1632594849|\n",
      "| 1530446479| 1530446479| 1530403200| 1632594849|\n",
      "| 1498910479| 1498910479| 1498867200| 1632594849|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# below, we give each one a correct date format.\n",
    "df3 = df.withColumn(\"timestamp_1\", unix_timestamp(\"timestamp_1\")) \\\n",
    "        .withColumn(\"timestamp_2\", unix_timestamp(\"timestamp_2\", \"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
    "        .withColumn(\"timestamp_3\", unix_timestamp(\"timestamp_3\", \"MM-dd-yyyy\")) \\\n",
    "        .withColumn(\"timestamp_4\", lit(unix_timestamp()))\n",
    "print(\"Exp1: covert date string to seconds with success\")\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Convert long(seconds since the UNIX epoch) back to string date\n",
    "\n",
    "We can use from_unixtime(unix_time: Column, format: String) to convert long back to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_3: string (nullable = true)\n",
      " |-- timestamp_4: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-----------+-------------------+\n",
      "|timestamp_1        |timestamp_2        |timestamp_3|timestamp_4        |\n",
      "+-------------------+-------------------+-----------+-------------------+\n",
      "|2019-07-01 12:01:19|07-01-2019 12:01:19|07-01-2019 |2021-09-25 18:39:03|\n",
      "|2018-07-01 12:01:19|07-01-2018 12:01:19|07-01-2018 |2021-09-25 18:39:03|\n",
      "|2017-07-01 12:01:19|07-01-2017 12:01:19|07-01-2017 |2021-09-25 18:39:03|\n",
      "+-------------------+-------------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert unix timestamp in second to string date\n",
    "df4 = df3.select(\n",
    "        from_unixtime(col(\"timestamp_1\")).alias(\"timestamp_1\"),\n",
    "        from_unixtime(col(\"timestamp_2\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"timestamp_2\"),\n",
    "        from_unixtime(col(\"timestamp_3\"), \"MM-dd-yyyy\").alias(\"timestamp_3\"),\n",
    "        from_unixtime(col(\"timestamp_4\")).alias(\"timestamp_4\")\n",
    "    )\n",
    "df4.printSchema()\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Convert string to spark timestamp\n",
    "\n",
    "Spark provides its own timestamp type. Instead of long, we can also use spark timestamp to store time data.\n",
    "\n",
    "To convert a string date to spark timestamp, we can use:\n",
    "- to_timestamp(col, format): the default format is yyyy-MM-dd HH:mm:ss. If it failed, return null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: timestamp (nullable = true)\n",
      " |-- timestamp_2: timestamp (nullable = true)\n",
      " |-- timestamp_3: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+-----------------------+-------------------+\n",
      "|timestamp_1        |timestamp_2            |timestamp_3        |\n",
      "+-------------------+-----------------------+-------------------+\n",
      "|2019-07-01 12:01:19|2019-07-01 12:01:19.888|2019-07-01 00:00:00|\n",
      "|2018-07-01 12:01:19|2018-07-01 12:01:19.666|2018-07-01 00:00:00|\n",
      "|2017-07-01 12:01:19|2017-07-01 12:01:19.111|2017-07-01 00:00:00|\n",
      "+-------------------+-----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to_timestamp() works similar to unix_timestamp(). For timestamp_1, it has the default format, so no need to specify the format\n",
    "# For 2 and 3, we need to specify the date format. Otherwise, it returns null\n",
    "\n",
    "# note the output column type\n",
    "\n",
    "df5 = df.withColumn(\"timestamp_1\", to_timestamp(\"timestamp_1\")) \\\n",
    "        .withColumn(\"timestamp_2\", to_timestamp(\"timestamp_2\", \"MM-dd-yyyy HH:mm:ss.SSS\")) \\\n",
    "        .withColumn(\"timestamp_3\", to_timestamp(\"timestamp_3\", \"MM-dd-yyyy\"))\n",
    "\n",
    "df5.printSchema()\n",
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also use to_timestamp in a sql \n",
    "spark.sql(\"select to_timestamp('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Convert spark timestamp back to string \n",
    "\n",
    "To convert timestamp back to string, we have two options \n",
    "- cast(\"String\"): It converts a timestamp column to default date format yyyy-MM-dd HH:mm:ss\n",
    "- date_format(column,format): It converts a timestamp column to any Java Date formats specified in DateTimeFormatter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Use cast(\"String\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_2: string (nullable = true)\n",
      " |-- timestamp_2: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----------------------+\n",
      "|timestamp_2            |timestamp_2            |\n",
      "+-----------------------+-----------------------+\n",
      "|2019-07-01 12:01:19.888|2019-07-01 12:01:19.888|\n",
      "|2018-07-01 12:01:19.666|2018-07-01 12:01:19.666|\n",
      "|2017-07-01 12:01:19.111|2017-07-01 12:01:19.111|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert spark timestamp back to string by using cast()\n",
    "# note they all use the default date format.\n",
    "df6 = df5.select(col(\"timestamp_2\").cast(\"string\"), \\\n",
    "                col(\"timestamp_2\").cast(\"string\"))\n",
    "df6.printSchema()\n",
    "df6.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Use date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp_1: timestamp (nullable = true)\n",
      " |-- timestamp_2: timestamp (nullable = true)\n",
      " |-- timestamp_3: timestamp (nullable = true)\n",
      " |-- str_date_yyyy_MM_dd: string (nullable = true)\n",
      " |-- str_date_MM/dd/yyyy_hh:mm: string (nullable = true)\n",
      " |-- str_date_yyyy_MMM_dd: string (nullable = true)\n",
      " |-- str_date_yyyy_MMMM_dd_E: string (nullable = true)\n",
      "\n",
      "+-------------------+-----------------------+-------------------+-------------------+-------------------------+--------------------+-----------------------+\n",
      "|timestamp_1        |timestamp_2            |timestamp_3        |str_date_yyyy_MM_dd|str_date_MM/dd/yyyy_hh:mm|str_date_yyyy_MMM_dd|str_date_yyyy_MMMM_dd_E|\n",
      "+-------------------+-----------------------+-------------------+-------------------+-------------------------+--------------------+-----------------------+\n",
      "|2019-07-01 12:01:19|2019-07-01 12:01:19.888|2019-07-01 00:00:00|2019 07 01         |07/01/2019 12:01         |2019 Jul 01         |2019 July 01 Mon       |\n",
      "|2018-07-01 12:01:19|2018-07-01 12:01:19.666|2018-07-01 00:00:00|2018 07 01         |07/01/2018 12:01         |2018 Jul 01         |2018 July 01 Sun       |\n",
      "|2017-07-01 12:01:19|2017-07-01 12:01:19.111|2017-07-01 00:00:00|2017 07 01         |07/01/2017 12:01         |2017 Jul 01         |2017 July 01 Sat       |\n",
      "+-------------------+-----------------------+-------------------+-------------------+-------------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use date_format() to convert spark timestamp to various string date\n",
    "df7 = df5.withColumn(\"str_date_yyyy_MM_dd\", date_format(col(\"timestamp_2\"), \"yyyy MM dd\")) \\\n",
    "        .withColumn(\"str_date_MM/dd/yyyy_hh:mm\", date_format(col(\"timestamp_2\"), \"MM/dd/yyyy hh:mm\")) \\\n",
    "        .withColumn(\"str_date_yyyy_MMM_dd\", date_format(col(\"timestamp_2\"), \"yyyy MMM dd\")) \\\n",
    "        .withColumn(\"str_date_yyyy_MMMM_dd_E\", date_format(col(\"timestamp_2\"), \"yyyy MMMM dd E\"))\n",
    "    \n",
    "df7.printSchema()\n",
    "df7.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 use to_date() function to convert string to date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|     input|      date|\n",
      "+----------+----------+\n",
      "|02-03-2013|2013-02-03|\n",
      "|05-06-2023|2023-05-06|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1=[[\"02-03-2013\"], \n",
    "       [\"05-06-2023\"]]\n",
    "columns1=[\"input\"]\n",
    "df8 = spark.createDataFrame(data=data1,schema=columns1 )\n",
    "df9 = df8.withColumn(\"date\", to_date(\"input\", \"MM-dd-yyyy\"))\n",
    "\n",
    "df9.printSchema()\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
