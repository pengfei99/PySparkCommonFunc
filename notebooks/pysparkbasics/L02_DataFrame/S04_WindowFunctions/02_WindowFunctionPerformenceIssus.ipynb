{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Window Function Performence issus\n",
    "\n",
    "In previous section, we have explained that the window specification change the partition of the data frame. As a result the partition number of your data frame may not \n",
    "be optimal at all. For example, if your window specification partition a column which only has one unique value, this will cause the entire dataset to be shuffled to a single executor. If the executor's memory/disk can not hold the data, the job will fail with OOM errors.\n",
    "\n",
    "If we use another column to do the partitionBy. The global order of the column that we want to sort will be lost.\n",
    "So we need to partition the dataframe by conserving the global order of the the sort column. Which means all_values_in_sort_column(partition_0) < all_values_in_sort_column(partition_1) < all_values_in_sort_column(partition_2). And all values in one partition are sorted, so we keep the global order of the sorted column. \n",
    "\n",
    "Note in spark 3.1.2. only the same value will be put in the same partition. If column price has 12 distinct value, the dataframe after orderBy(\"price\") will have 12 partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import spark_partition_id, row_number, rank, dense_rank, col, lit, coalesce, broadcast, max, sum\n",
    "from pyspark.sql.window import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=False\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"WindowFunctionPerformenceIssus\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"WindowFunctionPerformenceIssus\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1') \\\n",
    "                      .getOrCreate()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+\n",
      "|name|      date|  product|price|\n",
      "+----+----------+---------+-----+\n",
      "|Alex|2018-10-10|    Paint|    5|\n",
      "|Alex|2018-04-02|   Ladder|   10|\n",
      "|Alex|2018-06-22|    Stool|   15|\n",
      "|Alex|2018-12-09|   Vacuum|   20|\n",
      "|Alex|2018-07-12|   Bucket|   20|\n",
      "|Alex|2018-02-18|   Gloves|   20|\n",
      "|Alex|2018-03-03|  Brushes|   20|\n",
      "|Alex|2018-09-26|Sandpaper|   30|\n",
      "|Alex|2018-12-09|   Vacuum|   30|\n",
      "|Alex|2018-07-12|   Bucket|   30|\n",
      "|Alex|2018-02-18|   Gloves|   30|\n",
      "|Alex|2018-03-03|  Brushes|    5|\n",
      "|Alex|2018-09-26|Sandpaper|    5|\n",
      "+----+----------+---------+-----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Alex', '2018-10-10', 'Paint', 5),\n",
    "            ('Alex', '2018-04-02', 'Ladder', 10),\n",
    "            ('Alex', '2018-06-22', 'Stool', 15),\n",
    "            ('Alex', '2018-12-09', 'Vacuum', 20),\n",
    "            ('Alex', '2018-07-12', 'Bucket', 20),\n",
    "            ('Alex', '2018-02-18', 'Gloves', 20),\n",
    "            ('Alex', '2018-03-03', 'Brushes', 20),\n",
    "            ('Alex', '2018-09-26', 'Sandpaper', 30),\n",
    "            ('Alex', '2018-12-09', 'Vacuum', 30),\n",
    "            ('Alex', '2018-07-12', 'Bucket', 30),\n",
    "            ('Alex', '2018-02-18', 'Gloves', 30),\n",
    "            ('Alex', '2018-03-03', 'Brushes', 5),\n",
    "            ('Alex', '2018-09-26', 'Sandpaper', 5)]\n",
    "\n",
    "df = spark.createDataFrame(data,schema=['name','date','product','price'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------+\n",
      "|name|date      |product  |price|partition_id|\n",
      "+----+----------+---------+-----+------------+\n",
      "|Alex|2018-10-10|Paint    |5    |0           |\n",
      "|Alex|2018-04-02|Ladder   |10   |0           |\n",
      "|Alex|2018-06-22|Stool    |15   |0           |\n",
      "|Alex|2018-12-09|Vacuum   |20   |1           |\n",
      "|Alex|2018-07-12|Bucket   |20   |1           |\n",
      "|Alex|2018-02-18|Gloves   |20   |1           |\n",
      "|Alex|2018-03-03|Brushes  |20   |2           |\n",
      "|Alex|2018-09-26|Sandpaper|30   |2           |\n",
      "|Alex|2018-12-09|Vacuum   |30   |2           |\n",
      "|Alex|2018-07-12|Bucket   |30   |3           |\n",
      "|Alex|2018-02-18|Gloves   |30   |3           |\n",
      "|Alex|2018-03-03|Brushes  |5    |3           |\n",
      "|Alex|2018-09-26|Sandpaper|5    |3           |\n",
      "+----+----------+---------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the default partition of the dataframe\n",
    "# You can notice that we have 4 partition, and row are evenly split into these four partition \n",
    "df.withColumn(\"partition_id\",spark_partition_id()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Example of What is wrong with window spec without partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+----+------------+\n",
      "|name|      date|  product|price|row_number|rank|partition_id|\n",
      "+----+----------+---------+-----+----------+----+------------+\n",
      "|Alex|2018-10-10|    Paint|    5|         1|   1|           0|\n",
      "|Alex|2018-03-03|  Brushes|    5|         2|   1|           0|\n",
      "|Alex|2018-09-26|Sandpaper|    5|         3|   1|           0|\n",
      "|Alex|2018-04-02|   Ladder|   10|         4|   2|           0|\n",
      "|Alex|2018-06-22|    Stool|   15|         5|   3|           0|\n",
      "|Alex|2018-03-03|  Brushes|   20|         6|   4|           0|\n",
      "|Alex|2018-12-09|   Vacuum|   20|         7|   4|           0|\n",
      "|Alex|2018-07-12|   Bucket|   20|         8|   4|           0|\n",
      "|Alex|2018-02-18|   Gloves|   20|         9|   4|           0|\n",
      "|Alex|2018-09-26|Sandpaper|   30|        10|   5|           0|\n",
      "|Alex|2018-12-09|   Vacuum|   30|        11|   5|           0|\n",
      "|Alex|2018-07-12|   Bucket|   30|        12|   5|           0|\n",
      "|Alex|2018-02-18|   Gloves|   30|        13|   5|           0|\n",
      "+----+----------+---------+-----+----------+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's try to use a window function\n",
    "# We first define an orderBy window specification without partitionBy\n",
    "# You can notice all rows are in the same partition 0 which will be shuffled to the same executor.\n",
    "# If you have millions of rows in the same executor, you will get an OOM error\n",
    "\n",
    "window_with_only_order= Window.orderBy(\"price\")\n",
    "df1=df.withColumn(\"row_number\",row_number().over(window_with_only_order))\\\n",
    "      .withColumn(\"rank\",dense_rank().over(window_with_only_order)) \\\n",
    "      .withColumn(\"partition_id\",spark_partition_id())\n",
    "# note the entire dataframe only has 1 partition. Because our window spec does not have partitionBy\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Slove the above problem\n",
    "\n",
    "Now, we want to calculate the rank of price column, without separate them into different windows. In a small dataset we can directly use groupBy specification without partition. \n",
    "But with a big data set, we will have OOM error. \n",
    "\n",
    "To avoid this, we do the following\n",
    "\n",
    "Step1: use orderBy(\"price\") to partition the dataframe, create a column \"partition_id\" by using function spark_partition_id,\n",
    "\n",
    "Step2: Create a local_rank column to rank each row inside its partition.\n",
    "\n",
    "Step3: Get max rank of each partition by grouping the partition_id column\n",
    "\n",
    "Step4: Use a window spec to get cumulative rank number for each partition\n",
    "\n",
    "Step5: calculate the a sum factor to which can sum to local rank to become a global rank \n",
    "\n",
    "Step6: join the sum_factor with local rank and calculate the global rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------+\n",
      "|name|date      |product  |price|partition_id|\n",
      "+----+----------+---------+-----+------------+\n",
      "|Alex|2018-10-10|Paint    |5    |0           |\n",
      "|Alex|2018-03-03|Brushes  |5    |0           |\n",
      "|Alex|2018-09-26|Sandpaper|5    |0           |\n",
      "|Alex|2018-04-02|Ladder   |10   |1           |\n",
      "|Alex|2018-06-22|Stool    |15   |2           |\n",
      "|Alex|2018-03-03|Brushes  |20   |3           |\n",
      "|Alex|2018-12-09|Vacuum   |20   |3           |\n",
      "|Alex|2018-07-12|Bucket   |20   |3           |\n",
      "|Alex|2018-02-18|Gloves   |20   |3           |\n",
      "|Alex|2018-07-12|Bucket   |30   |4           |\n",
      "|Alex|2018-02-18|Gloves   |30   |4           |\n",
      "|Alex|2018-09-26|Sandpaper|30   |4           |\n",
      "|Alex|2018-12-09|Vacuum   |30   |4           |\n",
      "+----+----------+---------+-----+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:\n",
    "# To partition the dataframe and conserving the global sort order, we can use orderBy(\"sort_column_name\")\n",
    "# In our case, the column name is price which we want to sort.\n",
    "df_sort_part = df.orderBy(\"price\").withColumn(\"partition_id\", spark_partition_id())\n",
    "df_sort_part.show(truncate=False)\n",
    "df_sort_part.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------+----------+\n",
      "|name|      date|  product|price|partition_id|local_rank|\n",
      "+----+----------+---------+-----+------------+----------+\n",
      "|Alex|2018-04-02|   Ladder|   10|           1|         1|\n",
      "|Alex|2018-03-03|  Brushes|   20|           3|         1|\n",
      "|Alex|2018-12-09|   Vacuum|   20|           3|         1|\n",
      "|Alex|2018-07-12|   Bucket|   20|           3|         1|\n",
      "|Alex|2018-02-18|   Gloves|   20|           3|         1|\n",
      "|Alex|2018-07-12|   Bucket|   30|           4|         1|\n",
      "|Alex|2018-02-18|   Gloves|   30|           4|         1|\n",
      "|Alex|2018-09-26|Sandpaper|   30|           4|         1|\n",
      "|Alex|2018-12-09|   Vacuum|   30|           4|         1|\n",
      "|Alex|2018-06-22|    Stool|   15|           2|         1|\n",
      "|Alex|2018-03-03|  Brushes|    5|           0|         1|\n",
      "|Alex|2018-09-26|Sandpaper|    5|           0|         1|\n",
      "|Alex|2018-10-10|    Paint|    5|           0|         1|\n",
      "+----+----------+---------+-----+------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Create a local_rank column to rank each row inside its partition.\n",
    "# We create a window specification partitionBy the partition_id which is created by orderBy()\n",
    "win_part_id = Window.partitionBy(\"partition_id\").orderBy(\"price\")\n",
    "# does not work on cluster mode, the window function trigger a repartion to 200 partition\n",
    "df_rank = df_sort_part.withColumn(\"local_rank\", rank().over(win_part_id))\n",
    "df_rank.show()\n",
    "df_rank.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|partition_id|max_rank|\n",
      "+------------+--------+\n",
      "|           1|       1|\n",
      "|           3|       1|\n",
      "|           4|       1|\n",
      "|           2|       1|\n",
      "|           0|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Step 3: Get max rank of each partition by grouping the partition_id column\n",
    "df_tmp = df_rank.groupBy(\"partition_id\").agg(max(\"local_rank\").alias(\"max_rank\"))\n",
    "    \n",
    "df_tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------+\n",
      "|partition_id|max_rank|cum_rank|\n",
      "+------------+--------+--------+\n",
      "|           0|       1|       1|\n",
      "|           1|       1|       2|\n",
      "|           2|       1|       3|\n",
      "|           3|       1|       4|\n",
      "|           4|       1|       5|\n",
      "+------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Use a window spec to get cumulative rank number for each partition\n",
    "win_rank = Window.orderBy(\"partition_id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_stats = df_tmp.withColumn(\"cum_rank\", sum(\"max_rank\").over(win_rank))\n",
    "df_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------+------------+--------+--------+\n",
      "|partition_id|max_rank|cum_rank|partition_id|max_rank|cum_rank|\n",
      "+------------+--------+--------+------------+--------+--------+\n",
      "|           0|       1|       1|        null|    null|    null|\n",
      "|           1|       1|       2|           0|       1|       1|\n",
      "|           2|       1|       3|           1|       1|       2|\n",
      "|           3|       1|       4|           2|       1|       3|\n",
      "|           4|       1|       5|           3|       1|       4|\n",
      "+------------+--------+--------+------------+--------+--------+\n",
      "\n",
      "+------------+----------+\n",
      "|partition_id|sum_factor|\n",
      "+------------+----------+\n",
      "|           1|         1|\n",
      "|           3|         3|\n",
      "|           4|         4|\n",
      "|           2|         2|\n",
      "|           0|         0|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5 calculate the a sum factor to which can sum to local rank to become a global rank\n",
    "# tmp1 is a self join with the join condition, l.partition_id == r.partition_id +1\n",
    "# this means we shift the cumulative sum by 1 row on the right data frame\n",
    "tmp1 = df_stats.alias(\"l\").join(df_stats.alias(\"r\"), col(\"l.partition_id\") == col(\"r.partition_id\") + 1, \"left\")\n",
    "tmp1.show()\n",
    "\n",
    "join_df = tmp1.select(col(\"l.partition_id\"), coalesce(col(\"r.cum_rank\"), lit(0)).alias(\"sum_factor\"))\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----------+---------+-----+----------+----------+----+\n",
      "|partition_id|name|      date|  product|price|local_rank|sum_factor|rank|\n",
      "+------------+----+----------+---------+-----+----------+----------+----+\n",
      "|           0|Alex|2018-10-10|    Paint|    5|         1|         0|   1|\n",
      "|           0|Alex|2018-03-03|  Brushes|    5|         1|         0|   1|\n",
      "|           0|Alex|2018-09-26|Sandpaper|    5|         1|         0|   1|\n",
      "|           1|Alex|2018-04-02|   Ladder|   10|         1|         1|   2|\n",
      "|           2|Alex|2018-06-22|    Stool|   15|         1|         2|   3|\n",
      "|           3|Alex|2018-03-03|  Brushes|   20|         1|         3|   4|\n",
      "|           3|Alex|2018-07-12|   Bucket|   20|         1|         3|   4|\n",
      "|           3|Alex|2018-12-09|   Vacuum|   20|         1|         3|   4|\n",
      "|           3|Alex|2018-02-18|   Gloves|   20|         1|         3|   4|\n",
      "|           4|Alex|2018-07-12|   Bucket|   30|         1|         4|   5|\n",
      "|           4|Alex|2018-09-26|Sandpaper|   30|         1|         4|   5|\n",
      "|           4|Alex|2018-02-18|   Gloves|   30|         1|         4|   5|\n",
      "|           4|Alex|2018-12-09|   Vacuum|   30|         1|         4|   5|\n",
      "+------------+----+----------+---------+-----+----------+----------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6 join the sum_factor with local rank and calculate the global rank\n",
    "df_final = df_rank.join(broadcast(join_df), \"partition_id\", \"inner\") \\\n",
    "        .withColumn(\"rank\", col(\"local_rank\") + col(\"sum_factor\"))\n",
    "\n",
    "df_final.orderBy(\"rank\").show()\n",
    "df_final.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+----+------------+\n",
      "|name|      date|  product|price|row_number|rank|partition_id|\n",
      "+----+----------+---------+-----+----------+----+------------+\n",
      "|Alex|2018-10-10|    Paint|    5|         1|   1|           0|\n",
      "|Alex|2018-03-03|  Brushes|    5|         2|   1|           0|\n",
      "|Alex|2018-09-26|Sandpaper|    5|         3|   1|           0|\n",
      "|Alex|2018-04-02|   Ladder|   10|         4|   2|           0|\n",
      "|Alex|2018-06-22|    Stool|   15|         5|   3|           0|\n",
      "|Alex|2018-03-03|  Brushes|   20|         6|   4|           0|\n",
      "|Alex|2018-12-09|   Vacuum|   20|         7|   4|           0|\n",
      "|Alex|2018-07-12|   Bucket|   20|         8|   4|           0|\n",
      "|Alex|2018-02-18|   Gloves|   20|         9|   4|           0|\n",
      "|Alex|2018-07-12|   Bucket|   30|        10|   5|           0|\n",
      "|Alex|2018-02-18|   Gloves|   30|        11|   5|           0|\n",
      "|Alex|2018-09-26|Sandpaper|   30|        12|   5|           0|\n",
      "|Alex|2018-12-09|   Vacuum|   30|        13|   5|           0|\n",
      "+----+----------+---------+-----+----------+----+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can notice the above df has 200 partition. and df1 has one partition.\n",
    "df1.show()\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
