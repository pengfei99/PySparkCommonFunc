{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Window function in spark\n",
    "\n",
    "A window function performs a calculation across a set of rows(aka. Frame). The built-in\n",
    "window functions provided by Spark SQL include two categories:\n",
    "- Ranking functions:\n",
    "- Analytic functions:\n",
    "\n",
    "\n",
    "## 1.1 Window specification\n",
    "To use window functions, we need to create a **window specification**. A window specification defines which rows\n",
    "are included in the frame associated with a given input row. In another word, the window specification defines\n",
    "the default frame of a window. A window specification can be classified into three categories:\n",
    "\n",
    "1. PartitionBy specification:\n",
    "       - Created with Window.partitionBy on one or more columns\n",
    "       - All rows that have the same value on the partitionBy column will be in the same frame.\n",
    "       - The aggregation functions can be applied on each frame\n",
    "       - The windows functions can not be applied.\n",
    "\n",
    "\n",
    "2. Ordered specification:\n",
    "       - Created by using a partitionBy specification, followed by an orderBy specification\n",
    "       - The frame is not static, it moves when we iterate each row. By default, the frame contains\n",
    "         all previous rows and the currentRow.\n",
    "       - The window function can be applied to each moving frame (i.e. currentRow+allPreviousRow)\n",
    "       - The aggregation functions can be applied to each moving frame. As each row has a different\n",
    "         frame, the result of the aggregation is different for each row. Unlike the partitionBy\n",
    "         specification, all rows in the same partition has the same result.\n",
    "\n",
    "3. Custom Range Frame specification: (check exp4)\n",
    "       - Created by using a partitionBy specification,\n",
    "       - Usually followed by an orderBy specification,\n",
    "       - Then followed by \"rangeBetween\" or \"rowsBetween\"\n",
    "       - Each row has a corresponding frame which is controlled by rangeBetween or rowsBetween. For example,\n",
    "         rowsBetween(-3,Window.currentRow) means the three rows preceding the current row to the current row.\n",
    "         It defines a frame including the current input row and three rows appearing before the current row.\n",
    "       - Aggregation can be applied on each frame.\n",
    "\n",
    "\n",
    "In spark SQL, the partition specification are defined by keyword \"partitionBy\", ordering specification is defined by\n",
    "keyword \"orderBy\".\n",
    "\n",
    "\n",
    "## 1.2 Windows function can be divided into following categories\n",
    "\n",
    "1. Ranking functions:\n",
    "  - rank: returns the rank of rows within a window partition\n",
    "  - dense_rank: returns the rank of rows within a window partition, without any gaps. For example, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "  - percent_rank: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
    "  - ntile(n:Int): returns the ntile group id (from 1 to n inclusive) in an ordered window partition. For example, if n is 4, the first quarter of the rows will get rank 1, the second quarter will get 2, the thirds quarter will get 3, and the last will get 4. If the rows are less than n, it works too.\n",
    "  - row_number: returns a sequential number starting at 1 within a window partition.\n",
    "\n",
    "2. Analytic functions:\n",
    "   - cume_dist: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row. N = total number of rows in the partition. cumeDist(x) = number of values before (and including) x / N. similar to percent_rank()\n",
    "   - first()\n",
    "   - last()\n",
    "   - lag(e\\:Column,offset\\:Int,defaultValue\\:Object): returns the value that is offset rows before the current row, and null if there is less than offset rows before row. For example, an offset of one will return the previous row at any given point in the window partition. The defaultValue is optional\n",
    "   - lead(e:Column,offset:Int): returns the value that is offset rows after the current row, and null if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition.\n",
    "   - currentRow(): Window function: returns the special frame boundary that represents the current row in\n",
    "                          the window partition.\n",
    "3. Aggregation functions: All the aggregation function that we showed in S03_GroupByAndAggregation can be used here.\n",
    "   - sum(e:Column): returns the sum of selecting column for each partitions.\n",
    "   - first(e:Column): returns the first value within each partition.\n",
    "   - last(e:Column): returns the last value within each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, lag, lead, col, avg, \\\n",
    "    min, max, sum, round, count, datediff, unix_timestamp, stddev, collect_list, element_at, size, sort_array, \\\n",
    "    broadcast, spark_partition_id, lit, coalesce\n",
    "\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/19 06:26:35 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.184.142 instead (on interface ens33)\n",
      "21/09/19 06:26:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/09/19 06:26:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data frame: \n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+\n",
      "|name|date      |product  |price|\n",
      "+----+----------+---------+-----+\n",
      "|Alex|2018-10-10|Paint    |80   |\n",
      "|Alex|2018-04-02|Ladder   |20   |\n",
      "|Alex|2018-06-22|Stool    |20   |\n",
      "|Alex|2018-12-09|Vacuum   |40   |\n",
      "|Alex|2018-07-12|Bucket   |5    |\n",
      "|Alex|2018-02-18|Gloves   |5    |\n",
      "|Alex|2018-03-03|Brushes  |30   |\n",
      "|Alex|2018-09-26|Sandpaper|10   |\n",
      "|Bob |2018-12-09|Vacuum   |40   |\n",
      "|Bob |2018-07-12|Bucket   |5    |\n",
      "|Bob |2018-02-18|Gloves   |5    |\n",
      "|Bob |2018-03-03|Brushes  |30   |\n",
      "|Bob |2018-09-26|Sandpaper|10   |\n",
      "+----+----------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Windows functions\").getOrCreate()\n",
    "data = [('Alex', '2018-10-10', 'Paint', 80),\n",
    "        ('Alex', '2018-04-02', 'Ladder', 20),\n",
    "        ('Alex', '2018-06-22', 'Stool', 20),\n",
    "        ('Alex', '2018-12-09', 'Vacuum', 40),\n",
    "        ('Alex', '2018-07-12', 'Bucket', 5),\n",
    "        ('Alex', '2018-02-18', 'Gloves', 5),\n",
    "        ('Alex', '2018-03-03', 'Brushes', 30),\n",
    "        ('Alex', '2018-09-26', 'Sandpaper', 10),\n",
    "        ('Bob', '2018-12-09', 'Vacuum', 40),\n",
    "        ('Bob', '2018-07-12', 'Bucket', 5),\n",
    "        ('Bob', '2018-02-18', 'Gloves', 5),\n",
    "        ('Bob', '2018-03-03', 'Brushes', 30),\n",
    "        ('Bob', '2018-09-26', 'Sandpaper', 10)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=['name', 'date', 'product', 'price'])\n",
    "print(\"source data frame: \")\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window specifications for the notebook\n",
    "\n",
    "# We first create a window specification by using column name\n",
    "win_name = Window.partitionBy(\"name\")\n",
    "\n",
    "# The second specification takes the first and order it by using column price\n",
    "win_name_ordered = win_name.orderBy(\"price\")\n",
    "\n",
    "# we use name partition, but this time, we will order it by using date\n",
    "win_name_ordered_by_date = win_name.orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Ranking function example \n",
    "In this example, We show how to use window specification to create window. Then we will aplly below Ranking functions on ordered frame:\n",
    "- row_number\n",
    "- rank\n",
    "- dense_rank\n",
    "- percent_rank\n",
    "- ntile\n",
    "\n",
    "Note all above window functions require that the frame are ordered. You can try to\n",
    "replace win_name_ordered by win_name and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- row_number: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|row_number|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-07-12|   Bucket|    5|         1|\n",
      "|Alex|2018-02-18|   Gloves|    5|         2|\n",
      "|Alex|2018-09-26|Sandpaper|   10|         3|\n",
      "|Alex|2018-04-02|   Ladder|   20|         4|\n",
      "|Alex|2018-06-22|    Stool|   20|         5|\n",
      "|Alex|2018-03-03|  Brushes|   30|         6|\n",
      "|Alex|2018-12-09|   Vacuum|   40|         7|\n",
      "|Alex|2018-10-10|    Paint|   80|         8|\n",
      "| Bob|2018-07-12|   Bucket|    5|         1|\n",
      "| Bob|2018-02-18|   Gloves|    5|         2|\n",
      "| Bob|2018-09-26|Sandpaper|   10|         3|\n",
      "| Bob|2018-03-03|  Brushes|   30|         4|\n",
      "| Bob|2018-12-09|   Vacuum|   40|         5|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n",
      "+----+----------+---------+-----+----------+------------+\n",
      "|name|date      |product  |price|row_number|partition_id|\n",
      "+----+----------+---------+-----+----------+------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |1         |74          |\n",
      "|Alex|2018-02-18|Gloves   |5    |2         |74          |\n",
      "|Alex|2018-09-26|Sandpaper|10   |3         |74          |\n",
      "|Alex|2018-04-02|Ladder   |20   |4         |74          |\n",
      "|Alex|2018-06-22|Stool    |20   |5         |74          |\n",
      "|Alex|2018-03-03|Brushes  |30   |6         |74          |\n",
      "|Alex|2018-12-09|Vacuum   |40   |7         |74          |\n",
      "|Alex|2018-10-10|Paint    |80   |8         |74          |\n",
      "|Bob |2018-07-12|Bucket   |5    |1         |93          |\n",
      "|Bob |2018-02-18|Gloves   |5    |2         |93          |\n",
      "|Bob |2018-09-26|Sandpaper|10   |3         |93          |\n",
      "|Bob |2018-03-03|Brushes  |30   |4         |93          |\n",
      "|Bob |2018-12-09|Vacuum   |40   |5         |93          |\n",
      "+----+----------+---------+-----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We first create a window specification by using column name\n",
    "win_name = Window.partitionBy(\"name\")\n",
    "# The second specification takes the first and order it by using column price\n",
    "win_name_ordered = win_name.orderBy(\"price\")\n",
    "# The final specification contains two partition \"Alex\", \"Bob\", each partition is ordered by price in ascending order.\n",
    "\n",
    "# Create a new column by calling the row_number() ranking fucntion. \n",
    "# Two Things to be noted: \n",
    "# 1. To invoke a window function, we use \"function_name.over(specification)\"\n",
    "# 2. You can notice the row number restarted from 1 for Bob, because it's in a new partition\n",
    "df1 = df.withColumn(\"row_number\", row_number().over(win_name_ordered))\n",
    "   \n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "\n",
    "df1.withColumn(\"partition_id\", spark_partition_id()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- rank: integer (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+----+\n",
      "|name|      date|  product|price|rank|\n",
      "+----+----------+---------+-----+----+\n",
      "|Alex|2018-07-12|   Bucket|    5|   1|\n",
      "|Alex|2018-02-18|   Gloves|    5|   1|\n",
      "|Alex|2018-09-26|Sandpaper|   10|   3|\n",
      "|Alex|2018-04-02|   Ladder|   20|   4|\n",
      "|Alex|2018-06-22|    Stool|   20|   4|\n",
      "|Alex|2018-03-03|  Brushes|   30|   6|\n",
      "|Alex|2018-12-09|   Vacuum|   40|   7|\n",
      "|Alex|2018-10-10|    Paint|   80|   8|\n",
      "| Bob|2018-07-12|   Bucket|    5|   1|\n",
      "| Bob|2018-02-18|   Gloves|    5|   1|\n",
      "| Bob|2018-09-26|Sandpaper|   10|   3|\n",
      "| Bob|2018-03-03|  Brushes|   30|   4|\n",
      "| Bob|2018-12-09|   Vacuum|   40|   5|\n",
      "+----+----------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column with rank ranking function\n",
    "# Note that for Alex partition, there is no rank2, because we have two items in rank 1, the third item goes to\n",
    "# rank 3. If you want compact rank number, use dense rank\n",
    "df2 = df.withColumn(\"rank\", rank().over(win_name_ordered))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- dense_rank: integer (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|dense_rank|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-07-12|   Bucket|    5|         1|\n",
      "|Alex|2018-02-18|   Gloves|    5|         1|\n",
      "|Alex|2018-09-26|Sandpaper|   10|         2|\n",
      "|Alex|2018-04-02|   Ladder|   20|         3|\n",
      "|Alex|2018-06-22|    Stool|   20|         3|\n",
      "|Alex|2018-03-03|  Brushes|   30|         4|\n",
      "|Alex|2018-12-09|   Vacuum|   40|         5|\n",
      "|Alex|2018-10-10|    Paint|   80|         6|\n",
      "| Bob|2018-07-12|   Bucket|    5|         1|\n",
      "| Bob|2018-02-18|   Gloves|    5|         1|\n",
      "| Bob|2018-09-26|Sandpaper|   10|         2|\n",
      "| Bob|2018-03-03|  Brushes|   30|         3|\n",
      "| Bob|2018-12-09|   Vacuum|   40|         4|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a column with dense rank\n",
    "# Note that for Alex partition, even thought we have two items in rank 1, but the third item goes to\n",
    "# rank 2 not 3.\n",
    "df3 = df.withColumn(\"dense_rank\", dense_rank().over(win_name_ordered))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- percent_rank: double (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+-------------------+\n",
      "|name|      date|  product|price|       percent_rank|\n",
      "+----+----------+---------+-----+-------------------+\n",
      "|Alex|2018-07-12|   Bucket|    5|                0.0|\n",
      "|Alex|2018-02-18|   Gloves|    5|                0.0|\n",
      "|Alex|2018-09-26|Sandpaper|   10| 0.2857142857142857|\n",
      "|Alex|2018-04-02|   Ladder|   20|0.42857142857142855|\n",
      "|Alex|2018-06-22|    Stool|   20|0.42857142857142855|\n",
      "|Alex|2018-03-03|  Brushes|   30| 0.7142857142857143|\n",
      "|Alex|2018-12-09|   Vacuum|   40| 0.8571428571428571|\n",
      "|Alex|2018-10-10|    Paint|   80|                1.0|\n",
      "| Bob|2018-07-12|   Bucket|    5|                0.0|\n",
      "| Bob|2018-02-18|   Gloves|    5|                0.0|\n",
      "| Bob|2018-09-26|Sandpaper|   10|                0.5|\n",
      "| Bob|2018-03-03|  Brushes|   30|               0.75|\n",
      "| Bob|2018-12-09|   Vacuum|   40|                1.0|\n",
      "+----+----------+---------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a column with percent rank, the percent is calculate by dense_rank_number/total_item_number\n",
    "df4 = df.withColumn(\"percent_rank\", percent_rank().over(win_name_ordered))\n",
    "df4.printSchema()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- ntile_rank: integer (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|ntile_rank|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-07-12|   Bucket|    5|         1|\n",
      "|Alex|2018-02-18|   Gloves|    5|         1|\n",
      "|Alex|2018-09-26|Sandpaper|   10|         1|\n",
      "|Alex|2018-04-02|   Ladder|   20|         2|\n",
      "|Alex|2018-06-22|    Stool|   20|         2|\n",
      "|Alex|2018-03-03|  Brushes|   30|         2|\n",
      "|Alex|2018-12-09|   Vacuum|   40|         3|\n",
      "|Alex|2018-10-10|    Paint|   80|         3|\n",
      "| Bob|2018-07-12|   Bucket|    5|         1|\n",
      "| Bob|2018-02-18|   Gloves|    5|         1|\n",
      "| Bob|2018-09-26|Sandpaper|   10|         2|\n",
      "| Bob|2018-03-03|  Brushes|   30|         2|\n",
      "| Bob|2018-12-09|   Vacuum|   40|         3|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a column with ntile\n",
    "# here we set n=3, which means we divide each window into 3 parts. The rows in the 1st part will get 1 as ntile_rank,\n",
    "# The rows in the 2n part will get 2, etc.\n",
    "df4 = df.withColumn(\"ntile_rank\", ntile(3).over(win_name_ordered))\n",
    "df4.printSchema()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Analytic function examples\n",
    "In this example, we show how to use analytic functions on ordered frame\n",
    "- cume_dist\n",
    "- lag\n",
    "- lead\n",
    "\n",
    "Note all above window functions require that the **frame are ordered**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- cumulative_distribution: double (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+-----------------------+\n",
      "|name|      date|  product|price|cumulative_distribution|\n",
      "+----+----------+---------+-----+-----------------------+\n",
      "|Alex|2018-07-12|   Bucket|    5|                   0.25|\n",
      "|Alex|2018-02-18|   Gloves|    5|                   0.25|\n",
      "|Alex|2018-09-26|Sandpaper|   10|                  0.375|\n",
      "|Alex|2018-04-02|   Ladder|   20|                  0.625|\n",
      "|Alex|2018-06-22|    Stool|   20|                  0.625|\n",
      "|Alex|2018-03-03|  Brushes|   30|                   0.75|\n",
      "|Alex|2018-12-09|   Vacuum|   40|                  0.875|\n",
      "|Alex|2018-10-10|    Paint|   80|                    1.0|\n",
      "| Bob|2018-07-12|   Bucket|    5|                    0.4|\n",
      "| Bob|2018-02-18|   Gloves|    5|                    0.4|\n",
      "| Bob|2018-09-26|Sandpaper|   10|                    0.6|\n",
      "| Bob|2018-03-03|  Brushes|   30|                    0.8|\n",
      "| Bob|2018-12-09|   Vacuum|   40|                    1.0|\n",
      "+----+----------+---------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column that shows the cumulative_distribution \n",
    "df_cume_dist = df.withColumn(\"cumulative_distribution\", cume_dist().over(win_name_ordered))\n",
    "df_cume_dist.printSchema()\n",
    "df_cume_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- lag: long (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+----+\n",
      "|name|      date|  product|price| lag|\n",
      "+----+----------+---------+-----+----+\n",
      "|Alex|2018-07-12|   Bucket|    5|null|\n",
      "|Alex|2018-02-18|   Gloves|    5|null|\n",
      "|Alex|2018-09-26|Sandpaper|   10|null|\n",
      "|Alex|2018-04-02|   Ladder|   20|   5|\n",
      "|Alex|2018-06-22|    Stool|   20|   5|\n",
      "|Alex|2018-03-03|  Brushes|   30|  10|\n",
      "|Alex|2018-12-09|   Vacuum|   40|  20|\n",
      "|Alex|2018-10-10|    Paint|   80|  20|\n",
      "| Bob|2018-07-12|   Bucket|    5|null|\n",
      "| Bob|2018-02-18|   Gloves|    5|null|\n",
      "| Bob|2018-09-26|Sandpaper|   10|null|\n",
      "| Bob|2018-03-03|  Brushes|   30|   5|\n",
      "| Bob|2018-12-09|   Vacuum|   40|   5|\n",
      "+----+----------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column that shows the lag value by using price.\n",
    "# The lag function takes a column name (e.g. price) and an offset (3). \n",
    "# note if we set offset as 2, the first two row of lag is null, and the third rows gets the first row value of the\n",
    "# price column. If we set offset as 3, the first three rows will be null, and the fourth rows get the first row\n",
    "# value.\n",
    "df_lag = df.withColumn(\"lag\", lag(\"price\", 3).over(win_name_ordered))\n",
    "df_lag.printSchema()\n",
    "df_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- lead: long (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+----+\n",
      "|name|      date|  product|price|lead|\n",
      "+----+----------+---------+-----+----+\n",
      "|Alex|2018-07-12|   Bucket|    5|  20|\n",
      "|Alex|2018-02-18|   Gloves|    5|  20|\n",
      "|Alex|2018-09-26|Sandpaper|   10|  30|\n",
      "|Alex|2018-04-02|   Ladder|   20|  40|\n",
      "|Alex|2018-06-22|    Stool|   20|  80|\n",
      "|Alex|2018-03-03|  Brushes|   30|null|\n",
      "|Alex|2018-12-09|   Vacuum|   40|null|\n",
      "|Alex|2018-10-10|    Paint|   80|null|\n",
      "| Bob|2018-07-12|   Bucket|    5|  30|\n",
      "| Bob|2018-02-18|   Gloves|    5|  40|\n",
      "| Bob|2018-09-26|Sandpaper|   10|null|\n",
      "| Bob|2018-03-03|  Brushes|   30|null|\n",
      "| Bob|2018-12-09|   Vacuum|   40|null|\n",
      "+----+----------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column that pushes up 3 row of price column.\n",
    "# note if we set offset as 2, the last two row of lead is null in each partition, and the last third row gets the\n",
    "# value of last row of the price column. If we set offset as 3, the last three rows will be null, and the last\n",
    "# fourth rows get the last row value.\n",
    "df_lead = df.withColumn(\"lead\", lead(\"price\", 3).over(win_name_ordered))\n",
    "df_lead.printSchema()\n",
    "df_lead.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 1.\n",
    "Could you show the days from the previous purchase? Or the days before next purchase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|  lag_date|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-02-18|   Gloves|    5|      null|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-02-18|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-03-03|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-04-02|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-06-22|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-07-12|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-09-26|\n",
      "|Alex|2018-12-09|   Vacuum|   40|2018-10-10|\n",
      "| Bob|2018-02-18|   Gloves|    5|      null|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-02-18|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-03-03|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-07-12|\n",
      "| Bob|2018-12-09|   Vacuum|   40|2018-09-26|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n",
      "+----+----------+---------+-----+----------+---------------------------+\n",
      "|name|      date|  product|price|  lag_date|days_from_previous_purchase|\n",
      "+----+----------+---------+-----+----------+---------------------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|      null|                       null|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-02-18|                         13|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-03-03|                         30|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-04-02|                         81|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-06-22|                         20|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-07-12|                         76|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-09-26|                         14|\n",
      "|Alex|2018-12-09|   Vacuum|   40|2018-10-10|                         60|\n",
      "| Bob|2018-02-18|   Gloves|    5|      null|                       null|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-02-18|                         13|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-03-03|                        131|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-07-12|                         76|\n",
      "| Bob|2018-12-09|   Vacuum|   40|2018-09-26|                         74|\n",
      "+----+----------+---------+-----+----------+---------------------------+\n",
      "\n",
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price| lead_date|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-02-18|   Gloves|    5|2018-03-03|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-04-02|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-06-22|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-07-12|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-09-26|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-10-10|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-12-09|\n",
      "|Alex|2018-12-09|   Vacuum|   40|      null|\n",
      "| Bob|2018-02-18|   Gloves|    5|2018-03-03|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-07-12|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-09-26|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-12-09|\n",
      "| Bob|2018-12-09|   Vacuum|   40|      null|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n",
      "+----+----------+---------+-----+----------+-------------------------+\n",
      "|name|      date|  product|price| lead_date|days_before_next_purchase|\n",
      "+----+----------+---------+-----+----------+-------------------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|2018-03-03|                       13|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-04-02|                       30|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-06-22|                       81|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-07-12|                       20|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-09-26|                       76|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-10-10|                       14|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-12-09|                       60|\n",
      "|Alex|2018-12-09|   Vacuum|   40|      null|                     null|\n",
      "| Bob|2018-02-18|   Gloves|    5|2018-03-03|                       13|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-07-12|                      131|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-09-26|                       76|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-12-09|                       74|\n",
      "| Bob|2018-12-09|   Vacuum|   40|      null|                     null|\n",
      "+----+----------+---------+-----+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here we set lag on date column with offset 1, it means the second row will have the value of first row, then\n",
    "# apply the datediff function on this value with the current row date value, then we get days from the last\n",
    "# purchase.\n",
    "# Use the same logic by using lead, we get the days before next purchase, if we set offset as 2, we will get\n",
    "# the days before next 2 purchase\n",
    "\n",
    "# Step1 we create a new column called lag_date which is the date of previous purchase\n",
    "df_lag_day = df.withColumn(\"lag_date\",lag('date',1).over(win_name.orderBy(col('date'))))\n",
    "df_lag_day.show()\n",
    "\n",
    "# Step2, we create a column \"days\" by doing a date diff on \"date\" and \"lag_date\" \n",
    "df_diff=df_lag_day.withColumn(\"days_from_previous_purchase\",datediff(\"date\",\"lag_date\"))\n",
    "df_diff.show()\n",
    "\n",
    "# If you use lead\n",
    "# Step1 we create a new column called lead_date which is the date of previous purchase\n",
    "df_lead_day = df.withColumn(\"lead_date\",lead('date',1).over(win_name.orderBy(col('date'))))\n",
    "df_lead_day.show()\n",
    "\n",
    "# Step2, we create a column \"days\" by doing a date diff on \"date\" and \"lag_date\" \n",
    "df_diff=df_lead_day.withColumn(\"days_before_next_purchase\",datediff(\"lead_date\",\"date\"))\n",
    "df_diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "|name|      date|  product|price|days_from_last_purchase|days_before_next_purchase|\n",
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|                   null|                       13|\n",
      "|Alex|2018-03-03|  Brushes|   30|                     13|                       30|\n",
      "|Alex|2018-04-02|   Ladder|   20|                     30|                       81|\n",
      "|Alex|2018-06-22|    Stool|   20|                     81|                       20|\n",
      "|Alex|2018-07-12|   Bucket|    5|                     20|                       76|\n",
      "|Alex|2018-09-26|Sandpaper|   10|                     76|                       14|\n",
      "|Alex|2018-10-10|    Paint|   80|                     14|                       60|\n",
      "|Alex|2018-12-09|   Vacuum|   40|                     60|                     null|\n",
      "| Bob|2018-02-18|   Gloves|    5|                   null|                       13|\n",
      "| Bob|2018-03-03|  Brushes|   30|                     13|                      131|\n",
      "| Bob|2018-07-12|   Bucket|    5|                    131|                       76|\n",
      "| Bob|2018-09-26|Sandpaper|   10|                     76|                       74|\n",
      "| Bob|2018-12-09|   Vacuum|   40|                     74|                     null|\n",
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can do it in one line by putting lag function as parameter in datediff\n",
    "df_final=df.withColumn('days_from_last_purchase', datediff('date', lag('date', 1).over(win_name.orderBy(col('date'))))) \\\n",
    "        .withColumn('days_before_next_purchase', datediff(lead('date', 1).over(win_name.orderBy(col('date'))), 'date'))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Aggregation function example \n",
    "\n",
    "Show aggregation functions on ordered frame and basic partitionBy frame\n",
    "- avg/mean\n",
    "- min\n",
    "- max\n",
    "- sum\n",
    "\n",
    "In df1, we use a partition window specification, so the result is the same for all rows that are in the same partition.\n",
    "\n",
    "In df2, we use an ordered window specification, the result is different for each rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----+---+---+---+-----------+------------------------------------------------------------------+\n",
      "|name|date      |product  |price|avg  |sum|min|max|item_number|item_list                                                         |\n",
      "+----+----------+---------+-----+-----+---+---+---+-----------+------------------------------------------------------------------+\n",
      "|Alex|2018-10-10|Paint    |80   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-04-02|Ladder   |20   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-06-22|Stool    |20   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-12-09|Vacuum   |40   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-07-12|Bucket   |5    |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-02-18|Gloves   |5    |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-03-03|Brushes  |30   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-09-26|Sandpaper|10   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Bob |2018-12-09|Vacuum   |40   |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-07-12|Bucket   |5    |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-02-18|Gloves   |5    |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-03-03|Brushes  |30   |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-09-26|Sandpaper|10   |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "+----+----------+---------+-----+-----+---+---+---+-----------+------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:==================================================>    (91 + 3) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------+-----------+-----------+-----------+-------------------+------------------------------------------------------------------+\n",
      "|name|date      |product  |price|avg_to_date|sum_to_date|max_to_date|min_to_date|item_number_to_date|item_list_to_date                                                 |\n",
      "+----+----------+---------+-----+-----------+-----------+-----------+-----------+-------------------+------------------------------------------------------------------+\n",
      "|Alex|2018-02-18|Gloves   |5    |5.0        |5          |5          |5          |1                  |[Gloves]                                                          |\n",
      "|Alex|2018-03-03|Brushes  |30   |17.5       |35         |30         |30         |2                  |[Gloves, Brushes]                                                 |\n",
      "|Alex|2018-04-02|Ladder   |20   |18.33      |55         |30         |30         |3                  |[Gloves, Brushes, Ladder]                                         |\n",
      "|Alex|2018-06-22|Stool    |20   |18.75      |75         |30         |30         |4                  |[Gloves, Brushes, Ladder, Stool]                                  |\n",
      "|Alex|2018-07-12|Bucket   |5    |16.0       |80         |30         |30         |5                  |[Gloves, Brushes, Ladder, Stool, Bucket]                          |\n",
      "|Alex|2018-09-26|Sandpaper|10   |15.0       |90         |30         |30         |6                  |[Gloves, Brushes, Ladder, Stool, Bucket, Sandpaper]               |\n",
      "|Alex|2018-10-10|Paint    |80   |24.29      |170        |80         |80         |7                  |[Gloves, Brushes, Ladder, Stool, Bucket, Sandpaper, Paint]        |\n",
      "|Alex|2018-12-09|Vacuum   |40   |26.25      |210        |80         |80         |8                  |[Gloves, Brushes, Ladder, Stool, Bucket, Sandpaper, Paint, Vacuum]|\n",
      "|Bob |2018-02-18|Gloves   |5    |5.0        |5          |5          |5          |1                  |[Gloves]                                                          |\n",
      "|Bob |2018-03-03|Brushes  |30   |17.5       |35         |30         |30         |2                  |[Gloves, Brushes]                                                 |\n",
      "|Bob |2018-07-12|Bucket   |5    |13.33      |40         |30         |30         |3                  |[Gloves, Brushes, Bucket]                                         |\n",
      "|Bob |2018-09-26|Sandpaper|10   |12.5       |50         |30         |30         |4                  |[Gloves, Brushes, Bucket, Sandpaper]                              |\n",
      "|Bob |2018-12-09|Vacuum   |40   |18.0       |90         |40         |40         |5                  |[Gloves, Brushes, Bucket, Sandpaper, Vacuum]                      |\n",
      "+----+----------+---------+-----+-----------+-----------+-----------+-----------+-------------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We apply the aggregation function on the window specification that only has partition, so the \n",
    "# result is the same for all rows that are in the same \n",
    "df1 = df.withColumn(\"avg\", avg(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"sum\", sum(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"min\", min(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"max\", max(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"item_number\", count(\"*\").over(win_name)) \\\n",
    "        .withColumn(\"item_list\", collect_list(col(\"product\")).over(win_name))\n",
    "   \n",
    "df1.show(truncate=False)\n",
    "\n",
    "# if we apply aggregation function on a windows spec with order, you will get a cumulative result for each rows\n",
    "df2 = df.withColumn('avg_to_date', round(avg('price').over(win_name_ordered_by_date), 2)) \\\n",
    "        .withColumn('sum_to_date', sum('price').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn('max_to_date', max('price').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn('min_to_date', max('price').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn('item_number_to_date', count('*').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn(\"item_list_to_date\", collect_list(col(\"product\")).over(win_name_ordered_by_date))\n",
    "\n",
    "   \n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Range window specifications example\n",
    "\n",
    "Range window specification will create a sub window inside the main window (created by Window.partitionBy). **Only aggregation functions can apply over range window. Rank or analytic function will raise errors**. To build range window specifications, we need to use the two following functions \n",
    "- rowsBetween(start:Long,end:Long)-> WindowSpec : Here start, end are the index of rows relative to current rows, -1 means 1 row before current row, 1 mean 1 row after current row\n",
    "- rangeBetween(start:Long, end:Long)-> WindowSpec : The start, end boundary in rangeBetween is based on row value relative to currentRow. The value definition of the constant values used in range functions:\n",
    "   - Window.currentRow = 0\n",
    "   - Window.unboundedPreceding = Long.MinValue\n",
    "   - Window.unboundedFollowing = Long.MaxValue\n",
    "\n",
    "The (start, end) index are all inclusive. Their value can be \n",
    "- Window.unboundedPreceding\n",
    "- Window.unboundedFollowing\n",
    "- Window.currentRow. \n",
    "- Or a value relative to Window.currentRow, either negative or positive.\n",
    "\n",
    "Some examples of rowsBetween:\n",
    "- rowsBetween(Window.currentRow, 2): From current row to the next 2 rows \n",
    "- rowsBetween(-3, Window.currentRow): From the previous 3 rows to the current row. \n",
    "- rowsBetween(-1, 2): Frame contains previous row, current row and the next 2 rows \n",
    "- rowsBetween(Window.currentRow, Window.unboundedFollowing): From current row to all next rows \n",
    "- rowsBetween(Window.unboundedPreceding, Window.currentRow): From all previous rows to the current row. \n",
    "- rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing): all rows in the window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 86400 seconds in a day\n",
    "def day_to_seconds(day_num: int):\n",
    "    return day_num * 86400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1  rowsBetween example\n",
    "\n",
    "rowsBetween uses current row as base index (i.e. 0), and offset to specify start or end.\n",
    "- -1: one row before current row\n",
    "- 0: current row\n",
    "- 1: one row after current row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------+\n",
      "|name|date      |product  |price|max_of_last2|\n",
      "+----+----------+---------+-----+------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |5           |\n",
      "|Alex|2018-02-18|Gloves   |5    |5           |\n",
      "|Alex|2018-09-26|Sandpaper|10   |10          |\n",
      "|Alex|2018-04-02|Ladder   |20   |20          |\n",
      "|Alex|2018-06-22|Stool    |20   |20          |\n",
      "|Alex|2018-03-03|Brushes  |30   |30          |\n",
      "|Alex|2018-12-09|Vacuum   |40   |40          |\n",
      "|Alex|2018-10-10|Paint    |80   |80          |\n",
      "|Bob |2018-07-12|Bucket   |5    |5           |\n",
      "|Bob |2018-02-18|Gloves   |5    |5           |\n",
      "|Bob |2018-09-26|Sandpaper|10   |10          |\n",
      "|Bob |2018-03-03|Brushes  |30   |30          |\n",
      "|Bob |2018-12-09|Vacuum   |40   |40          |\n",
      "+----+----------+---------+-----+------------+\n",
      "\n",
      "+----+----------+---------+-----+-------------------------+\n",
      "|name|date      |product  |price|max_from_previous_to_next|\n",
      "+----+----------+---------+-----+-------------------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |5                        |\n",
      "|Alex|2018-02-18|Gloves   |5    |10                       |\n",
      "|Alex|2018-09-26|Sandpaper|10   |20                       |\n",
      "|Alex|2018-04-02|Ladder   |20   |20                       |\n",
      "|Alex|2018-06-22|Stool    |20   |30                       |\n",
      "|Alex|2018-03-03|Brushes  |30   |40                       |\n",
      "|Alex|2018-12-09|Vacuum   |40   |80                       |\n",
      "|Alex|2018-10-10|Paint    |80   |80                       |\n",
      "|Bob |2018-07-12|Bucket   |5    |5                        |\n",
      "|Bob |2018-02-18|Gloves   |5    |10                       |\n",
      "|Bob |2018-09-26|Sandpaper|10   |30                       |\n",
      "|Bob |2018-03-03|Brushes  |30   |40                       |\n",
      "|Bob |2018-12-09|Vacuum   |40   |40                       |\n",
      "+----+----------+---------+-----+-------------------------+\n",
      "\n",
      "+----+----------+---------+-----+----------------+\n",
      "|name|date      |product  |price|max_of_following|\n",
      "+----+----------+---------+-----+----------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |80              |\n",
      "|Alex|2018-02-18|Gloves   |5    |80              |\n",
      "|Alex|2018-09-26|Sandpaper|10   |80              |\n",
      "|Alex|2018-04-02|Ladder   |20   |80              |\n",
      "|Alex|2018-06-22|Stool    |20   |80              |\n",
      "|Alex|2018-03-03|Brushes  |30   |80              |\n",
      "|Alex|2018-12-09|Vacuum   |40   |80              |\n",
      "|Alex|2018-10-10|Paint    |80   |80              |\n",
      "|Bob |2018-07-12|Bucket   |5    |40              |\n",
      "|Bob |2018-02-18|Gloves   |5    |40              |\n",
      "|Bob |2018-09-26|Sandpaper|10   |40              |\n",
      "|Bob |2018-03-03|Brushes  |30   |40              |\n",
      "|Bob |2018-12-09|Vacuum   |40   |40              |\n",
      "+----+----------+---------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# last 2 row(current and the row before it) range window specification\n",
    "# Below range window takes the current row and the row before it. So it's the last 2. \n",
    "last2 = win_name_ordered.rowsBetween(-1, Window.currentRow)\n",
    "df.withColumn(\"max_of_last2\", max(\"price\").over(last2)).show(truncate=False)\n",
    "\n",
    "# Below range window takes the row before, current row, and the row after it\n",
    "privious_to_next = win_name_ordered.rowsBetween(-1,1)\n",
    "df.withColumn(\"max_from_previous_to_next\", max(\"price\").over(privious_to_next)).show(truncate=False)\n",
    "\n",
    "# max of all following row\n",
    "# Below range takes the current row and all the row behind it that are in the same partition.\n",
    "following = win_name_ordered.rowsBetween(Window.currentRow, Window.unboundedFollowing),\n",
    "df.withColumn(\"max_of_following\", max(\"price\").over(following)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2  rangeBetween example\n",
    "\n",
    "rowsBetween uses current **row value** as base index (i.e. 0), and offset to specify start or end.\n",
    "- -666: all the row that contains value is < current row value and > current_row_value - 666 \n",
    "- 0: current row\n",
    "- 666: all the row that contains value is > current row value and < current_row_value + 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+-----+----------+\n",
      "|name|date      |product|price|unix_date |\n",
      "+----+----------+-------+-----+----------+\n",
      "|Alex|2018-10-10|Paint  |80   |1539122400|\n",
      "|Alex|2018-04-02|Ladder |20   |1522620000|\n",
      "|Alex|2018-06-22|Stool  |20   |1529618400|\n",
      "|Alex|2018-12-09|Vacuum |40   |1544310000|\n",
      "|Alex|2018-07-12|Bucket |5    |1531346400|\n",
      "+----+----------+-------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert string date column to unix timestamp\n",
    "\n",
    "df1 = df.withColumn(\"unix_date\", unix_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "    \n",
    "df1.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz2, how to get the avg  price of sold proucts of the last 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp4 create a column that shows last 30 day avg before current row date\n",
      "+----+----------+---------+-----+----------+----------------+\n",
      "|name|date      |product  |price|unix_date |30day_moving_avg|\n",
      "+----+----------+---------+-----+----------+----------------+\n",
      "|Alex|2018-02-18|Gloves   |5    |1518908400|5.0             |\n",
      "|Alex|2018-03-03|Brushes  |30   |1520031600|17.5            |\n",
      "|Alex|2018-04-02|Ladder   |20   |1522620000|25.0            |\n",
      "|Alex|2018-06-22|Stool    |20   |1529618400|20.0            |\n",
      "|Alex|2018-07-12|Bucket   |5    |1531346400|12.5            |\n",
      "|Alex|2018-09-26|Sandpaper|10   |1537912800|10.0            |\n",
      "|Alex|2018-10-10|Paint    |80   |1539122400|45.0            |\n",
      "|Alex|2018-12-09|Vacuum   |40   |1544310000|40.0            |\n",
      "|Bob |2018-02-18|Gloves   |5    |1518908400|5.0             |\n",
      "|Bob |2018-03-03|Brushes  |30   |1520031600|17.5            |\n",
      "+----+----------+---------+-----+----------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We create a specific range window, the end is the date of current row, the start is the date 30 days \n",
    "# before current row\n",
    "# Here 0 is the relative unix_date of current row, the frame boundary of rangeBetween(-day_to_seconds(30), 0)\n",
    "# For example, row \"Alex|2018-02-18|Gloves |5 |1518908400|\" will be (1518908400-(30*86400),1518908400). All rows that\n",
    "# have unix_date column value in this frame boundary will be included in the frame.\n",
    "range_30 = win_name.orderBy(col(\"unix_date\")).rangeBetween(-day_to_seconds(30), 0)\n",
    "df2 = df1.withColumn(\"30day_moving_avg\", avg(\"price\").over(range_30))\n",
    "print(\"Exp4 create a column that shows last 30 day avg before current row date\")\n",
    "df2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz3, how to get the avg of 30 day before and 15 days after the current row date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+----------------+------------------+\n",
      "|name|date      |product  |price|unix_date |45day_moving_avg|45day_moving_std  |\n",
      "+----+----------+---------+-----+----------+----------------+------------------+\n",
      "|Alex|2018-02-18|Gloves   |5    |1518908400|17.5            |17.67766952966369 |\n",
      "|Alex|2018-03-03|Brushes  |30   |1520031600|17.5            |17.67766952966369 |\n",
      "|Alex|2018-04-02|Ladder   |20   |1522620000|25.0            |7.0710678118654755|\n",
      "|Alex|2018-06-22|Stool    |20   |1529618400|20.0            |null              |\n",
      "|Alex|2018-07-12|Bucket   |5    |1531346400|12.5            |10.606601717798213|\n",
      "|Alex|2018-09-26|Sandpaper|10   |1537912800|45.0            |49.49747468305833 |\n",
      "|Alex|2018-10-10|Paint    |80   |1539122400|45.0            |49.49747468305833 |\n",
      "|Alex|2018-12-09|Vacuum   |40   |1544310000|40.0            |null              |\n",
      "|Bob |2018-02-18|Gloves   |5    |1518908400|17.5            |17.67766952966369 |\n",
      "|Bob |2018-03-03|Brushes  |30   |1520031600|17.5            |17.67766952966369 |\n",
      "+----+----------+---------+-----+----------+----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that stddev of some row will return null. Because it requires at least two\n",
    "# observations to calculate standard deviation.\n",
    "range_45 = win_name.orderBy(\"unix_date\").rangeBetween(-day_to_seconds(30), day_to_seconds(15))\n",
    "df3 = df1.withColumn(\"45day_moving_avg\", avg(\"price\").over(range_45)) \\\n",
    "        .withColumn(\"45day_moving_std\", stddev(\"price\").over(range_45))\n",
    "df3.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz4, how to get the median of a window\n",
    "\n",
    "mean(avg) and median are commonly used in statistics. \n",
    "\n",
    "- mean is cheap to calculate, but outliers can have large effect. For example, the income of population, if we have 9 people has 10 dollar, and 1 person has 1010 dollar. The mean is 1100/10= 110. It does not represent any group's income. \n",
    "- Median is expansive to calculate. But in certain cases median are more robust comparing to mean, since it will filter out outlier values. If we retake the previous example, the median will be 10 dollar, which represent a group's income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------------------------+--------------+\n",
      "|name|date      |product  |price|price_list                    |rolling_median|\n",
      "+----+----------+---------+-----+------------------------------+--------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |[5, 5]                        |5             |\n",
      "|Alex|2018-02-18|Gloves   |5    |[5, 5]                        |5             |\n",
      "|Alex|2018-09-26|Sandpaper|10   |[5, 5, 10]                    |5             |\n",
      "|Alex|2018-04-02|Ladder   |20   |[5, 5, 10, 20, 20]            |10            |\n",
      "|Alex|2018-06-22|Stool    |20   |[5, 5, 10, 20, 20]            |10            |\n",
      "|Alex|2018-03-03|Brushes  |30   |[5, 5, 10, 20, 20, 30]        |20            |\n",
      "|Alex|2018-12-09|Vacuum   |40   |[5, 5, 10, 20, 20, 30, 40]    |20            |\n",
      "|Alex|2018-10-10|Paint    |80   |[5, 5, 10, 20, 20, 30, 40, 80]|20            |\n",
      "|Bob |2018-07-12|Bucket   |5    |[5, 5]                        |5             |\n",
      "|Bob |2018-02-18|Gloves   |5    |[5, 5]                        |5             |\n",
      "|Bob |2018-09-26|Sandpaper|10   |[5, 5, 10]                    |5             |\n",
      "|Bob |2018-03-03|Brushes  |30   |[5, 5, 10, 30]                |10            |\n",
      "|Bob |2018-12-09|Vacuum   |40   |[5, 5, 10, 30, 40]            |10            |\n",
      "+----+----------+---------+-----+------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win1=Window.partitionBy('name')\n",
    "win2=Window.partitionBy('name').orderBy('price')\n",
    "\n",
    "\n",
    "# In this example, we calculate a rolling median, because for each row, the price_list grows a little bit.\n",
    "df1=df.withColumn(\"price_list\",collect_list('price').over(win2)) \\\n",
    "      .withColumn(\"rolling_median\",element_at(\"price_list\",(size(\"price_list\")/2+1).cast(\"int\")))\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+--------------------+-------------+\n",
      "|name|      date|  product|price|          price_list|global_median|\n",
      "+----+----------+---------+-----+--------------------+-------------+\n",
      "|Alex|2018-10-10|    Paint|   80|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-04-02|   Ladder|   20|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-06-22|    Stool|   20|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-12-09|   Vacuum|   40|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-07-12|   Bucket|    5|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-02-18|   Gloves|    5|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-03-03|  Brushes|   30|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-09-26|Sandpaper|   10|[5, 5, 10, 20, 20...|           20|\n",
      "| Bob|2018-12-09|   Vacuum|   40|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-07-12|   Bucket|    5|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-02-18|   Gloves|    5|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-03-03|  Brushes|   30|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-09-26|Sandpaper|   10|  [5, 5, 10, 30, 40]|           10|\n",
      "+----+----------+---------+-----+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this example, we calculate a global median for each window\n",
    "# note, as the window specification only has partition, does not have order, the collect list need to be sorted\n",
    "\n",
    "df2=df.withColumn(\"price_list\",sort_array(collect_list('price').over(win1))) \\\n",
    "      .withColumn(\"global_median\",element_at(\"price_list\",(size(\"price_list\")/2+1).cast(\"int\")))\n",
    "  \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|name|          price_list|\n",
      "+----+--------------------+\n",
      "|Alex|[5, 5, 10, 20, 20...|\n",
      "| Bob|  [5, 5, 10, 30, 40]|\n",
      "+----+--------------------+\n",
      "\n",
      "+----+--------------------+------+\n",
      "|name|          price_list|median|\n",
      "+----+--------------------+------+\n",
      "|Alex|[5, 5, 10, 20, 20...|    20|\n",
      "| Bob|  [5, 5, 10, 30, 40]|    10|\n",
      "+----+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also use groupBy\n",
    "\n",
    "df3=df.groupBy(\"name\").agg(sort_array(collect_list(\"price\")).alias(\"price_list\"))\n",
    "df3.show()\n",
    "\n",
    "df4=df3.select(\"name\",\"price_list\",(element_at(\"price_list\", (size(\"price_list\")/2+1).cast(\"int\")).alias(\"median\")))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------------------------+------+\n",
      "|name|date      |product  |price|price_list                    |median|\n",
      "+----+----------+---------+-----+------------------------------+------+\n",
      "|Alex|2018-10-10|Paint    |80   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-04-02|Ladder   |20   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-06-22|Stool    |20   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-12-09|Vacuum   |40   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-07-12|Bucket   |5    |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-02-18|Gloves   |5    |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-03-03|Brushes  |30   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-09-26|Sandpaper|10   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Bob |2018-12-09|Vacuum   |40   |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-07-12|Bucket   |5    |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-02-18|Gloves   |5    |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-03-03|Brushes  |30   |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-09-26|Sandpaper|10   |[5, 5, 10, 30, 40]            |10    |\n",
      "+----+----------+---------+-----+------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The pyspark.sql.functions.broadcast(df) marks a DataFrame as small enough for use in broadcast joins.\n",
    "df.join(broadcast(df4), \"name\", \"inner\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}