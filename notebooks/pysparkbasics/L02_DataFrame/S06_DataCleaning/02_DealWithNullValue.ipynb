{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Deal with null values \n",
    "\n",
    "Null value is very common in data science. All your code should gracefully handle these null values. Three main strategy\n",
    "to handle null values:\n",
    "\n",
    "1. keep null values in data frame, all functions which worked with data frame need to handle the null value gracefully\n",
    "2. Remove all null values\n",
    "3. Use Imputation process to fill null values with non null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType\n",
    "from pyspark.sql.functions import lit, col, when, concat, udf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"RemoveDuplicates\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"RemoveDuplicates\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1') \\\n",
    "                      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.1 Handle null in data frame creation\n",
    "When you read source data from file, or you create dataframe from list. You can use schema to control how you want to handle the null values.\n",
    "\n",
    "In the schema definition, each column in a DataFrame has a **nullable property that can be set to True or False**. If nullable is set to False then the column cannot contain null values. \n",
    "\n",
    "In the following example, the column id and zip_code is set to nullable=False. If your data contains null in these two\n",
    "column, you can't create a dataframe with these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "field zip_code: This field is not nullable, but got None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-56f9aec7dd26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"population\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m    ])\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1388\u001b[0m                                 \"length of fields (%d)\" % (len(obj), len(verifiers))))\n\u001b[1;32m   1389\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m                     \u001b[0mverifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dict__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_nullability\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This field is not nullable, but got None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: field zip_code: This field is not nullable, but got None"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "        (1, None, \"STANDARD\", None, \"PR\", 30100),\n",
    "        (2, 704, None, \"PASEO COSTA DEL SUR\", \"PR\", None),\n",
    "        (3, 709, None, \"BDA SAN LUIS\", \"PR\", 3700),\n",
    "        (4, 76166, \"UNIQUE\", \"CINGULAR WIRELESS\", \"TX\", 84000),\n",
    "        (5, 76177, \"STANDARD\", None, \"TX\", None)\n",
    "    ]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"zip_code\", LongType(), False),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"population\", IntegerType(), True),\n",
    "    ])\n",
    "df1 = spark.createDataFrame(data1, schema=schema)\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the error message is very clear, we can't have null value in non nullable column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- zip_code: long (nullable = false)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's replace None with 703, you will see we can get our datafrmae correctly\n",
    "data2 = [\n",
    "        (1, 703, \"STANDARD\", None, \"PR\", 30100),\n",
    "        (2, 704, None, \"PASEO COSTA DEL SUR\", \"PR\", None),\n",
    "        (3, 709, None, \"BDA SAN LUIS\", \"PR\", 3700),\n",
    "        (4, 76166, \"UNIQUE\", \"CINGULAR WIRELESS\", \"TX\", 84000),\n",
    "        (5, 76177, \"STANDARD\", None, \"TX\", None)\n",
    "    ]\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema=schema)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Handle null value with built in spark function\n",
    "The built in spark functions handle null value gracefully, so we dont need to worry null.\n",
    "\n",
    "- isNull(): called by a column, return true if value is null. This function can be used in filter to remove rows with null values\n",
    "\n",
    "**The default behavior of spark built in function to handle null value is that if either, or both, of the operands \n",
    "column are null, then function returns null**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+------------+\n",
      "| id|zip_code|    type|               city|state|population|type_is_null|\n",
      "+---+--------+--------+-------------------+-----+----------+------------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100|       false|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|        true|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|        true|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|       false|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|       false|\n",
      "+---+--------+--------+-------------------+-----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can check if a value of a column is null or not.\n",
    "df_check_null=df2.withColumn(\"type_is_null\",df2.type.isNull())\n",
    "df_check_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "| id|zip_code|    type|               city|state|population|       city_and_type|\n",
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "|  1|     704|STANDARD|               null|   PR|     30100|                null|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|                null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|                null|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|CINGULAR WIRELESS...|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|                null|\n",
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if either, or both, of the operands column are null, then function returns null.\n",
    " \n",
    "df2.withColumn(\"city_and_type\", concat(df.city, df.type)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+-----------------------+\n",
      "|id |zip_code|type    |city               |state|population|city_and_type          |\n",
      "+---+--------+--------+-------------------+-----+----------+-----------------------+\n",
      "|1  |704     |STANDARD|null               |PR   |30100     | STANDARD              |\n",
      "|2  |704     |null    |PASEO COSTA DEL SUR|PR   |null      |PASEO COSTA DEL SUR    |\n",
      "|3  |709     |null    |BDA SAN LUIS       |PR   |3700      |BDA SAN LUIS           |\n",
      "|4  |76166   |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |CINGULAR WIRELESSUNIQUE|\n",
      "|5  |76177   |STANDARD|null               |TX   |null      | STANDARD              |\n",
      "+---+--------+--------+-------------------+-----+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can overwrite the default handling of the null value by adding null testing logic by ourself.\n",
    "# if city is null and type is not null, we replace city value by \" \"\n",
    "# if city is not null and type is null, we replace type value by \" \"\n",
    "# if both null, return null\n",
    "# if both not null, concat normally\n",
    "df2.withColumn(\"city_and_type\",\n",
    "                  when(df.city.isNull() & ~df.type.isNull(), concat(lit(\" \"), df.type))\n",
    "                  .when(~df.city.isNull() & df.type.isNull(), concat(df.city, lit(\" \")))\n",
    "                  .when(df.city.isNull() & df.type.isNull(), None)\n",
    "                  .otherwise(concat(df.city, df.type))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 Equality check with null values\n",
    "You can noticed in below example, if either, or both, of the operands are null, then == returns null, not a boolean.\n",
    "\n",
    "In some case, you want the == behave like this :\n",
    "- When one value is null and the other is not null, return False\n",
    "- When both values are null, return True\n",
    "\n",
    "eqNullSafe(col_name): It's called by a column, takes a column as argument and produce a bool value by using above rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "| id|zip_code|    type|               city|state|population|type_test|type_equality_test|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100| STANDARD|              true|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null| STANDARD|              null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700| STANDARD|              null|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000| STANDARD|             false|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null| STANDARD|              true|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can notice the type_equality_test column contains null values\n",
    "df2.withColumn(\"type_test\", lit(\"STANDARD\")) \\\n",
    "   .withColumn(\"type_equality_test\", df2.type == col(\"type_test\")) \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "| id|zip_code|    type|               city|state|population|type_test|type_equality_test|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100| STANDARD|              true|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null| STANDARD|             false|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700| STANDARD|             false|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000| STANDARD|             false|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null| STANDARD|              true|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now with our null testing logic, we don't have null in column type_equality_test anymore\n",
    "\n",
    "df3 = df2.withColumn(\"type_test\", lit(\"STANDARD\")) \\\n",
    "          .withColumn(\"type_equality_test\", \n",
    "                         when(df2.type.isNull() & col(\"type_test\").isNull(), True)\n",
    "                         .when(df2.type.isNull() | col(\"type_test\").isNull(), False)\n",
    "                         .otherwise(df2.type == col(\"type_test\")))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
