{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Deal with null values \n",
    "\n",
    "Null value is very common in data science. All your code should gracefully handle these null values. Three main strategy\n",
    "to handle null values:\n",
    "\n",
    "1. keep null values in data frame, all functions which worked with data frame need to handle the null value gracefully\n",
    "2. Remove all null values\n",
    "3. Use Imputation process to fill null values with non null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType\n",
    "from pyspark.sql.functions import lit, col, when, concat, udf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"RemoveDuplicates\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"RemoveDuplicates\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1') \\\n",
    "                      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.1 Handle null in data frame creation\n",
    "When you read source data from file, or you create dataframe from list. You can use schema to control how you want to handle the null values.\n",
    "\n",
    "In the schema definition, each column in a DataFrame has a **nullable property that can be set to True or False**. If nullable is set to False then the column cannot contain null values. \n",
    "\n",
    "In the following example, the column id and zip_code is set to nullable=False. If your data contains null in these two\n",
    "column, you can't create a dataframe with these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "field zip_code: This field is not nullable, but got None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-790f860bcd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"population\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     ])\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1388\u001b[0m                                 \"length of fields (%d)\" % (len(obj), len(verifiers))))\n\u001b[1;32m   1389\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifier\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m                     \u001b[0mverifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dict__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_nullability\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This field is not nullable, but got None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: field zip_code: This field is not nullable, but got None"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "        (1, None, \"STANDARD\", None, \"PR\", 30100),\n",
    "        (2, 704, None, \"PASEO COSTA DEL SUR\", \"PR\", None),\n",
    "        (3, 709, None, \"BDA SAN LUIS\", \"PR\", 3700),\n",
    "        (4, 76166, \"UNIQUE\", \"CINGULAR WIRELESS\", \"TX\", 84000),\n",
    "        (5, 76177, \"STANDARD\", None, \"TX\", None)\n",
    "    ]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"zip_code\", LongType(), False),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"population\", IntegerType(), True),\n",
    "    ])\n",
    "df1 = spark.createDataFrame(data1, schema=schema)\n",
    "\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the error message is very clear, we can't have null value in non nullable column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- zip_code: long (nullable = false)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now let's replace None with 703, you will see we can get our datafrmae correctly\n",
    "data2 = [\n",
    "        (1, 703, \"STANDARD\", None, \"PR\", 30100),\n",
    "        (2, 704, None, \"PASEO COSTA DEL SUR\", \"PR\", None),\n",
    "        (3, 709, None, \"BDA SAN LUIS\", \"PR\", 3700),\n",
    "        (4, 76166, \"UNIQUE\", \"CINGULAR WIRELESS\", \"TX\", 84000),\n",
    "        (5, 76177, \"STANDARD\", None, \"TX\", None)\n",
    "    ]\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema=schema)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Handle null value with built in spark function\n",
    "The built in spark functions handle null value gracefully, so we dont need to worry null.\n",
    "\n",
    "- isNull(): called by a column, return true if value is null. This function can be used in filter to remove rows with null values\n",
    "\n",
    "**The default behavior of spark built in function to handle null value is that if either, or both, of the operands \n",
    "column are null, then function returns null**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+------------+\n",
      "| id|zip_code|    type|               city|state|population|type_is_null|\n",
      "+---+--------+--------+-------------------+-----+----------+------------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100|       false|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|        true|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|        true|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|       false|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|       false|\n",
      "+---+--------+--------+-------------------+-----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can check if a value of a column is null or not.\n",
    "df_check_null=df2.withColumn(\"type_is_null\",df2.type.isNull())\n",
    "df_check_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "| id|zip_code|    type|               city|state|population|       city_and_type|\n",
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "|  1|     704|STANDARD|               null|   PR|     30100|                null|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|                null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|                null|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|CINGULAR WIRELESS...|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|                null|\n",
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if either, or both, of the operands column are null, then function returns null.\n",
    " \n",
    "df2.withColumn(\"city_and_type\", concat(df.city, df.type)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+-----------------------+\n",
      "|id |zip_code|type    |city               |state|population|city_and_type          |\n",
      "+---+--------+--------+-------------------+-----+----------+-----------------------+\n",
      "|1  |704     |STANDARD|null               |PR   |30100     | STANDARD              |\n",
      "|2  |704     |null    |PASEO COSTA DEL SUR|PR   |null      |PASEO COSTA DEL SUR    |\n",
      "|3  |709     |null    |BDA SAN LUIS       |PR   |3700      |BDA SAN LUIS           |\n",
      "|4  |76166   |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |CINGULAR WIRELESSUNIQUE|\n",
      "|5  |76177   |STANDARD|null               |TX   |null      | STANDARD              |\n",
      "+---+--------+--------+-------------------+-----+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can overwrite the default handling of the null value by adding null testing logic by ourself.\n",
    "# if city is null and type is not null, we replace city value by \" \"\n",
    "# if city is not null and type is null, we replace type value by \" \"\n",
    "# if both null, return null\n",
    "# if both not null, concat normally\n",
    "df2.withColumn(\"city_and_type\",\n",
    "                  when(df.city.isNull() & ~df.type.isNull(), concat(lit(\" \"), df.type))\n",
    "                  .when(~df.city.isNull() & df.type.isNull(), concat(df.city, lit(\" \")))\n",
    "                  .when(df.city.isNull() & df.type.isNull(), None)\n",
    "                  .otherwise(concat(df.city, df.type))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 Equality check with null values\n",
    "You can noticed in below example, if either, or both, of the operands are null, then == returns null, not a boolean.\n",
    "\n",
    "In some case, you want the == behave like this :\n",
    "- When one value is null and the other is not null, return False\n",
    "- When both values are null, return True\n",
    "\n",
    "eqNullSafe(col_name): It's called by a column, takes a column as argument and produce a bool value by using above rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "| id|zip_code|    type|               city|state|population|type_test|type_equality_test|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100| STANDARD|              true|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null| STANDARD|              null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700| STANDARD|              null|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000| STANDARD|             false|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null| STANDARD|              true|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can notice the type_equality_test column contains null values\n",
    "df2.withColumn(\"type_test\", lit(\"STANDARD\")) \\\n",
    "   .withColumn(\"type_equality_test\", df2.type == col(\"type_test\")) \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "| id|zip_code|    type|               city|state|population|type_test|type_equality_test|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100| STANDARD|              true|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null| STANDARD|             false|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700| STANDARD|             false|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000| STANDARD|             false|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null| STANDARD|              true|\n",
      "+---+--------+--------+-------------------+-----+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now with our null testing logic, we don't have null in column type_equality_test anymore\n",
    "\n",
    "df3 = df2.withColumn(\"type_test\", lit(\"STANDARD\")) \\\n",
    "          .withColumn(\"type_equality_test\", \n",
    "                         when(df2.type.isNull() & col(\"type_test\").isNull(), True)\n",
    "                         .when(df2.type.isNull() | col(\"type_test\").isNull(), False)\n",
    "                         .otherwise(df2.type == col(\"type_test\")))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.4 Handle null value inside your udf\n",
    "\n",
    "We know that all spark built in functions can handle null value gracefully. How about the UDF that you defined by yourself? The answer is no, by default your UDF will not handle null value at all. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this function, we define a udf, which takes a string, and return \"hi\"+str\n",
    "@udf(returnType=StringType())\n",
    "def hi_city_bad(city_name: str) -> str:\n",
    "    return \"hi \" + city_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-23-b900aee5ce1b>\", line 4, in hi_city_bad\nTypeError: can only concatenate str (not \"NoneType\") to str\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6f202872e2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi_city\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi_city_bad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-23-b900aee5ce1b>\", line 4, in hi_city_bad\nTypeError: can only concatenate str (not \"NoneType\") to str\n"
     ]
    }
   ],
   "source": [
    "# now we try to use the above udf to create a new column. The result shows we have a type issues. The udf requires a string, \n",
    "# but we have null as input.  \n",
    "df2.withColumn(\"hi_city\", hi_city_bad(df2.city)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve the udf, we need to handle the none type. In below example, we just\n",
    "# use the default behaviour of spark builtin functions. If one of the input argument is null, we return null too.\n",
    "@udf(returnType=StringType())\n",
    "def hi_city(city_name: str) -> str:\n",
    "    return None if city_name is None else \"hi \" + city_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "| id|zip_code|    type|               city|state|population|             hi_city|\n",
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100|                null|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|hi PASEO COSTA DE...|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|     hi BDA SAN LUIS|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|hi CINGULAR WIRELESS|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|                null|\n",
      "+---+--------+--------+-------------------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This time, we get the new column hi_city. \n",
    "df2.withColumn(\"hi_city\", hi_city(df2.city)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.5 Remove null values\n",
    "\n",
    "Sometimes, you may not want to handle null values in your dataframe, so you need to remove them. Spark provide two methods to do so.\n",
    "\n",
    "- drop(how='any', thresh=None, subset=None): function is used to remove/drop rows with NULL values in DataFrame columns\n",
    "     It has three 3 optional parameters:\n",
    "        -- how: This takes values ‘any’ or ‘all’. By using ‘any’, drop a row if it contains NULLs on \n",
    "                any columns. By using ‘all’, drop a row only if all columns have NULL values. Default is ‘any’.\n",
    "        -- thresh: This takes int value, Drop rows that have less than thresh hold non-null values. Default is ‘None’.\n",
    "        -- subset: Use this to select the columns for NULL values. Default is ‘None.\n",
    "- df.dropna() : is equivalent to df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+-----------------+-----+----------+\n",
      "|id |zip_code|type  |city             |state|population|\n",
      "+---+--------+------+-----------------+-----+----------+\n",
      "|4  |76166   |UNIQUE|CINGULAR WIRELESS|TX   |84000     |\n",
      "+---+--------+------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we use drop() without arguments, it removes all rows that have null values\n",
    "# we only have one row left.    \n",
    "df2.na.drop().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|id |zip_code|type    |city               |state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|1  |704     |STANDARD|null               |PR   |30100     |\n",
      "|2  |704     |null    |PASEO COSTA DEL SUR|PR   |null      |\n",
      "|3  |709     |null    |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166   |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177   |STANDARD|null               |TX   |null      |\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use drop() with how=all arguments, it removes all rows which have null values on all columns\n",
    "df2.na.drop(how=\"all\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----------------+-----+----------+\n",
      "|id |zip_code|type    |city             |state|population|\n",
      "+---+--------+--------+-----------------+-----+----------+\n",
      "|1  |703     |STANDARD|null             |PR   |30100     |\n",
      "|4  |76166   |UNIQUE  |CINGULAR WIRELESS|TX   |84000     |\n",
      "+---+--------+--------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use drop() with subset arguments, it removes all rows that have null values on the selected columns\n",
    "df2.na.drop(subset=[\"population\", \"type\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----------------+-----+----------+\n",
      "|id |zip_code|type    |city             |state|population|\n",
      "+---+--------+--------+-----------------+-----+----------+\n",
      "|1  |703     |STANDARD|null             |PR   |30100     |\n",
      "|3  |709     |null    |BDA SAN LUIS     |PR   |3700      |\n",
      "|4  |76166   |UNIQUE  |CINGULAR WIRELESS|TX   |84000     |\n",
      "+---+--------+--------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here, we use thresh=1, it means drop() function allows one null row in the output result. Default value is none, no null row\n",
    "df2.na.drop(subset=[\"population\"],thresh=1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+-----------------+-----+----------+\n",
      "|id |zip_code|type  |city             |state|population|\n",
      "+---+--------+------+-----------------+-----+----------+\n",
      "|4  |76166   |UNIQUE|CINGULAR WIRELESS|TX   |84000     |\n",
      "+---+--------+------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note dropna() is equivalent to df.na.drop()\n",
    "# dropna() removes all rows that have null values\n",
    "df.dropna().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.6 Imputation of null value\n",
    "Sometimes, we can't just remove the rows that contain null value. Because the non null columns are too important to be removed. So we need to replace the null value with some value that will not affect the result of the statistic. We can this process Imputation.\n",
    "\n",
    "Spark provides two fucntion **fillna and fill** to do the imputation. These two functions are aliases of each other and returns the same results.\n",
    "- fillna(value, subset=None) : it replaces NULL/None with a specific value, It has two arguments:\n",
    "            -- value: Value should be the data type of int, long, float, string, or dict. Value specified here \n",
    "                            will replace the NULL/None values.\n",
    "            -- subset: This is optional, when used it should be the subset of the column names where \n",
    "                       you want to replace NULL/None values.\n",
    "- fill(value, subset=None)\n",
    "\n",
    "Note the most important part of the imputation is to choose the right imputation value that will not affect your analysis. Here, we does not address this problems. We just show how to insert the imputation value into the dataframe correctly.\n",
    "\n",
    "**The type of the imputation value must match the column type that we want to fill**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     703|STANDARD|               null|   PR|     30100|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|               null|   TX|         0|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use fill(value=) without giving column names, it replaces null by 0 for all integer columns that contains null.\n",
    "# Impute all integer column null value with 0\n",
    "df2.na.fill(value=0).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     704|STANDARD|               null|   PR|     30100|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|               null|   TX|      null|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note, in below example, nothing happens to the city column, because wrong value type\n",
    "df.na.fill(value=0, subset=[\"city\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     703|STANDARD|                   |   PR|     30100|\n",
      "|  2|     704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|     709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|                   |   TX|      null|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     703|STANDARD|                   |   PR|     30100|\n",
      "|  2|     704|        |PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|     709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|                   |   TX|      null|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use fill(subset=[]) to specify which we column we want to fill.\n",
    "# note below two operations return the same result.\n",
    "df2.na.fill(value=\" \").show()\n",
    "df2.na.fill(value=\" \", subset=[\"type\",\"city\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     703|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|            unknown|   TX|      null|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can fill with any string value as impuation value in a string column\n",
    "df2.na.fill(value=\"unknown\", subset=[\"city\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|            unknown|   TX|         0|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you have different imputation value for different column, we can use the following operations to fill multiple column with different values\n",
    "    \n",
    "# First solution is to use a dict as the value argument. The key of the dict is the column name, the value of the dict is the value \n",
    "# you want to fill.\n",
    "df.na.fill(value={\"city\": \"unknown\", \"population\": 0}).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+-----+----------+\n",
      "| id|zip_code|    type|               city|state|population|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "|  1|     704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|     704|    null|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|     709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|   76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|   76177|STANDARD|            unknown|   TX|         0|\n",
      "+---+--------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second solution is less elegant. We just use two fill to archive the same result as above function\n",
    "df.na.fill(\"unknown\", [\"city\"]).na.fill(0, [\"population\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
