{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Read write data from postgresql server\n",
    "\n",
    "The official [doc](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "\n",
    "# 3.1.1 Check your postgresql server connexion\n",
    "``` bash\n",
    "psql -h postgresql://postgresql-955091 -p 5432 -U pengfei -W test test\n",
    "pwd: test\n",
    "```\n",
    "1. show database list\n",
    "\\l\n",
    "\n",
    "# check the version of your postgresql server\n",
    "\n",
    "``` sql\n",
    "SELECT version();\n",
    "```\n",
    "\n",
    "# 3.1.2 Get your postgresql jdbc driver\n",
    "The maven dependencies of the postgresql driver\n",
    "\n",
    "```xml\n",
    "<dependency>\n",
    "    <groupId>org.postgresql</groupId>\n",
    "    <artifactId>postgresql</artifactId>\n",
    "    <version>42.2.24</version>\n",
    "</dependency>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType\n",
    "from pyspark.sql.functions import lit, col, when, concat, udf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ef9e7f5a-15fb-4a43-8c78-a8275b48a833;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.24 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.24/postgresql-42.2.24.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.2.24!postgresql.jar (134ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (28ms)\n",
      ":: resolution report :: resolve 773ms :: artifacts dl 167ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.2.24 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ef9e7f5a-15fb-4a43-8c78-a8275b48a833\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (1192kB/10ms)\n",
      "2022-04-04 13:26:38,180 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-04-04 13:26:40,922 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "local=False\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .config('spark.jars.packages', 'org.postgresql:postgresql:42.2.24') \\\n",
    "                  .appName(\"RemoveDuplicates\").getOrCreate()\n",
    "    db_url=\"jdbc:postgresql://localhost:5432/test\"\n",
    "    table_name=\"employee\"\n",
    "    user=\"pengfei\"\n",
    "    password=\"toto\"\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"RemoveDuplicates\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",os.environ['IMAGE_NAME']) \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') \\\n",
    "                      .getOrCreate()\n",
    "    db_url=\"jdbc:postgresql://postgresql-124499:5432/test\"\n",
    "    table_name=\"employee\"\n",
    "    user=\"user-pengfei\"\n",
    "    password=\"toto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- dept_creation_year: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018              |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010              |20         |M     |4000  |\n",
      "|3     |Williams|1              |2018              |21         |M     |1000  |\n",
      "|4     |Jones   |2              |2005              |31         |F     |2000  |\n",
      "|5     |Brown   |2              |2010              |30         |F     |-1    |\n",
      "|6     |Foobar  |2              |2010              |150        |F     |-1    |\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "emp = [(1, \"Smith\", -1, \"2018\", \"10\", \"M\", 3000),\n",
    "           (2, \"Rose\", 1, \"2010\", \"20\", \"M\", 4000),\n",
    "           (3, \"Williams\", 1, \"2018\", \"21\", \"M\", 1000),\n",
    "           (4, \"Jones\", 2, \"2005\", \"31\", \"F\", 2000),\n",
    "           (5, \"Brown\", 2, \"2010\", \"30\", \"F\", -1),\n",
    "           (6, \"Foobar\", 2, \"2010\", \"150\", \"F\", -1)\n",
    "           ]\n",
    "emp_col_names = [\"emp_id\", \"name\", \"superior_emp_id\", \"dept_creation_year\",\n",
    "                     \"emp_dept_id\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=emp, schema=emp_col_names)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 We write a spark dataframe to the postgresql database server as a table\n",
    "\n",
    "We have two ways to write dataframe via jdbc: \n",
    "\n",
    "1. Use df.write.format(\"jdbc).option(...).save()\n",
    "\n",
    "\n",
    "```python\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_url) \\\n",
    "    .option(\"dbtable\", \"emp2\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .save()\n",
    "```\n",
    "2. Use df.write.jdbc(...)\n",
    "``` python\n",
    "# postgresql connexion config\n",
    "db_url=\"jdbc:postgresql://postgresql-955091:5432/test\"\n",
    "table=\"employee\"\n",
    "user=\"pengfei\"\n",
    "password=\"test\"\n",
    "driver=\"org.postgresql.Driver\"\n",
    "# note the driver value need to be changed if you use other database\n",
    "# e.g. Mysql: com.mysql.jdbc.Driver\n",
    "#     postgresql: org.postgresql.Driver\n",
    "db_properties={\"user\": user, \"password\": password, \"driver\" : driver }\n",
    "df.write.jdbc(url=db_url,table=table,mode='overwrite',properties=db_properties)\n",
    "```\n",
    "Below code write a dataframe to a database server as a table\n",
    "\n",
    "We need to check if **the generated table has the same schema as the dataframe**\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "   table_name, \n",
    "   column_name, \n",
    "   data_type \n",
    "FROM \n",
    "   information_schema.columns\n",
    "WHERE \n",
    "   table_name = 'employee';\n",
    "```   \n",
    "\n",
    "We get below result, we noticed long is converted to bigint, string is converte to text.\n",
    "\n",
    "``` text\n",
    "table_name |    column_name     | data_type \n",
    "------------+--------------------+-----------\n",
    " employee   | superior_emp_id    | bigint\n",
    " employee   | emp_id             | bigint\n",
    " employee   | salary             | bigint\n",
    " employee   | gender             | text\n",
    " employee   | dept_creation_year | text\n",
    " employee   | name               | text\n",
    " employee   | emp_dept_id        | text\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postgresql connexion config\n",
    "driver=\"org.postgresql.Driver\"\n",
    "# note the driver value need to be changed if you use other database\n",
    "# e.g. Mysql: com.mysql.jdbc.Driver\n",
    "#     postgresql: org.postgresql.Driver\n",
    "db_properties={\"user\": user, \"password\": password, \"driver\" : driver }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use solution 1 to write to a table named emp2\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_url) \\\n",
    "    .option(\"dbtable\", \"emp2\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We use solution 2 to write to a table named employee\n",
    "# note that the db_properties is a dictionary that contains user and password\n",
    "df.write.jdbc(url=db_url,table=table_name,mode='overwrite',properties=db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.4 We read a table into a spark dataframe\n",
    "\n",
    "Similar to write, we have two solutions to read a table to a dataframe:\n",
    "1. Use spark.read.jdbc()\n",
    "2. Use spark.read.format(\"jdbc\").options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use solution 1 to generate a dataframe from a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|\n",
      "|     5|   Brown|              2|              2010|         30|     F|    -1|\n",
      "|     6|  Foobar|              2|              2010|        150|     F|    -1|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- dept_creation_year: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read1=spark.read.jdbc(url=db_url, table=table, properties=db_properties)\n",
    "df_read1.show()\n",
    "df_read1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use solution 2 to generate a dataframe from a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|\n",
      "|     5|   Brown|              2|              2010|         30|     F|    -1|\n",
      "|     6|  Foobar|              2|              2010|        150|     F|    -1|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- dept_creation_year: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read2=spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", db_url) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .load()\n",
    "\n",
    "df_read2.show()\n",
    "df_read2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Two important things:\n",
    "1. You need to add jdbc driver to your sparkcontext.\n",
    "   For example, in a notebooke, you can add .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') to the SparkSession.builder.\n",
    "   In a submit mode, you need to add options such as \"--driver-class-path path/to.jar --jars path/to.jar\" or \"--packages org.postgresql:postgresql:42.2.24\"\n",
    "2. When you use read or write you need to specify your jdbc driver type.\n",
    "   For example, .option(\"driver\", \"org.postgresql.Driver\")\n",
    "                .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "\n",
    "\n",
    "When to use --jars, and --packages? Check this [answer](https://stackoverflow.com/questions/51434808/spark-submit-packages-vs-jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
