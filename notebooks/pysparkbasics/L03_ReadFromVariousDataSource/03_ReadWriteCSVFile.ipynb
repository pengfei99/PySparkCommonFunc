{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3.3 Read write csv files\n",
    "The official doc can be found [here](https://spark.apache.org/docs/latest/sql-data-sources-csv.html). Apache Parquet file is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or programming language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/13 09:21:11 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/02/13 09:21:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/13 09:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .appName(\"ReadWriteCSV\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"ReadWriteCSV\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",os.environ[\"IMAGE_NAME\"]) \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config(\"spark.kubernetes.driver.pod.name\", os.environ[\"POD_NAME\"]) \\\n",
    "                      .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') \\\n",
    "                      .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.1 Read simple CSV file\n",
    "\n",
    "Ideally, the csv file should contain a header that describes the column names. Spark can use this header to infer a schema. In below example, we set header=True and inferSchema=True.\n",
    "\n",
    "There are many ways to setup these options. We can set them up in method csv() as argument, or use method option, and options. Below code are equivalent.\n",
    "```python\n",
    "# as csv method argument\n",
    "df=spark.read.csv(path=clean_csv_path,header=True,inferSchema=True)\n",
    "\n",
    "# use option\n",
    "df=spark.read\\\n",
    "    .option(\"header\",True)\\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(path=clean_csv_path)\n",
    "\n",
    "# use options\n",
    "df=spark.read\\\n",
    "    .options(header=True,inferSchema=True)\\\n",
    "    .csv(path=clean_csv_path)\n",
    "```\n",
    "\n",
    "## Configurable Options\n",
    "\n",
    "As you can see, when read csv files, we need to specify many options such as:\n",
    "- header (bool) : e.g. header=True,inferSchema=True\n",
    "- inferSchema (bool) : e.g. inferSchema=True\n",
    "- delimiter (String) : e.g. delimiter=','\n",
    "- encoding (String) : e.g. encoding='UTF-8'\n",
    "- nullValue (String) : e.g. nullValue='1900-1-1' all date row with value '1900-1-1' will be considered as null\n",
    "- quotes (String) : When you have a column with a delimiter that used to split the columns (e.g. you have a text column contains ,), use quotes option to specify the quote character, by default it is ‚Äù and delimiters inside quotes are ignored. but using this option you can set any character.\n",
    "\n",
    "You can find the full option list https://spark.apache.org/docs/latest/sql-data-sources-csv.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "clean_csv_path=\"data/csv/adult_nospace_header.csv\"\n",
    "raw_csv_path=\"data/csv/adult.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|age|       workclass|fnlwgt|education|education-num|    marital-status|       occupation| relationship| race|   sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "| 39|       State-gov| 77516|Bachelors|           13|     Never-married|     Adm-clerical|Not-in-family|White|  Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50|Self-emp-not-inc| 83311|Bachelors|           13|Married-civ-spouse|  Exec-managerial|      Husband|White|  Male|           0|           0|            13| United-States| <=50K|\n",
      "| 38|         Private|215646|  HS-grad|            9|          Divorced|Handlers-cleaners|Not-in-family|White|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 53|         Private|234721|     11th|            7|Married-civ-spouse|Handlers-cleaners|      Husband|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 28|         Private|338409|Bachelors|           13|Married-civ-spouse|   Prof-specialty|         Wife|Black|Female|           0|           0|            40|          Cuba| <=50K|\n",
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read\\\n",
    "    .options(header=True,inferSchema=True,delimiter=',',nullValue=\"?\")\\\n",
    "    .csv(path=clean_csv_path)\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# output dataframe as json file\n",
    "json_schema=df.schema.json()\n",
    "schema_file_path=\"data/schema.json\"\n",
    "with open(schema_file_path,\"w\") as f:\n",
    "    f.write(json_schema)\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "32561"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice the column name and type are correct. If the csv has no head, spark will use _c0, _c1, ... as default column name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----------+---+--------------+-------------+--------------+------+-----+-----+----+----+--------------+------+\n",
      "|_c0|       _c1|   _c2|       _c3|_c4|           _c5|          _c6|           _c7|   _c8|  _c9| _c10|_c11|_c12|          _c13|  _c14|\n",
      "+---+----------+------+----------+---+--------------+-------------+--------------+------+-----+-----+----+----+--------------+------+\n",
      "| 39| State-gov| 77516| Bachelors| 13| Never-married| Adm-clerical| Not-in-family| White| Male| 2174|   0|  40| United-States| <=50K|\n",
      "+---+----------+------+----------+---+--------------+-------------+--------------+------+-----+-----+----+----+--------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(raw_csv_path).show(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(raw_csv_path).printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we did not set inferSchema to true, by default spark consider all columns datatype are String."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.2 Read multiple CSV files\n",
    "\n",
    " To read multiple csv files, you just need to pass all file names with comma between them as a path, for example\n",
    "```python\n",
    "df = spark.read.csv(\"path1,path2,path3\")\n",
    "```\n",
    "Does not work on local file, need to find out why"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/csv/adult_nospace_header.csv;data/csv/adult.csv\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/pliu/git/PySparkCommonFunc/notebooks/pysparkbasics/L03_ReadFromVariousDataSource/data/csv/adult_nospace_header.csv;data/csv/adult.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_9253/281556015.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmulti_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"data/csv/adult_nospace_header.csv;data/csv/adult.csv\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmulti_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmulti_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    408\u001B[0m             \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 410\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    411\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1308\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1309\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1310\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: file:/home/pliu/git/PySparkCommonFunc/notebooks/pysparkbasics/L03_ReadFromVariousDataSource/data/csv/adult_nospace_header.csv;data/csv/adult.csv"
     ]
    }
   ],
   "source": [
    "multi_path=\"data/csv/adult_nospace_header.csv,data/csv/adult.csv\"\n",
    "print(multi_path)\n",
    "spark.read.csv(multi_path).count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also give the parent directory path of the csv files. Spark will read all csv files in it, each file will be considered as a partition. Note if the directory contains subdirectory or files in other format, it will fail"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "65121"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_path=\"data/csv\"\n",
    "multi_df=spark.read.csv(path=parent_path,header=True,inferSchema=True)\n",
    "multi_df.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 39: string (nullable = true)\n",
      " |--  State-gov: string (nullable = true)\n",
      " |--  77516: string (nullable = true)\n",
      " |--  Bachelors: string (nullable = true)\n",
      " |--  13: string (nullable = true)\n",
      " |--  Never-married: string (nullable = true)\n",
      " |--  Adm-clerical: string (nullable = true)\n",
      " |--  Not-in-family: string (nullable = true)\n",
      " |--  White: string (nullable = true)\n",
      " |--  Male: string (nullable = true)\n",
      " |--  2174: string (nullable = true)\n",
      " |--  0: string (nullable = true)\n",
      " |--  40: string (nullable = true)\n",
      " |--  United-States: string (nullable = true)\n",
      " |--  <=50K: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note, it will use the schema inferred from the first csv file as the schema of dataframe. As the first read csv does not have header, so we have a schema completely wrong."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.3 Read CSV files with explicit schema\n",
    "\n",
    "Sometimes, you will encounter csv files without header. So you need to specify column names or give a complete schema.\n",
    "\n",
    "Below example read a csv with a given list of column names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: string (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: string (nullable = true)\n",
      " |-- capital-loss: string (nullable = true)\n",
      " |-- hours-per-week: string (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols=[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\"]\n",
    "spark.read.csv(raw_csv_path,header=False).toDF(\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\").printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice the column type are all string. To make the column type right, you need to set option inferSchema to True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: double (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: double (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: double (nullable = true)\n",
      " |-- capital-loss: double (nullable = true)\n",
      " |-- hours-per-week: double (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(raw_csv_path,header=False,inferSchema=True).toDF(\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\").printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice the inferred schema still has problems, the integer column has double as data type. So the best way is to give a schema explicitly. You can specify a schema by using the StructType class, or via a generated schema.json file\n",
    "\n",
    "For more detail about spark schema, please visit this [notebook](../L02_DataFrame/S01_DataStructures/01_Spark_Schema.ipynb) notebooks/pysparkbasics/L02_DataFrame/S01_DataStructures/01_Spark_Schema.ipynb\n",
    "\n",
    "Here we only show how to read schema from a json file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(age,IntegerType,true),StructField(workclass,StringType,true),StructField(fnlwgt,IntegerType,true),StructField(education,StringType,true),StructField(education-num,IntegerType,true),StructField(marital-status,StringType,true),StructField(occupation,StringType,true),StructField(relationship,StringType,true),StructField(race,StringType,true),StructField(sex,StringType,true),StructField(capital-gain,IntegerType,true),StructField(capital-loss,IntegerType,true),StructField(hours-per-week,IntegerType,true),StructField(native-country,StringType,true),StructField(income,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "json_schema=None\n",
    "with open(schema_file_path,'r') as f:\n",
    "    schema_json_str=f.read()\n",
    "    f.close()\n",
    "    json_schema=StructType.fromJson(json.loads(schema_json_str))\n",
    "\n",
    "print(json_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "df_with_schema=spark.read.options(header=False,inferSchema=False,nullValue=\"?\").schema(schema=json_schema).csv(raw_csv_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "|age|        workclass|fnlwgt| education|education-num|     marital-status|        occupation|  relationship|  race|    sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "| 39|        State-gov|  null| Bachelors|         null|      Never-married|      Adm-clerical| Not-in-family| White|   Male|        null|        null|          null| United-States| <=50K|\n",
      "| 50| Self-emp-not-inc|  null| Bachelors|         null| Married-civ-spouse|   Exec-managerial|       Husband| White|   Male|        null|        null|          null| United-States| <=50K|\n",
      "| 38|          Private|  null|   HS-grad|         null|           Divorced| Handlers-cleaners| Not-in-family| White|   Male|        null|        null|          null| United-States| <=50K|\n",
      "| 53|          Private|  null|      11th|         null| Married-civ-spouse| Handlers-cleaners|       Husband| Black|   Male|        null|        null|          null| United-States| <=50K|\n",
      "| 28|          Private|  null| Bachelors|         null| Married-civ-spouse|    Prof-specialty|          Wife| Black| Female|        null|        null|          null|          Cuba| <=50K|\n",
      "+---+-----------------+------+----------+-------------+-------------------+------------------+--------------+------+-------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_schema.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice with the explicit schema, the column type is integer not double now. The reason the inferSchema does not work is that the csv fields contains space.\n",
    "For example below filter does not find anything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "|age|workclass|fnlwgt|education|education-num|marital-status|occupation|relationship|race|sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_with_schema.filter(col(\"workclass\")==\"State-gov\").show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+-------------+-------------+-------------------+---------------+--------------+-------------------+-----+------------+------------+--------------+--------------+------+\n",
      "|age| workclass|fnlwgt|    education|education-num|     marital-status|     occupation|  relationship|               race|  sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+----------+------+-------------+-------------+-------------------+---------------+--------------+-------------------+-----+------------+------------+--------------+--------------+------+\n",
      "| 39| State-gov|  null|    Bachelors|         null|      Never-married|   Adm-clerical| Not-in-family|              White| Male|        null|        null|          null| United-States| <=50K|\n",
      "| 30| State-gov|  null|    Bachelors|         null| Married-civ-spouse| Prof-specialty|       Husband| Asian-Pac-Islander| Male|        null|        null|          null|         India|  >50K|\n",
      "| 22| State-gov|  null| Some-college|         null| Married-civ-spouse|  Other-service|       Husband|              Black| Male|        null|        null|          null| United-States| <=50K|\n",
      "| 41| State-gov|  null|    Assoc-voc|         null| Married-civ-spouse|   Craft-repair|       Husband|              White| Male|        null|        null|          null| United-States| <=50K|\n",
      "| 29| State-gov|  null|    Bachelors|         null| Married-civ-spouse| Prof-specialty|       Husband|              White| Male|        null|        null|          null| United-States|  >50K|\n",
      "+---+----------+------+-------------+-------------+-------------------+---------------+--------------+-------------------+-----+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we add space, we will have results\n",
    "df_with_schema.filter(col(\"workclass\")==\" State-gov\").show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To remove space, we can use trim() function. Check below example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+-------------+-------------+-------------------+---------------+--------------+-------------------+-----+------------+------------+--------------+--------------+------+-------------+\n",
      "|age| workclass|fnlwgt|    education|education-num|     marital-status|     occupation|  relationship|               race|  sex|capital-gain|capital-loss|hours-per-week|native-country|income|new_workclass|\n",
      "+---+----------+------+-------------+-------------+-------------------+---------------+--------------+-------------------+-----+------------+------------+--------------+--------------+------+-------------+\n",
      "| 39| State-gov|  null|    Bachelors|         null|      Never-married|   Adm-clerical| Not-in-family|              White| Male|        null|        null|          null| United-States| <=50K|    State-gov|\n",
      "| 30| State-gov|  null|    Bachelors|         null| Married-civ-spouse| Prof-specialty|       Husband| Asian-Pac-Islander| Male|        null|        null|          null|         India|  >50K|    State-gov|\n",
      "| 22| State-gov|  null| Some-college|         null| Married-civ-spouse|  Other-service|       Husband|              Black| Male|        null|        null|          null| United-States| <=50K|    State-gov|\n",
      "| 41| State-gov|  null|    Assoc-voc|         null| Married-civ-spouse|   Craft-repair|       Husband|              White| Male|        null|        null|          null| United-States| <=50K|    State-gov|\n",
      "| 29| State-gov|  null|    Bachelors|         null| Married-civ-spouse| Prof-specialty|       Husband|              White| Male|        null|        null|          null| United-States|  >50K|    State-gov|\n",
      "+---+----------+------+-------------+-------------+-------------------+---------------+--------------+-------------------+-----+------------+------------+--------------+--------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "df_with_schema.withColumn(\"new_workclass\",trim(col(\"workclass\"))).filter(col(\"new_workclass\")==\"State-gov\").show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we used option nullValue=\"?\", spark will convert all \"?\" to null value. In below example, we can count null values in column `workclass`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "1836"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col(\"workclass\").isNull()).count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3.4 Write CSV files\n",
    "\n",
    "The output csv file numbers depends on the partition number of the dataframe. The default option values are good. Only thing you need to pay attention is header by default is false. Note spark do provide compression on csv file. But we don't see the interest. Because we use csv for human readability, if we want to better space-saving, we will choose other data format\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "output_path=\"/tmp/output_test\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check the output csv files, you will find it without header\n",
    "df.write.csv(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# now check the output csv file with below options.\n",
    "df.write.mode(\"overwrite\").options(header=True,delimiter=\";\").csv(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}