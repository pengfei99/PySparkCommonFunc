{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3.2 Read write parquet files\n",
    "\n",
    "The official doc can be found [here](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html). Apache Parquet file is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/12 02:22:32 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/02/12 02:22:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/12 02:22:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .appName(\"ReadWriteParquet\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"ReadWriteParquet\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config(\"spark.kubernetes.driver.pod.name\", os.environ[\"POD_NAME\"]) \\\n",
    "                      .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') \\\n",
    "                      .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.1 Read parquet file\n",
    "\n",
    "As parquet file contains rich metadata such as schema, compression codec, etc. Reading a parquet requires none configuration (e.g. separator, null_value)\n",
    "Below command use spark.read.parquet() to read a parquet file. Note the file_path could be a single parquet file or a directory that contains a list of parquet files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_path=\"data/adult.snappy.parquet\"\n",
    "df=spark.read.parquet(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|age|       workclass|fnlwgt|education|education-num|    marital-status|       occupation| relationship| race|   sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "| 39|       State-gov| 77516|Bachelors|           13|     Never-married|     Adm-clerical|Not-in-family|White|  Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50|Self-emp-not-inc| 83311|Bachelors|           13|Married-civ-spouse|  Exec-managerial|      Husband|White|  Male|           0|           0|            13| United-States| <=50K|\n",
      "| 38|         Private|215646|  HS-grad|            9|          Divorced|Handlers-cleaners|Not-in-family|White|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 53|         Private|234721|     11th|            7|Married-civ-spouse|Handlers-cleaners|      Husband|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 28|         Private|338409|Bachelors|           13|Married-civ-spouse|   Prof-specialty|         Wife|Black|Female|           0|           0|            40|          Cuba| <=50K|\n",
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row numbers: 32561\n"
     ]
    }
   ],
   "source": [
    "row_numbers=df.count()\n",
    "print(f\"row numbers: {row_numbers}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.2 Partition discovering\n",
    "\n",
    "In the above example, we read a single parquet file. Now let's read a partitioned parquet file. All built-in file sources (including Text/CSV/JSON/ORC/Parquet) are able to discover and infer partitioning information automatically."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.3 Write parquet\n",
    "\n",
    "When spark write parquet file, you can set up few options\n",
    "- save mode : in this tutorial, we only use overwrite mode. For more information. please check [doc](./README.md)\n",
    "- compression codec:\n",
    "- partition by column\n",
    "- mergeSchema\n",
    "\n",
    "The output parquet file can be one or more based on the dataframe partitions. Below example show us the different output parquet files when we change partition numbers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data partition number: 1\n"
     ]
    }
   ],
   "source": [
    "partition_number=df.rdd.getNumPartitions()\n",
    "print(f\"Data partition number: {partition_number}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "output_path=\"/tmp/out_test\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 332K\r\n",
      "drwxr-xr-x  2 pliu pliu 4.0K Feb 12 06:33 .\r\n",
      "drwxrwxrwt 26 root root 4.0K Feb 12 06:33 ..\r\n",
      "-rw-r--r--  1 pliu pliu 314K Feb 12 06:33 part-00000-513396ab-0d8a-4322-8ced-3e3f1c8265e2-c000.snappy.parquet\r\n",
      "-rw-r--r--  1 pliu pliu 2.5K Feb 12 06:33 .part-00000-513396ab-0d8a-4322-8ced-3e3f1c8265e2-c000.snappy.parquet.crc\r\n",
      "-rw-r--r--  1 pliu pliu    0 Feb 12 06:33 _SUCCESS\r\n",
      "-rw-r--r--  1 pliu pliu    8 Feb 12 06:33 ._SUCCESS.crc\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {output_path}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice, the above dataframe write to only one parquet file. That's because the partition number of the dataframe is 1. Now let's try to repartition the dataframe and write to parquet file again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(age#0, 4), REPARTITION_WITH_NUM, [id=#151]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [age#0,workclass#1,fnlwgt#2,education#3,education-num#4,marital-status#5,occupation#6,relationship#7,race#8,sex#9,capital-gain#10,capital-loss#11,hours-per-week#12,native-country#13,income#14] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/pliu/git/PySparkCommonFunc/notebooks/pysparkbasics/L03_ReadFromVario..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:int,workclass:string,fnlwgt:int,education:string,education-num:int,marital-status:stri...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the execution plan\n",
    "df.repartition(4,col(\"age\")).explain()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Note all dataframe are immutable, you need to create a new dataframe to save the modified dataframe\n",
    "df1=df.repartition(4,col(\"age\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data partition number: 4\n"
     ]
    }
   ],
   "source": [
    "# now you should see the partition number is 4\n",
    "partition_number1=df1.rdd.getNumPartitions()\n",
    "print(f\"Data partition number: {partition_number1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "df1.write.mode(\"overwrite\").parquet(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-d8a1f2a5-4b5c-4f4c-8c04-d7972e3f9fcd-c000.snappy.parquet\r\n",
      "part-00001-d8a1f2a5-4b5c-4f4c-8c04-d7972e3f9fcd-c000.snappy.parquet\r\n",
      "part-00002-d8a1f2a5-4b5c-4f4c-8c04-d7972e3f9fcd-c000.snappy.parquet\r\n",
      "part-00003-d8a1f2a5-4b5c-4f4c-8c04-d7972e3f9fcd-c000.snappy.parquet\r\n",
      "_SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls {output_path}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can notice we have 4 parquet file instead of 1. Because we changed the partition number of the dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.4 Write parquet with specific compression algo\n",
    "You can notice, by default spark write parquet file with snappy compression. But if you want, we can change the default compression codec.\n",
    "Below are possible compression codec shorten names\n",
    "- none, uncompressed\n",
    "- snappy\n",
    "- gzip\n",
    "- lzo\n",
    "- brotli\n",
    "- lz4\n",
    "- zstd\n",
    "\n",
    "Note they are case-insensitive, and after setup, it will override `spark.sql.parquet.compression.codec`\n",
    "Note only the snappy, gzip dependencies are included in the spark distribution. If you want to other compression codec, you need to add the dependencies by yourself\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "compression_algo=\"gzip\"\n",
    "\n",
    "df.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"parquet.compression\",compression_algo) \\\n",
    "    .parquet(output_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 260K\r\n",
      "drwxr-xr-x  2 pliu pliu 4.0K Feb 12 06:24 .\r\n",
      "drwxrwxrwt 26 root root 4.0K Feb 12 06:24 ..\r\n",
      "-rw-r--r--  1 pliu pliu 242K Feb 12 06:24 part-00000-a56b9b54-2dd7-444a-8116-796d298f439d-c000.gz.parquet\r\n",
      "-rw-r--r--  1 pliu pliu 1.9K Feb 12 06:24 .part-00000-a56b9b54-2dd7-444a-8116-796d298f439d-c000.gz.parquet.crc\r\n",
      "-rw-r--r--  1 pliu pliu    0 Feb 12 06:24 _SUCCESS\r\n",
      "-rw-r--r--  1 pliu pliu    8 Feb 12 06:24 ._SUCCESS.crc\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah {output_path}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice there are three more files\n",
    "```text\n",
    "-rw-r--r--  1 pliu pliu 1.9K Feb 12 06:24 .part-00000-a56b9b54-2dd7-444a-8116-796d298f439d-c000.gz.parquet.crc\n",
    "-rw-r--r--  1 pliu pliu    0 Feb 12 06:24 _SUCCESS\n",
    "-rw-r--r--  1 pliu pliu    8 Feb 12 06:24 ._SUCCESS.crc\n",
    "```\n",
    "These files are the checksum, as spark is a distributed system, jobs may fail. In case job failure, the checksum and metadata will be used to check and rerun the failed jobs\n",
    "\n",
    "You can remove them by setting up\n",
    "```python\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "spark.conf.set(\"parquet.enable.summary-metadata\", \"false\")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.5 Write parquet with specific partition\n",
    "\n",
    "In section 3.2.3, We only show the partition number, we can do better. Because a good partitioned table(parquet) can help spark to do projection and predicate push down.\n",
    "\n",
    "In below column, we will partition the dataframe by using column sex, then race.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.partitionBy(\"sex\",\"race\").mode(\"overwrite\").parquet(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/out_test:\r\n",
      "'sex=Female'  'sex=Male'   _SUCCESS\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Female':\r\n",
      "'race=Amer-Indian-Eskimo'  'race=Black'  'race=White'\r\n",
      "'race=Asian-Pac-Islander'  'race=Other'\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Female/race=Amer-Indian-Eskimo':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Female/race=Asian-Pac-Islander':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Female/race=Black':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Female/race=Other':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Female/race=White':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Male':\r\n",
      "'race=Amer-Indian-Eskimo'  'race=Black'  'race=White'\r\n",
      "'race=Asian-Pac-Islander'  'race=Other'\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Male/race=Amer-Indian-Eskimo':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Male/race=Asian-Pac-Islander':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Male/race=Black':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Male/race=Other':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n",
      "\r\n",
      "'/tmp/out_test/sex=Male/race=White':\r\n",
      "part-00000-5f34fe08-6254-4ed9-b8f0-1da94abf23c8.c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R {output_path}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You could notice, it generates the below directory. Each directory and subdirectory only contains rows that satisfy the condition(directory name). This will help spark to skipped unnecessary reads.\n",
    "\n",
    "```text\n",
    ".\n",
    "├── sex=Female\n",
    "│   ├── race=Amer-Indian-Eskimo\n",
    "│   ├── race=Asian-Pac-Islander\n",
    "│   ├── race=Black\n",
    "│   ├── race=Other\n",
    "│   └── race=White\n",
    "├── sex=Male\n",
    "│   ├── race=Amer-Indian-Eskimo\n",
    "│   ├── race=Asian-Pac-Islander\n",
    "│   ├── race=Black\n",
    "│   ├── race=Other\n",
    "│   └── race=White\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2.6 Merge schema\n",
    "\n",
    "Like Protocol Buffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.\n",
    "\n",
    "Since schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by\n",
    "\n",
    "- setting data source option mergeSchema to true when reading Parquet files (as shown in the examples below), or\n",
    "- setting the global SQL option spark.sql.parquet.mergeSchema to true.\n",
    "\n",
    "### merge schema example\n",
    "\n",
    "In this example, we create two data frame,\n",
    "- df1, we have two columns: single, double.\n",
    "- df2, we have two columns: single, triple\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|single|double|\n",
      "+------+------+\n",
      "|     1|     1|\n",
      "|     2|     4|\n",
      "|     3|     9|\n",
      "|     4|    16|\n",
      "|     5|    25|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|single|triple|\n",
      "+------+------+\n",
      "|     6|   216|\n",
      "|     7|   343|\n",
      "|     8|   512|\n",
      "|     9|   729|\n",
      "|    10|  1000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "schema_output_path1= f\"{output_path}/merge_schema/test_table/key=1\"\n",
    "schema_output_path2= f\"{output_path}/merge_schema/test_table/key=2\"\n",
    "df1 = spark.createDataFrame(sc.parallelize(range(1, 6)).map(lambda i: Row(single=i, double=i ** 2)))\n",
    "df1.show()\n",
    "df2 = spark.createDataFrame(sc.parallelize(range(6, 11)).map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "df2.show()\n",
    "# then we write the two data frame in a partition folder, here we put key=1, key=2.\n",
    "df1.write.parquet(schema_output_path1)\n",
    "df2.write.parquet(schema_output_path2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the above code will generate below parquet files\n",
    "\n",
    "```text\n",
    "merge_schema/\n",
    "└── test_table\n",
    "    ├── key=1\n",
    "    │   ├── part-00000-6ec4cd58-26fd-4398-a910-66eae8699174-c000.snappy.parquet\n",
    "    │   ├── part-00001-6ec4cd58-26fd-4398-a910-66eae8699174-c000.snappy.parquet\n",
    "    │   ├── part-00002-6ec4cd58-26fd-4398-a910-66eae8699174-c000.snappy.parquet\n",
    "    │   ├── part-00003-6ec4cd58-26fd-4398-a910-66eae8699174-c000.snappy.parquet\n",
    "    │   └── _SUCCESS\n",
    "    └── key=2\n",
    "        ├── part-00000-892aa2e3-753f-4ebe-8136-51c735b0a46a-c000.snappy.parquet\n",
    "        ├── part-00001-892aa2e3-753f-4ebe-8136-51c735b0a46a-c000.snappy.parquet\n",
    "        ├── part-00002-892aa2e3-753f-4ebe-8136-51c735b0a46a-c000.snappy.parquet\n",
    "        ├── part-00003-892aa2e3-753f-4ebe-8136-51c735b0a46a-c000.snappy.parquet\n",
    "        └── _SUCCESS\n",
    "\n",
    "```\n",
    "\n",
    "If we read the parent folder(merge_schema) that contains the partitioned folder(test_table/key=1,key=2). The partition key become a column name, we call it partitioned column.\n",
    "\n",
    "As the data frame in each partition folder has different schema, we need to set mergeSchema to true. Otherwise, it will only use the schema\n",
    "of the first parquet file which it reads. It will drop all columns in other parquet files that do not exist in the schema. Check the below data frame."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n",
      "+------+------+---+\n",
      "|single|double|key|\n",
      "+------+------+---+\n",
      "|     9|  null|  2|\n",
      "|    10|  null|  2|\n",
      "|     4|    16|  1|\n",
      "|     5|    25|  1|\n",
      "|     6|  null|  2|\n",
      "|     8|  null|  2|\n",
      "|     7|  null|  2|\n",
      "|     1|     1|  1|\n",
      "|     2|     4|  1|\n",
      "|     3|     9|  1|\n",
      "+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parent_path=f\"{output_path}/merge_schema/test_table\"\n",
    "# note by default mergeSchema is set to false, no need to add it. We add it only for clarity fo the example\n",
    "unmergedDF = spark.read.option(\"mergeSchema\", \"false\").parquet(parent_path)\n",
    "unmergedDF.printSchema()\n",
    "unmergedDF.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You could notice it only has three column single, double, key. The two first column come from the first parquet partition schema. The column key indicate the row value is from parquet partition 1 or 2.\n",
    "\n",
    "Now let's try to set `mergeSchema` to true. You can notice the mergedDF has four columns right now."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- single: long (nullable = true)\n",
      " |-- double: long (nullable = true)\n",
      " |-- triple: long (nullable = true)\n",
      " |-- key: integer (nullable = true)\n",
      "\n",
      "+------+------+------+---+\n",
      "|single|double|triple|key|\n",
      "+------+------+------+---+\n",
      "|     9|  null|   729|  2|\n",
      "|    10|  null|  1000|  2|\n",
      "|     4|    16|  null|  1|\n",
      "|     5|    25|  null|  1|\n",
      "|     6|  null|   216|  2|\n",
      "|     8|  null|   512|  2|\n",
      "|     7|  null|   343|  2|\n",
      "|     1|     1|  null|  1|\n",
      "|     2|     4|  null|  1|\n",
      "|     3|     9|  null|  1|\n",
      "+------+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(parent_path)\n",
    "mergedDF.printSchema()\n",
    "mergedDF.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}