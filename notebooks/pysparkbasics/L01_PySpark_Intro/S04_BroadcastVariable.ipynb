{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Broadcast variables\n",
    "\n",
    "In spark, broadcast variables are read-only shared variables that are cached and available on all nodes in a\n",
    "cluster rather than shipping a copy of it with tasks. Instead of sending this data along with every task,\n",
    "Spark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce\n",
    "communication costs. It means all executor in the same worker can share the same broadcast variable\n",
    "\n",
    "Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"BroadcastVariable\")\\\n",
    "        .config(\"spark.executor.memory\", \"2g\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"BroadcastVariable\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a broadcast variable\n",
    "\n",
    "We can create a broadcast variables from any variable, in our case, it's called **states_map**. We use the following command **broadcast_states = spark.sparkContext.broadcast(states_map)** to create a broadcast var called **broadcast_states**. The **broadcast_states** is a wrapper around **states_map**, and its value can be accessed by calling the value method. \n",
    "\n",
    "Note that broadcast variables are not sent to workers when we created the broadcast var **by calling sc.broadcast()**, the broadcast var will be sent to executors **when they are first used**.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data: \n",
      " [('James', 'Smith', 'USA', 'CA'), ('Michael', 'Rose', 'USA', 'NY'), ('Robert', 'Williams', 'USA', 'CA'), ('Maria', 'Jones', 'USA', 'FL')]\n"
     ]
    }
   ],
   "source": [
    "states_map = {\"NY\": \"New York\", \"CA\": \"California\", \"FL\": \"Florida\"}\n",
    "broadcast_states = spark.sparkContext.broadcast(states_map)\n",
    "\n",
    "data = [(\"James\", \"Smith\", \"USA\", \"CA\"),\n",
    "        (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
    "        (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
    "        (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
    "            ]\n",
    "print(\"source data: \\n {}\".format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Use a broadcast variables \n",
    "\n",
    "After the broadcast variable (i.e. broadcast_states) is created, pay attention to two points:\n",
    "1. Always use the broadcast variable (i.e. broadcast_states) instead of the value (i.e. states_map) in any functions run on the cluster so that **states_map** is shipped to the nodes only once. \n",
    "2. The value **states_map** should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp1: after the rdd map on broadcast var \n",
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# get the broadcast state map\n",
    "states_map = broadcast_states.value\n",
    "result = rdd.map(lambda x: (x[0], x[1], x[2], states_map[x[3]])).collect()\n",
    "print(\"Exp1: after the rdd map on broadcast var \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a broadcast variables in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data frame\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "Exp2: after the data frame map on broadcast var \n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|     state|\n",
      "+---------+--------+-------+----------+\n",
      "|    James|   Smith|    USA|California|\n",
      "|  Michael|    Rose|    USA|  New York|\n",
      "|   Robert|Williams|    USA|California|\n",
      "|    Maria|   Jones|    USA|   Florida|\n",
      "+---------+--------+-------+----------+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"firstname\", \"lastname\", \"country\", \"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "print(\"Source data frame\")\n",
    "df.show()\n",
    "\n",
    "# get the broadcast state map\n",
    "states_map = broadcast_states.value\n",
    "df1 = df.rdd.map(lambda x: (x[0], x[1], x[2], states_map[x[3]])).toDF(columns)\n",
    "print(\"Exp2: after the data frame map on broadcast var \")\n",
    "df1.show()\n",
    "\n",
    "# Once the variable is broadcast, we can use it in any dataframe operation which is impossible for local variable\n",
    "# Because executors does not have access on the local variables defines on spark drivers\n",
    "local_states_map = [\"NY\"]\n",
    "try:\n",
    "    df2 = df.where(df.state.isin(local_states_map))\n",
    "    df2.show()\n",
    "except:\n",
    "    print(\"Can't use local variables in dataframe operations\")\n",
    "\n",
    "# isin takes a list, so we need to get the keys of the dict and return it to a list, and I only take the first\n",
    "# element of the list.\n",
    "keys = list(states_map.keys())[0:1]\n",
    "df3 = df.where(df[\"state\"].isin(keys))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}