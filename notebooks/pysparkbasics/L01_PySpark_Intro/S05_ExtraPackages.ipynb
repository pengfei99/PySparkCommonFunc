{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31eae603-8351-4512-baf8-df1fb0512ffb",
   "metadata": {},
   "source": [
    "# Import extra package into spark\n",
    "\n",
    "Sometimes, we need to import third party packages into spark session. For example, to read sas files, we need to use a package `saurfang:spark-sas7bdat:3.0.0-s_2.12` developed by a saurfang. To read data from a postgres database, we need to use a package `org.postgresql:postgresql:42.2.24` provided by postgres.\n",
    "\n",
    "There are many ways to import packages into spark session:\n",
    "- by importing raw jar\n",
    "- by using maven dependencies (maven will download the required jar automatically)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e71d2-b950-4ca1-9e89-0cc4c2773ee8",
   "metadata": {},
   "source": [
    "## 1. In submit mode\n",
    "\n",
    "\n",
    "In submit mode, we can use two possible options:\n",
    "- --jar option\n",
    "- --packages option\n",
    "\n",
    "### 1.1 Jar \n",
    "\n",
    "To use --jars option, you must make sure the jar files exist (already downloaded by admin or you) in all the workers. You can use below command to import the packages\n",
    "\n",
    "```bash\n",
    "# use the -- jars option\n",
    "spark-submit --jars /path/file1.jar,/path/file2.jar,/path/file3.jar\n",
    "             ......\n",
    "             ......\n",
    "             your-application.py \n",
    "             \n",
    "# add all jars inside a folder\n",
    "# tr will replace `space` by `,`\n",
    "spark-submit --jars $(echo /path/*.jar | tr ' ' ',') \\ \n",
    "             your-application.py \n",
    "             \n",
    "# If you need a jar only on the **driver node** then use spark.driver.extraClassPath or --driver-class-path  \n",
    "spark-submit --jars /path/file1.jar,/path/file2.jar \\ \n",
    "    --driver-class-path /path/file3.jar \\ \n",
    "    your-application.py\n",
    "```\n",
    "\n",
    "### 1.2 Packages\n",
    "\n",
    "If you want to use the `--packages` option, the packages must be available in the maven repo configured in the worker (The default url is https://repo.maven.apache.org/maven2/).\n",
    "\n",
    "```bash\n",
    "# use the --packages option\n",
    "spark-submit --packages saurfang:spark-sas7bdat:3.0.0-s_2.12, org.postgresql:postgresql:42.2.24\n",
    "             ......\n",
    "             ......\n",
    "             your-application.py \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5369a-5cd4-43a0-a0d3-15a629919383",
   "metadata": {},
   "source": [
    "## 2. In spark shell mode\n",
    "\n",
    "In spark shell mode, it's quite similar to submit mode, we can use two possible options:\n",
    "- --jar option\n",
    "- --packages option\n",
    "\n",
    "### 2.1 Jar \n",
    "\n",
    "To use --jars option, you must make sure the jar files exist (already downloaded by admin or you) in all the workers. You can use below command to import the packages\n",
    "\n",
    "```bash\n",
    "# use the -- jars option\n",
    "spark-shell --jars /path/file1.jar,/path/file2.jar,/path/file3.jar\n",
    "\n",
    "             \n",
    "# add all jars inside a folder\n",
    "# tr will replace `space` by `,`\n",
    "spark-shell --jars $(echo /path/*.jar | tr ' ' ',') \\ \n",
    "          \n",
    "             \n",
    "# If you need a jar only on the **driver node** then use spark.driver.extraClassPath or --driver-class-path  \n",
    "spark-shell --jars /path/file1.jar,/path/file2.jar \\ \n",
    "    --driver-class-path /path/file3.jar \\ \n",
    " \n",
    "    \n",
    "# for pyspark, you may use blow command\n",
    "pyspark --jars /path/file1.jar,/path/file2.jar,/path/file3.jar\n",
    "```\n",
    "\n",
    "### 2.2 Packages\n",
    "\n",
    "If you want to use the `--packages` option, the packages must be available in the maven repo configured in the worker (The default url is https://repo.maven.apache.org/maven2/).\n",
    "\n",
    "```bash\n",
    "# use the --packages option\n",
    "spark-shell --packages saurfang:spark-sas7bdat:3.0.0-s_2.12, org.postgresql:postgresql:42.2.24\n",
    "\n",
    "# for pyspark\n",
    "pyspark --packages saurfang:spark-sas7bdat:3.0.0-s_2.12, org.postgresql:postgresql:42.2.24\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92164274-f40a-421d-9e3d-91eec8b65ba2",
   "metadata": {},
   "source": [
    "## 3. In notebook mode \n",
    "\n",
    "In notebook mode, we create sparksession with `SparkSession.builder`, we can also use two options:\n",
    "\n",
    "- --jar option\n",
    "- --packages option\n",
    "\n",
    "### 3.1 Jar \n",
    "\n",
    "We can use --jars to import jars by using the driver and executor classpaths while creating SparkSession in PySpark as shown below. **This takes the highest priority over other approaches(It will overwrite the conf of other approach)**.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession,DataFrame\n",
    "\n",
    "import os\n",
    "\n",
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"import_packages\")\\\n",
    "        .config(\"spark.jars\", \"/path/file1.jar,/path/file2.jar\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/path/file3.jar\") \\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"import_packages\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .config(\"spark.jars\", \"/path/file1.jar,/path/file2.jar\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/path/file3.jar\") \\\n",
    "        .getOrCreate()\n",
    "```\n",
    "In this example, the **file1.jar and file2.jar** are added to both driver and executors and **file3.jar is added only to the driver classpath**.\n",
    "You can notice we have local mode, and k8s mode for the spark session config.\n",
    "\n",
    "> Note, the jar files must exists on the worker/driver for both mode. The image of the executor in k8s mode must contain the jar file too. This can be difficult if you are not k8s admin. So the --packages option is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0809f31-1152-4f09-a92d-4b9226cffa33",
   "metadata": {},
   "source": [
    "### 3.2 Packages\n",
    "\n",
    "This solution is recommended for spark k8s cluster, because we don't need extra privilege to import packages. Spark will download the necessary jars from the maven central repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe4fe5b-54af-43ba-bdc3-5ce635386720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b0c333-e550-484a-8a47-9b644d1ede1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 10:36:32,481 WARN sql.SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"import_packages\")\\\n",
    "        .config('spark.jars.packages','saurfang:spark-sas7bdat:3.0.0-s_2.12,org.postgresql:postgresql:42.2.24') \\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"import_packages\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .config('spark.jars.packages','saurfang:spark-sas7bdat:3.0.0-s_2.12,org.postgresql:postgresql:42.2.24') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135de481-9760-4b5c-8cc8-236f7ea22404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
