{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 1. Pivot Row to column and back\n",
    "\n",
    "It's very common in data transformation, we need to transform some row into column. In this tutorial, we will show how to do pivot properly.\n",
    "\n",
    "## 1.1 Prepare the spark environmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as f\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "local=False\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"Pivot_and_UnPivot\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", os.environ[\"IMAGE_NAME\"])\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Prepare the data\n",
    "\n",
    "The source data frame has three column(e.g. \"Product\", \"Amount\", \"Country\") which describes the product exporting number for each country of a year.\n",
    "\n",
    "For example, \"Banana\", 1000, \"USA\" means USA export 1000 tons of banana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main output: source data schema\n",
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "main output: source data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\", 1000, \"USA\"), (\"Carrots\", 1500, \"USA\"), (\"Beans\", 1600, \"USA\"),\n",
    "            (\"Orange\", 2000, \"USA\"), (\"Orange\", 2000, \"USA\"), (\"Banana\", 400, \"China\"),\n",
    "            (\"Carrots\", 1200, \"China\"), (\"Beans\", 1500, \"China\"), (\"Orange\", 4000, \"China\"),\n",
    "            (\"Banana\", 2000, \"Canada\"), (\"Carrots\", 2000, \"Canada\"), (\"Beans\", 2000, \"Mexico\")]\n",
    "\n",
    "columns = [\"Product\", \"Amount\", \"Country\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "print(\"main output: source data schema\")\n",
    "df.printSchema()\n",
    "print(\"main output: source data\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 First example\n",
    "Let's first understand what a pivot will do. Below we first build a df that calculates the sum of amount by product. \n",
    "The data frame are shown in below figure.\n",
    "```text\n",
    "+-------+----------+\n",
    "|Product|amount_sum|\n",
    "+-------+----------+\n",
    "|  Beans|      5100|\n",
    "| Banana|      3400|\n",
    "|Carrots|      4700|\n",
    "| Orange|      8000|\n",
    "+-------+----------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Product|amount_sum|\n",
      "+-------+----------+\n",
      "|  Beans|      5100|\n",
      "| Banana|      3400|\n",
      "|Carrots|      4700|\n",
      "| Orange|      8000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product_sum=df.groupBy(\"Product\").agg(f.sum(\"Amount\").alias(\"amount_sum\"))\n",
    "df_product_sum.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pivot function\n",
    "Now we want the product name become column names. Below code shows how to pivot rows to column.\n",
    "Note pivot function can only be called after a groupBy. For now, we leave the groupBy argument empty, It means there is no group.\n",
    "\n",
    "Then the pivot function takes a column name, for each distinct value in the given column, it will create a new column. \n",
    "Note, as you may have duplicate values in the given column. So the pivot function will return a list of other column values. You need to use an aggregation function to transform these list to a single value. In this example, we use first to get the first value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+------+\n",
      "|Banana|Beans|Carrots|Orange|\n",
      "+------+-----+-------+------+\n",
      "|  3400| 5100|   4700|  8000|\n",
      "+------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product_sum.groupBy().pivot(\"Product\").agg(f.first(\"amount_sum\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.4 A more advance example\n",
    "In the above example, we have seen it's quite easy to pivot with two dimentions. Now we will add another dimension. We want see the export number for each country and product.\n",
    "\n",
    "First, let's see which country export which product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|Product|collect_list(Country)|\n",
      "+-------+---------------------+\n",
      "|Orange |[China, USA, USA]    |\n",
      "|Beans  |[China, USA, Mexico] |\n",
      "|Carrots|[China, USA, Canada] |\n",
      "|Banana |[USA, Canada, China] |\n",
      "+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the country list groupby Product\n",
    "df.groupBy(\"Product\").agg(f.collect_list(\"Country\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|Product|Country|sum(Amount)|\n",
      "+-------+-------+-----------+\n",
      "|Carrots|USA    |1500       |\n",
      "|Banana |USA    |1000       |\n",
      "|Beans  |USA    |1600       |\n",
      "|Banana |China  |400        |\n",
      "|Orange |USA    |4000       |\n",
      "|Orange |China  |4000       |\n",
      "|Carrots|China  |1200       |\n",
      "|Beans  |China  |1500       |\n",
      "|Carrots|Canada |2000       |\n",
      "|Beans  |Mexico |2000       |\n",
      "|Banana |Canada |2000       |\n",
      "+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the Amount by Product and country\n",
    "df.groupBy(\"Product\", \"Country\").sum(\"Amount\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the multiple groupBy, we can get the export number for each country and product. But it's not very easy to read. So we want to pivot the distinct country value to columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The pivot function will transform the country list into columns with a value calculated by an aggregation function sum\n",
    "# Note, for the rows that country does not export the product, spark fills it with null.\n",
    "pivot_country = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivot_country.printSchema()\n",
    "pivot_country.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.5 Performence issues\n",
    "\n",
    "Pivot is a very expensive operation. If the data frame that you want to pivot is quite big, you may want to optimize it by using following solutions.\n",
    "- Solution 1: We can provide a list of the column name(row value) that we want to pivot.\n",
    "- Solution 2: Two phrase groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Provide a column list \n",
    "\n",
    "By providing a column list by indicating which column you want to have can improves performence a lot. Otherwise, spark need to change the column size of the result data frame on the fly.\n",
    "\n",
    "You only need to build a list that contains all the distinct values, then put this list as the second argumnet in the pivot function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_list = [\"USA\", \"China\", \"Canada\", \"Mexico\"]\n",
    "pivot_country2 = df.groupBy(\"Product\").pivot(\"Country\", country_list).sum(\"Amount\")\n",
    "pivot_country2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the order of the values list does not reflect on the order of columns. Because spark sort the value in alphabet order.\n",
    "What happens if I remove USA from the list? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+\n",
      "|Product|China|Canada|Mexico|\n",
      "+-------+-----+------+------+\n",
      "|Orange |4000 |null  |null  |\n",
      "|Beans  |1500 |null  |2000  |\n",
      "|Banana |400  |2000  |null  |\n",
      "|Carrots|1200 |2000  |null  |\n",
      "+-------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_list1 = [ \"China\", \"Canada\", \"Mexico\"]\n",
    "pivot_country3 = df.groupBy(\"Product\").pivot(\"Country\", country_list1).sum(\"Amount\")\n",
    "pivot_country3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice spark will simply ignore all rows of \"USA\", and the resulting data frame only contains the column that are defined in the column list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Use two phase groupBy \n",
    "\n",
    "The optimization philosophy is exactly the same as the provinding a column list. To avoid spark change the resulting data frame's column dimension. **We need to tell spark how many columns the data frame will have before it does the pivot function**. So in the two phase groupBy, the first groupBy calculate all the possible disctinct value of the pivot column. The second phase do the pivot function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 1. Use groupBy to get all dictinct value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|Product|Country|sum_amount|\n",
      "+-------+-------+----------+\n",
      "|Carrots| Canada|      2000|\n",
      "|  Beans| Mexico|      2000|\n",
      "| Banana| Canada|      2000|\n",
      "| Banana|  China|       400|\n",
      "| Orange|    USA|      4000|\n",
      "| Orange|  China|      4000|\n",
      "|Carrots|  China|      1200|\n",
      "|  Beans|  China|      1500|\n",
      "|Carrots|    USA|      1500|\n",
      "| Banana|    USA|      1000|\n",
      "|  Beans|    USA|      1600|\n",
      "+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tmp=df.groupBy(\"Product\",\"Country\").agg(f.sum(\"Amount\").alias(\"sum_amount\"))\n",
    "df_tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 2. Pivot the column\n",
    "Just little note, the sum function after pivot is different from the f.sum(). They do the same thing, but not from the same liberary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico| USA|\n",
      "+-------+------+-----+------+----+\n",
      "| Orange|  null| 4000|  null|4000|\n",
      "|  Beans|  null| 1500|  2000|1600|\n",
      "| Banana|  2000|  400|  null|1000|\n",
      "|Carrots|  2000| 1200|  null|1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot_country4=df_tmp.groupBy(\"Product\").pivot(\"Country\").sum(\"sum_amount\")\n",
    "df_pivot_country4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Unpivot\n",
    "Unpivot is a reverse operation, we can achieve by rotating column values into rows values. \n",
    "PySpark SQL doesn’t have unpivot function hence will use the stack() function. Below code converts \n",
    "column countries to row.\n",
    "\n",
    "\n",
    "In the stack function, the first argument is the number of pairs that you want to unpivot. Below we set 3\n",
    "Then, we put the three pair: '<row_value>', column_name. The first argument will be the value in each row after the pivot. The second argument is the column name of the source dataframe (must be the same, otherwise raise error).\n",
    "\n",
    "Below example, you can notice the value Can_ada is in the resulting data frame. \n",
    "\n",
    "The **as (Country, Total)** defines the column name in the resulting data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Banana |Can_ada|2000 |\n",
      "|Banana |China  |400  |\n",
      "|Carrots|Can_ada|2000 |\n",
      "|Carrots|China  |1200 |\n",
      "+-------+-------+-----+\n",
      "\n",
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Total: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_expr1 = \"stack(3, 'Can_ada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
    "unpivot_df1 = pivot_country.select(\"Product\", f.expr(unpivot_expr1)).where(\"Total is not null\")\n",
    "unpivot_df1.show(truncate=False)\n",
    "unpivot_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below exampl shows if we want to add another column \"USA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Orange |USA    |4000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Beans  |USA    |1600 |\n",
      "|Banana |Canada |2000 |\n",
      "|Banana |China  |400  |\n",
      "|Banana |USA    |1000 |\n",
      "|Carrots|Canada |2000 |\n",
      "|Carrots|China  |1200 |\n",
      "|Carrots|USA    |1500 |\n",
      "+-------+-------+-----+\n",
      "\n",
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Total: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unpivot_expr2 = \"stack(4, 'Canada', Canada, 'China', China, 'Mexico', Mexico, 'USA', USA) as (Country,Total)\"\n",
    "unpivot_df2 = pivot_country.select(\"Product\", f.expr(unpivot_expr2)).where(\"Total is not null\")\n",
    "unpivot_df2.show(truncate=False)\n",
    "unpivot_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
