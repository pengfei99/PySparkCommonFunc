{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Find the best PER contract\n",
    "\n",
    "PER (Plan d’Epargne Retraite) is a French investment account which allows you to prepare your retirement and reduce some tax. But there are so many banks and insurance company that offers various contract of PER.\n",
    "\n",
    "The objective of this challenge is to find the `best PER contract of 2022` in the market.\n",
    "\n",
    "The raw data is coming from https://www.francetransactions.com/per-plan-epargne-retraite-206/comparatif-per.html\n",
    "\n",
    "\n",
    "## Data description\n",
    "\n",
    "- Frais sur versements maximum : Des réductions importantes, jusqu'à l'annulation complète des frais sur versements (0%), peuvent être proposées aux épargnants, selon leur intermédiaire. (This tells you how much money you need to pay when you transfer money into your PER account, for example if it's 5%, and you put 100 euros into your account, you only invest 95 euros. With a 2% gain per year, it will take long time to just gain back your capital. **So, I highly recommend you to avoid all contracts that has more than 2% frais versement** vous conseillons donc d’éviter les contrats qui prévoient des frais de versement supérieurs à 2%.)\n",
    "\n",
    "\n",
    "- Frais de gestion sur le fonds euros pour les supports assurance-vie, frais de gestion de compte pour les autres supports (compte-titres, etc.).\n",
    "\n",
    "- Frais de gestion sur les unités de compte pour les supports en assurance-vie. Frais de gestion des produits sur les encours du contrat pour les autres supports (compte-titres, etc.).\n",
    "\n",
    "- Taux publié par les assureurs, nets des frais de gestion, nets de prélèvements sociaux.\n",
    "\n",
    "- Taux nets pour les épargnants, nets des prélèvements sociaux.\n",
    "\n",
    "- Votes instantanés des lecteurs, 1 vote par contrat par lecteur (adresse IP) est autorisé. N'induit rien sur la qualité intrinsèque des contrats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/09 12:48:41 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/08/09 12:48:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/09 12:48:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .appName(\"PER_challenge\")\\\n",
    "                  .getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"PER_challenge\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",os.environ['IMAGE_NAME']) \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config('spark.jars.packages','com.crealytics:spark-excel_2.12:3.1.2_0.17.1') \\\n",
    "                      .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "file_path=\"../../data/per.xls\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** No CODEPAGE record, no encoding_override: will use 'iso-8859-1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pliu/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/pandas/namespace.py:1078: FutureWarning: convert_float is deprecated and will be removed in a future version\n",
      "  return pd.read_excel(\n"
     ]
    }
   ],
   "source": [
    "pdf=ps.read_excel(file_path, sheet_name='per', index_col=[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                Assureur/Support  Avis sur 5  Frais Vers.  Frais Gestion Fonds ?  Frais Gestion UC  Frais/rente                   Fonds euros  Taux brut  Nombre SCPI  Nombre SCI  Nombre OPCI  Nombre ETF  Nombre UC\nPER                                                                                                                                                                                                                                                  \nABEILLE RETRAITE PLURIELLE      ABEILLE RETRAITE PROFESSIONNELLE         NaN          5.0                   1.00              1.00          NaN             ABEILLE EURO PERP        NaN            0           0            0           0         80\nAFER RETRAITE INDIVIDUELLE                               ABEILLE         NaN          3.0                   1.00              1.00          0.0  ABEILLE RP SECURITE RETRAITE        NaN            0           0            0           0         80\nALLIANZ PER HORIZON                                      ALLIANZ         NaN          4.8                   0.85              0.85          NaN              ALLIANZ RETRAITE        NaN            0           0            0           0         92\nAMBITION RETRAITE INDIVIDUELLE                       LA MONDIALE         NaN          3.9                   0.70              0.70          0.0          FONDS EUROS RETRAITE        NaN            0           0            0           0          0\nAMPLI-PER LIBERTE                                 AMPLI-MUTUELLE         NaN          0.0                   0.50              0.40          0.0               AMPLI PER EUROS        NaN            2           0            0           3          4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Assureur/Support</th>\n      <th>Avis sur 5</th>\n      <th>Frais Vers.</th>\n      <th>Frais Gestion Fonds ?</th>\n      <th>Frais Gestion UC</th>\n      <th>Frais/rente</th>\n      <th>Fonds euros</th>\n      <th>Taux brut</th>\n      <th>Nombre SCPI</th>\n      <th>Nombre SCI</th>\n      <th>Nombre OPCI</th>\n      <th>Nombre ETF</th>\n      <th>Nombre UC</th>\n    </tr>\n    <tr>\n      <th>PER</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ABEILLE RETRAITE PLURIELLE</th>\n      <td>ABEILLE RETRAITE PROFESSIONNELLE</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>NaN</td>\n      <td>ABEILLE EURO PERP</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>AFER RETRAITE INDIVIDUELLE</th>\n      <td>ABEILLE</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>ABEILLE RP SECURITE RETRAITE</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>ALLIANZ PER HORIZON</th>\n      <td>ALLIANZ</td>\n      <td>NaN</td>\n      <td>4.8</td>\n      <td>0.85</td>\n      <td>0.85</td>\n      <td>NaN</td>\n      <td>ALLIANZ RETRAITE</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>AMBITION RETRAITE INDIVIDUELLE</th>\n      <td>LA MONDIALE</td>\n      <td>NaN</td>\n      <td>3.9</td>\n      <td>0.70</td>\n      <td>0.70</td>\n      <td>0.0</td>\n      <td>FONDS EUROS RETRAITE</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>AMPLI-PER LIBERTE</th>\n      <td>AMPLI-MUTUELLE</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.50</td>\n      <td>0.40</td>\n      <td>0.0</td>\n      <td>AMPLI PER EUROS</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_10079/4072172637.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    673\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    674\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 675\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    676\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    698\u001B[0m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    699\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 700\u001B[0;31m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    701\u001B[0m         \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    702\u001B[0m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    510\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    511\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 512\u001B[0;31m             \u001B[0mstruct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_inferSchemaFromList\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    513\u001B[0m             \u001B[0mconverter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_converter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstruct\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    514\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconverter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    438\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"can not infer schema from empty dataset\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 439\u001B[0;31m         \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_merge_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0m_infer_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mrow\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    440\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_has_nulltype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Some of types cannot be determined after inferring\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    438\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"can not infer schema from empty dataset\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 439\u001B[0;31m         \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_merge_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0m_infer_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mrow\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    440\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_has_nulltype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Some of types cannot be determined after inferring\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/types.py\u001B[0m in \u001B[0;36m_infer_schema\u001B[0;34m(row, names)\u001B[0m\n\u001B[1;32m   1065\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1066\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1067\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Can not infer schema for type: %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1068\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1069\u001B[0m     \u001B[0mfields\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return DoubleType()\n",
    "    elif f == 'float32': return FloatType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(col_name:str, pandas_col_type):\n",
    "    try: spark_col_type = equivalent_type(pandas_col_type)\n",
    "    except: spark_col_type = StringType()\n",
    "    return StructField(col_name, spark_col_type,nullable=True)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types):\n",
    "        print(column)\n",
    "        struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    print(p_schema)\n",
    "    return spark.createDataFrame(pandas_df, p_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assureur/Support\n",
      "Avis sur 5\n",
      "Frais Vers.\n",
      "Frais Gestion Fonds ?\n",
      "Frais Gestion UC\n",
      "Frais/rente\n",
      "Fonds euros\n",
      "Taux brut\n",
      "Nombre SCPI\n",
      "Nombre SCI\n",
      "Nombre OPCI\n",
      "Nombre ETF\n",
      "Nombre UC\n",
      "StructType(List(StructField(Assureur/Support,StringType,true),StructField(Avis sur 5,DoubleType,true),StructField(Frais Vers.,DoubleType,true),StructField(Frais Gestion Fonds ?,DoubleType,true),StructField(Frais Gestion UC,DoubleType,true),StructField(Frais/rente,DoubleType,true),StructField(Fonds euros,StringType,true),StructField(Taux brut,DoubleType,true),StructField(Nombre SCPI,LongType,true),StructField(Nombre SCI,LongType,true),StructField(Nombre OPCI,LongType,true),StructField(Nombre ETF,LongType,true),StructField(Nombre UC,LongType,true)))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "StructType can not accept object 'Assureur/Support' in type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_10079/379063485.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpandas_to_spark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_10079/3916371968.py\u001B[0m in \u001B[0;36mpandas_to_spark\u001B[0;34m(pandas_df)\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0mp_schema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstruct_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp_schema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpandas_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp_schema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    673\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    674\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 675\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    676\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    698\u001B[0m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    699\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 700\u001B[0;31m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    701\u001B[0m         \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    702\u001B[0m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    507\u001B[0m         \u001B[0;31m# make sure data could consumed multiple times\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 509\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    510\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    511\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mprepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m    680\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    681\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mprepare\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 682\u001B[0;31m                 \u001B[0mverify_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    683\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    684\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDataType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/types.py\u001B[0m in \u001B[0;36mverify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1409\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mverify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1410\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mverify_nullability\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1411\u001B[0;31m             \u001B[0mverify_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1412\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1413\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mverify\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-3iAQ1Rpl-py3.8/lib/python3.8/site-packages/pyspark/sql/types.py\u001B[0m in \u001B[0;36mverify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1396\u001B[0m                     \u001B[0mverifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1397\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1398\u001B[0;31m                 raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\n\u001B[0m\u001B[1;32m   1399\u001B[0m                                         % (obj, type(obj))))\n\u001B[1;32m   1400\u001B[0m         \u001B[0mverify_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mverify_struct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: StructType can not accept object 'Assureur/Support' in type <class 'str'>"
     ]
    }
   ],
   "source": [
    "df=pandas_to_spark(pdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}