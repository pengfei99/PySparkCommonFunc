{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Validate data with spark",
   "id": "4e4065b8a9882745"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54723c86524cf80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import col, when, isnan, trim\n",
    "import pyspark.sql.types as spark_types"
   ],
   "id": "99fd0b75e9f080a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create a spark session in local mode\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Use_pyspark_in_CASD\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "d2fd8eb3b095a23a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# you can get and set configuration of your spark session any moments\n",
    "# get all conf\n",
    "spark.sparkContext.getConf().getAll()"
   ],
   "id": "136f7ee6fc9b7eec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def get_empty_row_count_per_column(df: DataFrame):\n",
    "    \"\"\"\n",
    "    This function is used to get the count of empty rows per column. We have four types empty:\n",
    "    1. null value\n",
    "    2. NaN value\n",
    "    3. Empty string value\n",
    "    4. Special character to present null value(e.g. ?, -, etc.)\n",
    "    \"\"\"\n",
    "    totalRowCount = df.count()\n",
    "\n",
    "    nullSymbols = [\"?\", \"-\"]\n",
    "    aggExpression = []\n",
    "\n",
    "    # step2: build the condition expression for detecting various null case\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        c = col(colName)\n",
    "        colType = df.schema[colName].dataType\n",
    "        # always test null\n",
    "        nullExpr = when(c.isNull(), 1).otherwise(0).alias(nullCountCol)\n",
    "        aggExpression.append(nullExpr)\n",
    "        # test isnan for only numeric columns\n",
    "        nanExpr = when(isnan(c), 1).otherwise(0).alias(nanCountCol)\n",
    "        if isinstance(colType, spark_types.NumericType):\n",
    "            aggExpression.append(nanExpr)\n",
    "        # string null value only for string columns\n",
    "        if isinstance(colType, spark_types.StringType):\n",
    "            aggExpression.append(when(trim(c) == \"\", 1).otherwise(0).alias(blankCountCol))\n",
    "            aggExpression.append(when(c.isin(nullSymbols), 1).otherwise(0).alias(nullSymbolCountCol))\n",
    "\n",
    "    # Perform full-column conditional tagging\n",
    "    flaggedDf = df.select(*aggExpression)\n",
    "\n",
    "    # step3: sum all per-column null case flags in one single pass\n",
    "    try:\n",
    "        summed = flaggedDf.agg(*[spark_sum(c).alias(c) for c in flaggedDf.columns]).collect()[0].asDict()\n",
    "    except Exception as e:\n",
    "        print(f\"Aggregation failed on flaggedDf columns: {flaggedDf.columns}: {e}\")\n",
    "\n",
    "    result = []\n",
    "    # step4: build a list of dict which contains all info for the final result dataframe\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        nullCount = summed.get(nullCountCol, 0)\n",
    "        nanCount = summed.get(nanCountCol, 0)\n",
    "        blankCount = summed.get(blankCountCol, 0)\n",
    "        symbolCount = summed.get(nullSymbolCountCol, 0)\n",
    "        totalEmpty = nullCount + nanCount + blankCount + symbolCount\n",
    "\n",
    "        result.append((\n",
    "            colName, nullCount, nanCount, blankCount,\n",
    "            symbolCount, totalEmpty, totalRowCount\n",
    "        ))\n",
    "    # convert the list of dict into a new dataframe\n",
    "    resDf = spark.createDataFrame(result, [\"column_name\", \"null_count\", \"nan_count\", \"blank_count\",\n",
    "                                           \"null_symbol_count\", \"total_empty_row_count\",\n",
    "                                           \"total_row_count\"])\n",
    "    #\n",
    "\n",
    "    return resDf\n"
   ],
   "id": "8573e12aca54ceda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_duplicated_row_count(df: DataFrame):\n",
    "    duplicate_row_count = df.count() - df.dropDuplicates().count()\n",
    "    print(f\"Duplicate row count: {duplicate_row_count}\")"
   ],
   "id": "3a7fcfb1c9e6c404"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
