{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Join in spark\n",
    "\n",
    "**Spark DataFrame** supports all basic SQL Join Types like **INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN**. Spark SQL Joins are **wider transformations that result in data shuffling over the network**, hence they have huge performance issues when not designed with care.\n",
    "\n",
    "Thanks to DataFrames & Dataset, Spark SQL **Joins comes with more optimization by default**. However we still need to pay more attention when we use join to avoid performance issues.\n",
    "\n",
    "In this tutorial, you will learn different Join syntaxes and using different Join types on two DataFrames and Datasets using python examples.\n",
    "\n",
    "We will talk about **Join on Multiple DataFrames** in the next section (5.2 JoinOnMultipleDataFrame).\n",
    "\n",
    "# 5.2 The Join function\n",
    "\n",
    "\n",
    "\n",
    "pyspark.sql.DataFrame.join(otherDf, onCond=None,how=None) : It joins one df with another. It has three parameters:\n",
    "\n",
    "- otherDf: a DataFrame which is on the right side of the join\n",
    "\n",
    "\n",
    "- onCond: a condition of the join. It can be a str, list or Column, this parameter is optional. If it's a string or a list of strings indicating the column name, the columns must exist in both data frame, and it performs an equi-join. If the column name is different on two sides, we can use \"df1.col1==df2.col2\" to match the columns. If multi columns are involved, we can use list such as \"[df1.age1==df2.age2, df1.name1==df2.name2]\" \n",
    "    \n",
    "- how: is the join type, default value is \"inner\", it has string type and optional. The value must be one of: **inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:08:09.207500895Z",
     "start_time": "2023-12-12T16:08:08.306894414Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:08:55.112248854Z",
     "start_time": "2023-12-12T16:08:42.085916103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/12 17:08:48 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/12/12 17:08:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/12 17:08:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\").appName(\"JoinOnTwoDataFrame\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"JoinOnTwoDataFrame\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:09:31.645909488Z",
     "start_time": "2023-12-12T16:09:19.640829252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- dept_creation_year: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018              |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010              |20         |M     |4000  |\n",
      "|3     |Williams|1              |2018              |21         |M     |1000  |\n",
      "|4     |Jones   |2              |2005              |31         |F     |2000  |\n",
      "|5     |Brown   |2              |2010              |30         |      |-1    |\n",
      "|6     |Foobar  |2              |2010              |150        |      |-1    |\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n"
     ]
    }
   ],
   "source": [
    "emp = [(1, \"Smith\", -1, \"2018\", \"10\", \"M\", 3000),\n",
    "           (2, \"Rose\", 1, \"2010\", \"20\", \"M\", 4000),\n",
    "           (3, \"Williams\", 1, \"2018\", \"21\", \"M\", 1000),\n",
    "           (4, \"Jones\", 2, \"2005\", \"31\", \"F\", 2000),\n",
    "           (5, \"Brown\", 2, \"2010\", \"30\", \"\", -1),\n",
    "           (6, \"Foobar\", 2, \"2010\", \"150\", \"\", -1)\n",
    "           ]\n",
    "emp_col_names = [\"emp_id\", \"name\", \"superior_emp_id\", \"dept_creation_year\",\n",
    "                     \"emp_dept_id\", \"gender\", \"salary\"]\n",
    "emp_df = spark.createDataFrame(data=emp, schema=emp_col_names)\n",
    "emp_df.printSchema()\n",
    "emp_df.show(truncate=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:09:32.310642994Z",
     "start_time": "2023-12-12T16:09:31.644224080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      " |-- dept_creation_year: string (nullable = true)\n",
      "+------------+-------+------------------+\n",
      "|dept_name   |dept_id|dept_creation_year|\n",
      "+------------+-------+------------------+\n",
      "|Finance     |10     |2018              |\n",
      "|Marketing_US|20     |2010              |\n",
      "|Marketing_FR|21     |2018              |\n",
      "|Sales_US    |30     |2005              |\n",
      "|Sales_FR    |31     |2010              |\n",
      "|IT          |50     |2005              |\n",
      "+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\", 10, \"2018\"),\n",
    "            (\"Marketing_US\", 20, \"2010\"),\n",
    "            (\"Marketing_FR\", 21, \"2018\"),\n",
    "            (\"Sales_US\", 30, \"2005\"),\n",
    "            (\"Sales_FR\", 31, \"2010\"),\n",
    "            (\"IT\", 50, \"2005\")\n",
    "            ]\n",
    "\n",
    "dept_col_name = [\"dept_name\", \"dept_id\", \"dept_creation_year\"]\n",
    "dept_df = spark.createDataFrame(data=dept, schema=dept_col_name)\n",
    "dept_df.printSchema()\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Join examples\n",
    "\n",
    "We will use example to illustrate different join type and their syntaxes in python.\n",
    "\n",
    "## 5.3.1 Inner join with different join column name\n",
    "In this example, we inner join two dataframes on a column, note that the name of the joining column are different for the two dataframe. So no confusion.\n",
    "\n",
    "Note after the join, the joining column of the two dataframe are both in the result dataframe. On can be removed to free some space, if your dataset is too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:09:37.658743881Z",
     "start_time": "2023-12-12T16:09:34.531462559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===========================================================(4 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.emp_dept_id ==dept_df.dept_id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Inner join with the same join column name\n",
    "\n",
    "In this example, we will inner join two dataframes on a column, that has the same column name(\"dept_id\") on the two dataframe.\n",
    "\n",
    "Try the bad example, you will see the result has two column called \"dept_id\". If you try to select the \"dept_id\" column, you will receive an error message \"Reference 'dept_id' is ambiguous,\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:09:41.512495888Z",
     "start_time": "2023-12-12T16:09:41.180184266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-------+------+------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|dept_id|gender|salary|\n",
      "+------+--------+---------------+------------------+-------+------+------+\n",
      "|     1|   Smith|             -1|              2018|     10|     M|  3000|\n",
      "|     2|    Rose|              1|              2010|     20|     M|  4000|\n",
      "|     3|Williams|              1|              2018|     21|     M|  1000|\n",
      "|     4|   Jones|              2|              2005|     31|     F|  2000|\n",
      "|     5|   Brown|              2|              2010|     30|      |    -1|\n",
      "|     6|  Foobar|              2|              2010|    150|      |    -1|\n",
      "+------+--------+---------------+------------------+-------+------+------+\n"
     ]
    }
   ],
   "source": [
    "# change the column name from \"emp_dept_id\" to \"dept_id\"\n",
    "emp_df_dup=emp_df.withColumnRenamed(\"emp_dept_id\",\"dept_id\")\n",
    "emp_df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:09:47.115282642Z",
     "start_time": "2023-12-12T16:09:46.425586262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-------+------+------+------------+-------+------------------+\n",
      "|     1|   Smith|             -1|              2018|     10|     M|  3000|     Finance|     10|              2018|\n",
      "|     2|    Rose|              1|              2010|     20|     M|  4000|Marketing_US|     20|              2010|\n",
      "|     3|Williams|              1|              2018|     21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|     30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     4|   Jones|              2|              2005|     31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "+------+--------+---------------+------------------+-------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# note we have two column called \"dept_id\" in the result dataframe\n",
    "bad_df=emp_df_dup.join(dept_df,emp_df_dup.dept_id==dept_df.dept_id,\"inner\")\n",
    "bad_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'dept_id' is ambiguous, could be: dept_id, dept_id.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-f6d673e9a364>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# if we select the dept_id column, we get an error\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mbad_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dept_id\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mselect\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   1667\u001B[0m         \u001B[0;34m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Alice'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Bob'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1668\u001B[0m         \"\"\"\n\u001B[0;32m-> 1669\u001B[0;31m         \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jcols\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1670\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1671\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Reference 'dept_id' is ambiguous, could be: dept_id, dept_id."
     ]
    }
   ],
   "source": [
    "# if we select the dept_id column, we get an error\n",
    "bad_df.select(\"dept_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To avoid the above error**, we can ask spark to create only one column after inner join. The simplest solution is that we just use the directly the column name which are shared by the two dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------------+------------------+------+------+------------+------------------+\n",
      "|dept_id|emp_id|    name|superior_emp_id|dept_creation_year|gender|salary|   dept_name|dept_creation_year|\n",
      "+-------+------+--------+---------------+------------------+------+------+------------+------------------+\n",
      "|     31|     4|   Jones|              2|              2005|     F|  2000|    Sales_FR|              2010|\n",
      "|     10|     1|   Smith|             -1|              2018|     M|  3000|     Finance|              2018|\n",
      "|     21|     3|Williams|              1|              2018|     M|  1000|Marketing_FR|              2018|\n",
      "|     30|     5|   Brown|              2|              2010|      |    -1|    Sales_US|              2005|\n",
      "|     20|     2|    Rose|              1|              2010|     M|  4000|Marketing_US|              2010|\n",
      "+-------+------+--------+---------------+------------------+------+------+------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# note, we only have one dept_id column in the result dataframe\n",
    "good_df=emp_df_dup.join(dept_df,\"dept_id\",\"inner\")\n",
    "good_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|dept_id|\n",
      "+-------+\n",
      "|     31|\n",
      "|     10|\n",
      "|     21|\n",
      "|     30|\n",
      "|     20|\n",
      "+-------+\n"
     ]
    }
   ],
   "source": [
    "good_df.select(\"dept_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 Inner join on multiple column \n",
    "\n",
    "which has same column name\n",
    "\n",
    "In this example, we inner join two dataframes on multiple column, which are \"dept_id\" and \"dept_creation_year\". I intentionally introduced three error in the emp_df, you can notice the last three row, the year and dept_id does not match any rows in the dept_df. So the join only returns 3 rows.\n",
    "\n",
    "Important note, in the **cond list, we separate two condition with \",\" and this is considered as an \"and\". If you want to express \"and\" explicitly, use \"&\" instead of \",\". To express \"or\", use \"|\".**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# we use an implicit and, note we have duplicated column name \n",
    "emp_df.join(dept_df,[emp_df.emp_dept_id == dept_df.dept_id,emp_df.dept_creation_year == dept_df.dept_creation_year], \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# we use an explicite and\n",
    "# note condition must be in () to connect with &\n",
    "emp_df.join(dept_df,[(emp_df.emp_dept_id == dept_df.dept_id) & (emp_df.dept_creation_year == dept_df.dept_creation_year)], \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|Marketing_FR|     21|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|     Finance|     10|              2018|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|    Sales_FR|     31|              2010|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_US|     30|              2005|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|          IT|     50|              2005|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|Marketing_US|     20|              2010|\n",
      "|     6|  Foobar|              2|              2010|        150|      |    -1|Marketing_US|     20|              2010|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_FR|     31|              2010|\n",
      "|     6|  Foobar|              2|              2010|        150|      |    -1|    Sales_FR|     31|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# we use an explicite or\n",
    "emp_df.join(dept_df,[(emp_df.emp_dept_id == dept_df.dept_id) | (emp_df.dept_creation_year == dept_df.dept_creation_year)], \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o135.and. Trace:\npy4j.Py4JException: Method and([class java.lang.String]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-ff400df2870c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# this cond does not work, we can't mix the two mode together in one condition\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mcond_bad\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0memp_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0memp_dept_id\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mdept_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdept_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"dept_creation_year\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0memp_df_dup\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdept_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcond_bad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"inner\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mjoin\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   1326\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1327\u001B[0m                 \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mon\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"on should be Column or list of Column\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1328\u001B[0;31m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__and__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1329\u001B[0m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   1326\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1327\u001B[0m                 \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mon\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"on should be Column or list of Column\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1328\u001B[0;31m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__and__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1329\u001B[0m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m_\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mother\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mother\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m         \u001B[0mnjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnjc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0m_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdoc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n\u001B[1;32m    331\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m                     format(target_id, \".\", name, value))\n\u001B[0m\u001B[1;32m    333\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m             raise Py4JError(\n",
      "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o135.and. Trace:\npy4j.Py4JException: Method and([class java.lang.String]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
     ]
    }
   ],
   "source": [
    "# this cond does not work, we can't mix the two mode together in one condition\n",
    "cond_bad = [emp_df.emp_dept_id == dept_df.dept_id, \"dept_creation_year\"]\n",
    "emp_df_dup.join(dept_df, cond_bad, \"inner\").show(truncate=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------+--------+---------------+------+------+------------+\n",
      "|dept_id|dept_creation_year|emp_id|name    |superior_emp_id|gender|salary|dept_name   |\n",
      "+-------+------------------+------+--------+---------------+------+------+------------+\n",
      "|20     |2010              |2     |Rose    |1              |M     |4000  |Marketing_US|\n",
      "|21     |2018              |3     |Williams|1              |M     |1000  |Marketing_FR|\n",
      "|10     |2018              |1     |Smith   |-1             |M     |3000  |Finance     |\n",
      "+-------+------------------+------+--------+---------------+------+------+------------+\n"
     ]
    }
   ],
   "source": [
    "cond_good = [\"dept_id\", \"dept_creation_year\"]\n",
    "# as the two dataframe has two duplicated column name \"dept_id\" and \"dept_creation_year\", we can use cond_good.\n",
    "emp_df_dup.join(dept_df, cond_good, \"inner\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 Outer Join(a.k.a full, fullouter join) \n",
    "The **outer join returns all rows from both datasets, where join expression does not match it returns null on respective record columns**.\n",
    "\n",
    "note : you will find the output df has two rows contains null,\n",
    "- row 1: dept_id = 50, where the emp_dept_id does not have this value\n",
    "- row 2: emp_dept_id = 150, where the dept_id does not have this value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|  null|    null|           null|              null|       null|  null|  null|          IT|     50|              2005|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     6|  Foobar|              2|              2010|        150|      |    -1|        null|   null|              null|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df,emp_df.emp_dept_id==dept_df.dept_id, \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|  null|    null|           null|              null|       null|  null|  null|          IT|     50|              2005|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     6|  Foobar|              2|              2010|        150|      |    -1|        null|   null|              null|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df,emp_df.emp_dept_id==dept_df.dept_id, \"full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|  null|    null|           null|              null|       null|  null|  null|          IT|     50|              2005|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     6|  Foobar|              2|              2010|        150|      |    -1|        null|   null|              null|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df,emp_df.emp_dept_id==dept_df.dept_id, \"fullouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the above 3 join type **(e.g. outer, full, fullouter) returns the same result**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.5 left outer join \n",
    "**Left (a.k.a Leftouter join) returns all rows from the left dataset, when match join expression does not match it assigns null on respective record columns.**\n",
    "\n",
    "\n",
    "\n",
    "Note : you will find the output df has one row contains null,\n",
    "- row 1: emp_dept_id = 150, where the dept_id does not have this value. Because in emp_df.join(), emp_df is consider as left side df, and dept_df is the right side. So all rows of left df are conserved, for those who does not have a match on right hand df, the right hand df columns are filled with null\n",
    "\n",
    "For the rows of the right df who does not have a match on the left df, they are dropped (e.g. row with dept_id =50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     6|  Foobar|              2|              2010|        150|      |    -1|        null|   null|              null|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.emp_dept_id == dept_df.dept_id, \"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     6|  Foobar|              2|              2010|        150|      |    -1|        null|   null|              null|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.emp_dept_id == dept_df.dept_id, \"leftouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the above **two join type (e.g. left, leftouter) returns the same result**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.6 Right outer join \n",
    "\n",
    "**Right (a.k.a Rightouter join) returns all rows from the right dataset, when match join expression does not match it assigns null on respective record columns. For the rows of the left hand df who does not have a match on the right df, they are dropped** (e.g. row with emp_dept_id =50)\n",
    "\n",
    "note : you will find the output df has one row contains null,\n",
    "- row 1: dept_id = 50, where the emp_dept_id does not have this value. Because in emp_df.join(), emp_df is consider as left side df, and dept_df is the right side\n",
    "\n",
    "So all rows of right hand (i.e. dept_df) df are conserved, for those who does not have a match on left hand df, the left hand df columns are filled with null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|  null|    null|           null|              null|       null|  null|  null|          IT|     50|              2005|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.emp_dept_id == dept_df.dept_id, \"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|   dept_name|dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|  null|    null|           null|              null|       null|  null|  null|          IT|     50|              2005|\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|    Sales_FR|     31|              2010|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|     Finance|     10|              2018|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|Marketing_FR|     21|              2018|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|    Sales_US|     30|              2005|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|Marketing_US|     20|              2010|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.emp_dept_id == dept_df.dept_id, \"rightouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the above **two join type (e.g. right, rightouter) returns the same result**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.7 Left Semi Join\n",
    "\n",
    "**leftsemi join equals to a inner join and a select of the columns of left hand side df. As a result, all columns from the right dataset are ignored.** \n",
    "\n",
    "Note the result of below example, we only have the left hand side columns, and the row where emp_dep_id=150 is dropped, because there is no match no the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n",
      "|     4|   Jones|              2|              2005|         31|     F|  2000|\n",
      "|     1|   Smith|             -1|              2018|         10|     M|  3000|\n",
      "|     3|Williams|              1|              2018|         21|     M|  1000|\n",
      "|     5|   Brown|              2|              2010|         30|      |    -1|\n",
      "|     2|    Rose|              1|              2010|         20|     M|  4000|\n",
      "+------+--------+---------------+------------------+-----------+------+------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.emp_dept_id == dept_df.dept_id, \"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.8 Left Anti Join\n",
    "leftanti join does the opposite of the inner join and select of the columns of left hand side df. As a result, all columns from the right dataset are ignored. \n",
    "\n",
    "Note the result of below example, we only have the left hand side columns. We only had one row where emp_dep_id=150, because this row has no match no the right side. So the anti join will return this row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------------+------------------+-----------+------+------+\n",
      "|emp_id|  name|superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|\n",
      "+------+------+---------------+------------------+-----------+------+------+\n",
      "|     6|Foobar|              2|              2010|        150|      |    -1|\n",
      "+------+------+---------------+------------------+-----------+------+------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.emp_dept_id == dept_df.dept_id, \"leftanti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.9 SelfJoin\n",
    "**All above join type can be applied to the same dataframe.** \n",
    "\n",
    "Below example shows an inner join on the same dataframe which use join condition superior_emp_id= emp_id to generate a table with his superior name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-------------+\n",
      "|emp_id|name    |superior_emp_id|superior_name|\n",
      "+------+--------+---------------+-------------+\n",
      "|2     |Rose    |1              |Smith        |\n",
      "|3     |Williams|1              |Smith        |\n",
      "|4     |Jones   |2              |Rose         |\n",
      "|5     |Brown   |2              |Rose         |\n",
      "|6     |Foobar  |2              |Rose         |\n",
      "+------+--------+---------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "emp_df.alias(\"emp1\").join(emp_df.alias(\"emp2\"), col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"), \"inner\") \\\n",
    "        .select(col(\"emp1.emp_id\").alias(\"emp_id\"), col(\"emp1.name\").alias(\"name\"),\n",
    "                col(\"emp2.emp_id\").alias(\"superior_emp_id\"), col(\"emp2.name\").alias(\"superior_name\")) \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Pure SQL\n",
    "We can also use pure sql to do joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|name    |superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|dept_name   |dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|4     |Jones   |2              |2005              |31         |F     |2000  |Sales_FR    |31     |2010              |\n",
      "|1     |Smith   |-1             |2018              |10         |M     |3000  |Finance     |10     |2018              |\n",
      "|3     |Williams|1              |2018              |21         |M     |1000  |Marketing_FR|21     |2018              |\n",
      "|5     |Brown   |2              |2010              |30         |      |-1    |Sales_US    |30     |2005              |\n",
      "|2     |Rose    |1              |2010              |20         |M     |4000  |Marketing_US|20     |2010              |\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# create views based on the dataframe\n",
    "emp_df.createOrReplaceTempView(\"EMP\")\n",
    "dept_df.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "# use where to avoid join\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|emp_id|name    |superior_emp_id|dept_creation_year|emp_dept_id|gender|salary|dept_name   |dept_id|dept_creation_year|\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n",
      "|4     |Jones   |2              |2005              |31         |F     |2000  |Sales_FR    |31     |2010              |\n",
      "|1     |Smith   |-1             |2018              |10         |M     |3000  |Finance     |10     |2018              |\n",
      "|3     |Williams|1              |2018              |21         |M     |1000  |Marketing_FR|21     |2018              |\n",
      "|5     |Brown   |2              |2010              |30         |      |-1    |Sales_US    |30     |2005              |\n",
      "|2     |Rose    |1              |2010              |20         |M     |4000  |Marketing_US|20     |2010              |\n",
      "+------+--------+---------------+------------------+-----------+------+------+------------+-------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# inner join on dept_id column\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
