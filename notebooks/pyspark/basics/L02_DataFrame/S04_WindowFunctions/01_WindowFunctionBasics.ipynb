{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Window function in spark\n",
    "\n",
    "A window function performs a calculation across a set of rows(aka. Frame). The built-in\n",
    "window functions provided by Spark SQL include two categories:\n",
    "- Ranking functions:\n",
    "- Analytic functions:\n",
    "\n",
    "\n",
    "## 1 Window specification\n",
    "To use window functions, we need to create a **window specification**. A window specification defines which rows\n",
    "are included in the frame associated with a given input row. In another word, the window specification defines\n",
    "the default frame of a window. A window specification can be classified into three categories:\n",
    "\n",
    "1. PartitionBy specification\n",
    "2. Ordered specification\n",
    "3. Custom Range Frame specification\n",
    "\n",
    "\n",
    "### 1.1 PartitionBy specification\n",
    "\n",
    "The partitionBy specification has the below properties:\n",
    "\n",
    "- Created with Window.partitionBy on one or more columns\n",
    "- All rows that have the same value on the partitionBy column will be in the same frame.\n",
    "- The aggregation functions can be applied on each frame\n",
    "- The windows functions can not be applied.\n",
    "\n",
    "Below is an example, we partition the dataframe with the value of the column `name` of the dataset.\n",
    "```text\n",
    "win_name = Window.partitionBy(\"name\")\n",
    "```\n",
    "\n",
    "### 1.2 Ordered specification:\n",
    "\n",
    "The Ordered specification has the below properties:\n",
    "\n",
    "- Created by using a partitionBy specification, followed by an orderBy specification\n",
    "- The frame is not static, it moves when we iterate each row. By default, the frame contains\n",
    " all previous rows and the currentRow.\n",
    "- The window function can be applied to each moving frame (i.e. currentRow+allPreviousRow)\n",
    "- The aggregation functions can be applied to each moving frame. As each row has a different\n",
    " frame, the result of the aggregation is different for each row. Unlike the partitionBy\n",
    " specification, all rows in the same partition has the same result.\n",
    "\n",
    "Below is an example, we partition the dataframe with the value of the column `name` of the dataset, then order the row of each partition with the value order of the column `price`.\n",
    "```text\n",
    "win_name = Window.partitionBy(\"name\").orderBy(\"price\")\n",
    "```\n",
    "\n",
    "### 1.3 Custom Range Frame specification: (check exp4)\n",
    "\n",
    "- Created by using a partitionBy specification,\n",
    "- Usually followed by an orderBy specification,\n",
    "- Then followed by \"rangeBetween\" or \"rowsBetween\"\n",
    "- Each row has a corresponding frame which is controlled by rangeBetween or rowsBetween. For example,\n",
    " rowsBetween(-3,Window.currentRow) means the three rows preceding the current row to the current row.\n",
    " It defines a frame including the current input row and three rows appearing before the current row.\n",
    "- Aggregation can be applied on each frame.\n",
    "\n",
    "\n",
    "#### 1.3.1 Range between\n",
    "\n",
    "**rangeBetween**: uses current **row value** as base index (i.e. 0), and offset to specify start or end.\n",
    "- -666: all the row that contains value is < current row value and > current_row_value - 666 \n",
    "- 0: current row\n",
    "- 666: all the row that contains value is > current row value and < current_row_value + 666\n",
    "\n",
    "In another word, it defines the `frame boundaries based on the values of the order column in the partition that fall within a specified range relative to the current row's value`. \n",
    " \n",
    "Below is an example, we partition the dataframe with the value of the column `name` of the dataset, then order the row of each partition with the value order of the column `price`, the frame is dynamic which is calculated\n",
    "based on the `price value of current row`. In the current row, if the price value is 20, the frame should contain all rows which price value is between 0 and 40 in the same partition.\n",
    "\n",
    "```text\n",
    "win_name = Window.partitionBy(\"name\").orderBy(\"price\").rangeBetween(-20,20)\n",
    "```\n",
    "\n",
    "#### 1.3.2 Rows between\n",
    "\n",
    "**rowsBetween**: uses current row as base index (i.e. 0), and offset to specify start or end.\n",
    "- -1: one row before current row\n",
    "- 0: current row\n",
    "- 1: one row after current row\n",
    "\n",
    "In another word, it defines the frame boundaries based on `the number of rows before and after the current row` within the partition.\n",
    "\n",
    "Below is an example, we partition the dataframe with the value of the column `name` of the dataset, then order the row of each partition with the value order of the column `price`, the frame is dynamic which is calculated\n",
    "based on the `index of current row inside the partition`. If the current row has index 2 (3rd row of the partition), the rolling frame should contain 3 row, current row, the row before, and the row after.\n",
    "\n",
    "```text\n",
    "win_name = Window.partitionBy(\"name\").orderBy(\"price\").rowsBetween(-1,1)\n",
    "```\n",
    "\n",
    "\n",
    "## 2 Windows function can be divided into following categories\n",
    "\n",
    "### 2.1 Ranking functions:\n",
    "\n",
    "- rank: returns the rank of rows within a window partition\n",
    "- dense_rank: returns the rank of rows within a window partition, without any gaps. For example, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "- percent_rank: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
    "- ntile(n:Int): returns the ntile group id (from 1 to n inclusive) in an ordered window partition. For example, if n is 4, the first quarter of the rows will get rank 1, the second quarter will get 2, the thirds quarter will get 3, and the last will get 4. If the rows are less than n, it works too.\n",
    "- row_number: returns a sequential number starting at 1 within a window partition.\n",
    "\n",
    "### 2.2 Analytic functions:\n",
    "\n",
    "- cume_dist: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row. N = total number of rows in the partition. cumeDist(x) = number of values before (and including) x / N. similar to percent_rank()\n",
    "- first()\n",
    "- last()\n",
    "- lag(e\\:Column,offset\\:Int,defaultValue\\:Object): returns the value that is offset rows before the current row, and null if there is less than offset rows before row. For example, an offset of one will return the previous row at any given point in the window partition. The defaultValue is optional\n",
    "- lead(e:Column,offset:Int): returns the value that is offset rows after the current row, and null if there is less than offset rows after the current row. For example, an offset of one will return the next row at any given point in the window partition.\n",
    "- currentRow(): Window function: returns the special frame boundary that represents the current row in\n",
    "                      the window partition.\n",
    "\n",
    "### 2.3 Aggregation functions: All the aggregation function that we showed in S03_GroupByAndAggregation can be used here.\n",
    "\n",
    "- sum(e:Column): returns the sum of selecting column for each partition.\n",
    "- first(e:Column): returns the first value within each partition/rolling frame.\n",
    "- last(e:Column): returns the last value within each partition/rolling frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:49:42.953887016Z",
     "start_time": "2024-01-10T13:49:42.162929942Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, lag, lead, col, avg, \\\n",
    "    min, max, sum, round, count, datediff, unix_timestamp, stddev, collect_list, element_at, size, sort_array, \\\n",
    "    broadcast, spark_partition_id, lit, coalesce, to_date, first, last\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T13:49:58.540268655Z",
     "start_time": "2024-01-10T13:49:44.927785425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/10 14:49:51 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "24/01/10 14:49:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/10 14:49:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder.master(\"local[2]\").appName(\"WindowsFunctions\").getOrCreate()\n",
    "else: \n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"WindowsFunctions\") \\\n",
    "                      .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:master\") \\\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1') \\\n",
    "                      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:50:09.515316709Z",
     "start_time": "2024-01-10T13:50:00.762482017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data frame: \n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+\n",
      "|name|date_str  |product  |price|\n",
      "+----+----------+---------+-----+\n",
      "|Alex|2018-10-10|Paint    |80   |\n",
      "|Alex|2018-04-02|Ladder   |20   |\n",
      "|Alex|2018-06-22|Stool    |20   |\n",
      "|Alex|2018-12-09|Vacuum   |40   |\n",
      "|Alex|2018-07-12|Bucket   |5    |\n",
      "|Alex|2018-02-18|Gloves   |5    |\n",
      "|Alex|2018-03-03|Brushes  |30   |\n",
      "|Alex|2018-09-26|Sandpaper|10   |\n",
      "|Bob |2018-12-09|Vacuum   |40   |\n",
      "|Bob |2018-07-12|Bucket   |3    |\n",
      "|Bob |2018-02-18|Stool    |5    |\n",
      "|Bob |2018-03-03|Brushes  |30   |\n",
      "|Bob |2018-09-26|Sandpaper|10   |\n",
      "+----+----------+---------+-----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [('Alex', '2018-10-10', 'Paint', 80),\n",
    "        ('Alex', '2018-04-02', 'Ladder', 20),\n",
    "        ('Alex', '2018-06-22', 'Stool', 20),\n",
    "        ('Alex', '2018-12-09', 'Vacuum', 40),\n",
    "        ('Alex', '2018-07-12', 'Bucket', 5),\n",
    "        ('Alex', '2018-02-18', 'Gloves', 5),\n",
    "        ('Alex', '2018-03-03', 'Brushes', 30),\n",
    "        ('Alex', '2018-09-26', 'Sandpaper', 10),\n",
    "        ('Bob', '2018-12-09', 'Vacuum', 40),\n",
    "        ('Bob', '2018-07-12', 'Bucket', 3),\n",
    "        ('Bob', '2018-02-18', 'Stool', 5),\n",
    "        ('Bob', '2018-03-03', 'Brushes', 30),\n",
    "        ('Bob', '2018-09-26', 'Sandpaper', 10)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=['name', 'date_str', 'product', 'price'])\n",
    "print(\"source data frame: \")\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "+----+---------+-----+----------+\n",
      "|name|  product|price|      date|\n",
      "+----+---------+-----+----------+\n",
      "|Alex|    Paint|   80|2018-10-10|\n",
      "|Alex|   Ladder|   20|2018-04-02|\n",
      "|Alex|    Stool|   20|2018-06-22|\n",
      "|Alex|   Vacuum|   40|2018-12-09|\n",
      "|Alex|   Bucket|    5|2018-07-12|\n",
      "|Alex|   Gloves|    5|2018-02-18|\n",
      "|Alex|  Brushes|   30|2018-03-03|\n",
      "|Alex|Sandpaper|   10|2018-09-26|\n",
      "| Bob|   Vacuum|   40|2018-12-09|\n",
      "| Bob|   Bucket|    3|2018-07-12|\n",
      "| Bob|    Stool|    5|2018-02-18|\n",
      "| Bob|  Brushes|   30|2018-03-03|\n",
      "| Bob|Sandpaper|   10|2018-09-26|\n",
      "+----+---------+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"date\",to_date(\"date_str\")).drop(\"date_str\")\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T13:50:12.574664673Z",
     "start_time": "2024-01-10T13:50:11.533129032Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Ranking function example \n",
    "\n",
    "In this example, we show how to use window specification to create window. Then we will apply below Ranking functions on ordered frame:\n",
    "- row_number\n",
    "- rank\n",
    "- dense_rank\n",
    "- percent_rank\n",
    "- ntile\n",
    "\n",
    "Note all above window functions require that the frame are ordered. You can try to\n",
    "replace win_name_ordered by win_name and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Window specifications for the Ranking function example \n",
    "\n",
    "# We first create a window specification by using column name\n",
    "# The below window specification contains two partition \"Alex\", \"Bob\"\n",
    "win_name = Window.partitionBy(\"name\")\n",
    "\n",
    "# The second specification takes the first and order it by using column price\n",
    "# The final specification contains two partition \"Alex\", \"Bob\", each partition is ordered by price in ascending order.\n",
    "win_name_ordered_by_price = win_name.orderBy(\"price\")\n",
    "\n",
    "# we use name partition, but this time, we will order rows in each partition by using date\n",
    "win_name_ordered_by_date = win_name.orderBy(\"date\")\n",
    "\n",
    "# a custom window with range between\n",
    "win_name_ordered_by_price_range_5= win_name_ordered_by_price.rangeBetween(Window.unboundedPreceding,1)\n",
    "\n",
    "# a custom window with rows between\n",
    "win_name_ordered_by_price_row_1= win_name_ordered_by_price.rowsBetween(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:06:39.404432625Z",
     "start_time": "2023-12-19T16:06:39.330651890Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Row number\n",
    "\n",
    "This function shows the row number in the frame of each row. \n",
    "\n",
    "Two Things to be noted in below example: \n",
    "1. To invoke a window function, we use `function_name.over(specification)`\n",
    "2. You can notice the row number restarted from 1 for Bob, because it's in a new partition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T14:52:14.496230633Z",
     "start_time": "2023-12-19T14:52:11.014605852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- row_number: integer (nullable = false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+----------+----------+\n",
      "|name|  product|price|      date|row_number|\n",
      "+----+---------+-----+----------+----------+\n",
      "|Alex|   Bucket|    5|2018-07-12|         1|\n",
      "|Alex|   Gloves|    5|2018-02-18|         2|\n",
      "|Alex|Sandpaper|   10|2018-09-26|         3|\n",
      "|Alex|   Ladder|   20|2018-04-02|         4|\n",
      "|Alex|    Stool|   20|2018-06-22|         5|\n",
      "|Alex|  Brushes|   30|2018-03-03|         6|\n",
      "|Alex|   Vacuum|   40|2018-12-09|         7|\n",
      "|Alex|    Paint|   80|2018-10-10|         8|\n",
      "| Bob|   Bucket|    5|2018-07-12|         1|\n",
      "| Bob|   Gloves|    5|2018-02-18|         2|\n",
      "| Bob|Sandpaper|   10|2018-09-26|         3|\n",
      "| Bob|  Brushes|   30|2018-03-03|         4|\n",
      "| Bob|   Vacuum|   40|2018-12-09|         5|\n",
      "+----+---------+-----+----------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a new column row_number by calling the row_number() ranking function. \n",
    "df1 = df.withColumn(\"row_number\", row_number().over(win_name_ordered_by_price))\n",
    "   \n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+----------+----------+------------+\n",
      "|name|product  |price|date      |row_number|partition_id|\n",
      "+----+---------+-----+----------+----------+------------+\n",
      "|Alex|Bucket   |5    |2018-07-12|1         |0           |\n",
      "|Alex|Gloves   |5    |2018-02-18|2         |0           |\n",
      "|Alex|Sandpaper|10   |2018-09-26|3         |0           |\n",
      "|Alex|Ladder   |20   |2018-04-02|4         |0           |\n",
      "|Alex|Stool    |20   |2018-06-22|5         |0           |\n",
      "|Alex|Brushes  |30   |2018-03-03|6         |0           |\n",
      "|Alex|Vacuum   |40   |2018-12-09|7         |0           |\n",
      "|Alex|Paint    |80   |2018-10-10|8         |0           |\n",
      "|Bob |Bucket   |5    |2018-07-12|1         |0           |\n",
      "|Bob |Gloves   |5    |2018-02-18|2         |0           |\n",
      "|Bob |Sandpaper|10   |2018-09-26|3         |0           |\n",
      "|Bob |Brushes  |30   |2018-03-03|4         |0           |\n",
      "|Bob |Vacuum   |40   |2018-12-09|5         |0           |\n",
      "+----+---------+-----+----------+----------+------------+\n"
     ]
    }
   ],
   "source": [
    "# with the below code, we can notice that the logical division of the data frame has no impact on the physical data partition. \n",
    "df1.withColumn(\"partition_id\", spark_partition_id()).show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T14:52:29.908492158Z",
     "start_time": "2023-12-19T14:52:28.891102934Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- row_number: integer (nullable = false)\n",
      "+----+---------+-----+----------+----------+\n",
      "|name|  product|price|      date|row_number|\n",
      "+----+---------+-----+----------+----------+\n",
      "|Alex|   Gloves|    5|2018-02-18|         1|\n",
      "|Alex|  Brushes|   30|2018-03-03|         2|\n",
      "|Alex|   Ladder|   20|2018-04-02|         3|\n",
      "|Alex|    Stool|   20|2018-06-22|         4|\n",
      "|Alex|   Bucket|    5|2018-07-12|         5|\n",
      "|Alex|Sandpaper|   10|2018-09-26|         6|\n",
      "|Alex|    Paint|   80|2018-10-10|         7|\n",
      "|Alex|   Vacuum|   40|2018-12-09|         8|\n",
      "| Bob|   Gloves|    5|2018-02-18|         1|\n",
      "| Bob|  Brushes|   30|2018-03-03|         2|\n",
      "| Bob|   Bucket|    5|2018-07-12|         3|\n",
      "| Bob|Sandpaper|   10|2018-09-26|         4|\n",
      "| Bob|   Vacuum|   40|2018-12-09|         5|\n",
      "+----+---------+-----+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "# In this example we order the row with date column\n",
    "df2 = df.withColumn(\"row_number\", row_number().over(win_name_ordered_by_date))\n",
    "   \n",
    "df2.printSchema()\n",
    "df2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:00:24.545827164Z",
     "start_time": "2023-12-19T15:00:23.644471256Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Window Frame specifiedwindowframe(RangeFrame, unboundedpreceding$(), 1) must match the required frame specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# test with range between or row between\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df3 \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrow_number\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrow_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwin_name_ordered_by_price_range_5\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m df3\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:3036\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   3034\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   3035\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol should be Column\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 3036\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Window Frame specifiedwindowframe(RangeFrame, unboundedpreceding$(), 1) must match the required frame specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())"
     ]
    }
   ],
   "source": [
    "# test with range between or row between\n",
    "df3 = df.withColumn(\"row_number\",row_number().over(win_name_ordered_by_price_range_5))\n",
    "df3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T08:18:13.801012426Z",
     "start_time": "2023-12-20T08:18:13.575617923Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice that we can not apply row_number() on the range window specification. **Only aggregation function can apply on the range window specification.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Rank and dense_rank\n",
    "\n",
    "The **rank()/dense_rank()** returns the rank of rows within a window partition. The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth.\n",
    "\n",
    "In the first example, we do first a rank(), you can notice for the Alex partition, there is no rank 2, because we have two items in rank 1, the third item goes to rank 3. If you want compact rank number, use dense rank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T15:13:55.484353438Z",
     "start_time": "2023-12-19T15:13:54.796482623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- rank: integer (nullable = false)\n",
      "+----+---------+-----+----------+----+\n",
      "|name|  product|price|      date|rank|\n",
      "+----+---------+-----+----------+----+\n",
      "|Alex|   Bucket|    5|2018-07-12|   1|\n",
      "|Alex|   Gloves|    5|2018-02-18|   1|\n",
      "|Alex|Sandpaper|   10|2018-09-26|   3|\n",
      "|Alex|   Ladder|   20|2018-04-02|   4|\n",
      "|Alex|    Stool|   20|2018-06-22|   4|\n",
      "|Alex|  Brushes|   30|2018-03-03|   6|\n",
      "|Alex|   Vacuum|   40|2018-12-09|   7|\n",
      "|Alex|    Paint|   80|2018-10-10|   8|\n",
      "| Bob|   Bucket|    5|2018-07-12|   1|\n",
      "| Bob|   Gloves|    5|2018-02-18|   1|\n",
      "| Bob|Sandpaper|   10|2018-09-26|   3|\n",
      "| Bob|  Brushes|   30|2018-03-03|   4|\n",
      "| Bob|   Vacuum|   40|2018-12-09|   5|\n",
      "+----+---------+-----+----------+----+\n"
     ]
    }
   ],
   "source": [
    "# create a new column with rank() ranking function\n",
    "df1 = df.withColumn(\"rank\", rank().over(win_name_ordered_by_price))\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- rank: integer (nullable = false)\n",
      "+----+---------+-----+----------+----+\n",
      "|name|  product|price|      date|rank|\n",
      "+----+---------+-----+----------+----+\n",
      "|Alex|   Gloves|    5|2018-02-18|   1|\n",
      "|Alex|  Brushes|   30|2018-03-03|   2|\n",
      "|Alex|   Ladder|   20|2018-04-02|   3|\n",
      "|Alex|    Stool|   20|2018-06-22|   4|\n",
      "|Alex|   Bucket|    5|2018-07-12|   5|\n",
      "|Alex|Sandpaper|   10|2018-09-26|   6|\n",
      "|Alex|    Paint|   80|2018-10-10|   7|\n",
      "|Alex|   Vacuum|   40|2018-12-09|   8|\n",
      "| Bob|   Gloves|    5|2018-02-18|   1|\n",
      "| Bob|  Brushes|   30|2018-03-03|   2|\n",
      "| Bob|   Bucket|    5|2018-07-12|   3|\n",
      "| Bob|Sandpaper|   10|2018-09-26|   4|\n",
      "| Bob|   Vacuum|   40|2018-12-09|   5|\n",
      "+----+---------+-----+----------+----+\n"
     ]
    }
   ],
   "source": [
    "# create a new column with rank() ranking function\n",
    "df2 = df.withColumn(\"rank\", rank().over(win_name_ordered_by_date))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:14:36.187703203Z",
     "start_time": "2023-12-19T15:14:35.528679978Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T15:15:26.473440674Z",
     "start_time": "2023-12-19T15:15:25.996165587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- dense_rank: integer (nullable = false)\n",
      "+----+---------+-----+----------+----------+\n",
      "|name|  product|price|      date|dense_rank|\n",
      "+----+---------+-----+----------+----------+\n",
      "|Alex|   Bucket|    5|2018-07-12|         1|\n",
      "|Alex|   Gloves|    5|2018-02-18|         1|\n",
      "|Alex|Sandpaper|   10|2018-09-26|         2|\n",
      "|Alex|   Ladder|   20|2018-04-02|         3|\n",
      "|Alex|    Stool|   20|2018-06-22|         3|\n",
      "|Alex|  Brushes|   30|2018-03-03|         4|\n",
      "|Alex|   Vacuum|   40|2018-12-09|         5|\n",
      "|Alex|    Paint|   80|2018-10-10|         6|\n",
      "| Bob|   Bucket|    5|2018-07-12|         1|\n",
      "| Bob|   Gloves|    5|2018-02-18|         1|\n",
      "| Bob|Sandpaper|   10|2018-09-26|         2|\n",
      "| Bob|  Brushes|   30|2018-03-03|         3|\n",
      "| Bob|   Vacuum|   40|2018-12-09|         4|\n",
      "+----+---------+-----+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "# create a column with dense rank\n",
    "# Note that for Alex partition, even thought we have two items in rank 1, but the third item goes to\n",
    "# rank 2 not 3.\n",
    "df3 = df.withColumn(\"dense_rank\", dense_rank().over(win_name_ordered_by_price))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 percent_rank\n",
    "\n",
    "The **percent_rank** functions returns the relative rank (i.e. percentile) of rows within a window partition."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T15:18:40.943559210Z",
     "start_time": "2023-12-19T15:18:40.511024602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- percent_rank: double (nullable = false)\n",
      "+----+---------+-----+----------+-------------------+\n",
      "|name|  product|price|      date|       percent_rank|\n",
      "+----+---------+-----+----------+-------------------+\n",
      "|Alex|   Bucket|    5|2018-07-12|                0.0|\n",
      "|Alex|   Gloves|    5|2018-02-18|                0.0|\n",
      "|Alex|Sandpaper|   10|2018-09-26| 0.2857142857142857|\n",
      "|Alex|   Ladder|   20|2018-04-02|0.42857142857142855|\n",
      "|Alex|    Stool|   20|2018-06-22|0.42857142857142855|\n",
      "|Alex|  Brushes|   30|2018-03-03| 0.7142857142857143|\n",
      "|Alex|   Vacuum|   40|2018-12-09| 0.8571428571428571|\n",
      "|Alex|    Paint|   80|2018-10-10|                1.0|\n",
      "| Bob|   Bucket|    5|2018-07-12|                0.0|\n",
      "| Bob|   Gloves|    5|2018-02-18|                0.0|\n",
      "| Bob|Sandpaper|   10|2018-09-26|                0.5|\n",
      "| Bob|  Brushes|   30|2018-03-03|               0.75|\n",
      "| Bob|   Vacuum|   40|2018-12-09|                1.0|\n",
      "+----+---------+-----+----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# create a column with percent rank, the percent is calculated by dense_rank_number/total_item_number\n",
    "df1 = df.withColumn(\"percent_rank\", percent_rank().over(win_name_ordered_by_price))\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- percent_rank: double (nullable = false)\n",
      "+----+---------+-----+----------+-------------------+\n",
      "|name|  product|price|      date|       percent_rank|\n",
      "+----+---------+-----+----------+-------------------+\n",
      "|Alex|   Gloves|    5|2018-02-18|                0.0|\n",
      "|Alex|  Brushes|   30|2018-03-03|0.14285714285714285|\n",
      "|Alex|   Ladder|   20|2018-04-02| 0.2857142857142857|\n",
      "|Alex|    Stool|   20|2018-06-22|0.42857142857142855|\n",
      "|Alex|   Bucket|    5|2018-07-12| 0.5714285714285714|\n",
      "|Alex|Sandpaper|   10|2018-09-26| 0.7142857142857143|\n",
      "|Alex|    Paint|   80|2018-10-10| 0.8571428571428571|\n",
      "|Alex|   Vacuum|   40|2018-12-09|                1.0|\n",
      "| Bob|   Gloves|    5|2018-02-18|                0.0|\n",
      "| Bob|  Brushes|   30|2018-03-03|               0.25|\n",
      "| Bob|   Bucket|    5|2018-07-12|                0.5|\n",
      "| Bob|Sandpaper|   10|2018-09-26|               0.75|\n",
      "| Bob|   Vacuum|   40|2018-12-09|                1.0|\n",
      "+----+---------+-----+----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"percent_rank\", percent_rank().over(win_name_ordered_by_date))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:19:43.821977220Z",
     "start_time": "2023-12-19T15:19:43.345897901Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 ntile\n",
    "\n",
    "The **ntile** function returns the `ntile group id` (from 1 to n inclusive) in an ordered window partition. For example, if n is 4, the first quarter of the rows will get value 1, the second quarter will get 2, the third quarter will get 3, and the last quarter will get 4.\n",
    "\n",
    "In the below example, we set the n=3, which means we divide each window into 3 parts. The rows in the 1st part will get 1 as ntile_rank, the rows in the 2n part will get 2, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T15:23:25.177893293Z",
     "start_time": "2023-12-19T15:23:24.508919387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- ntile_rank: integer (nullable = false)\n",
      "+----+---------+-----+----------+----------+\n",
      "|name|  product|price|      date|ntile_rank|\n",
      "+----+---------+-----+----------+----------+\n",
      "|Alex|   Bucket|    5|2018-07-12|         1|\n",
      "|Alex|   Gloves|    5|2018-02-18|         1|\n",
      "|Alex|Sandpaper|   10|2018-09-26|         1|\n",
      "|Alex|   Ladder|   20|2018-04-02|         2|\n",
      "|Alex|    Stool|   20|2018-06-22|         2|\n",
      "|Alex|  Brushes|   30|2018-03-03|         2|\n",
      "|Alex|   Vacuum|   40|2018-12-09|         3|\n",
      "|Alex|    Paint|   80|2018-10-10|         3|\n",
      "| Bob|   Bucket|    5|2018-07-12|         1|\n",
      "| Bob|   Gloves|    5|2018-02-18|         1|\n",
      "| Bob|Sandpaper|   10|2018-09-26|         2|\n",
      "| Bob|  Brushes|   30|2018-03-03|         2|\n",
      "| Bob|   Vacuum|   40|2018-12-09|         3|\n",
      "+----+---------+-----+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "# create a column with ntile\n",
    "# here we set n=3, \n",
    "df1 = df.withColumn(\"ntile_rank\", ntile(3).over(win_name_ordered_by_price))\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Analytic function examples\n",
    "\n",
    "In this section, we show how to use analytic functions on ordered frame\n",
    "- cume_dist\n",
    "- first\n",
    "- last\n",
    "- lag\n",
    "- lead\n",
    "\n",
    "Note all above window functions require that the **frame are ordered**."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 cum_dist\n",
    "\n",
    "The **cum_dist** function returns the `cumulative distribution` of values within a window partition, i.e. the fraction of rows that are below the current row.\n",
    "\n",
    "You can find more information of the `cumulative distribution` here https://en.wikipedia.org/wiki/Cumulative_distribution_function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T08:30:53.882067051Z",
     "start_time": "2023-12-20T08:30:53.178892880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- cumulative_distribution: double (nullable = false)\n",
      "+----+---------+-----+----------+-----------------------+\n",
      "|name|  product|price|      date|cumulative_distribution|\n",
      "+----+---------+-----+----------+-----------------------+\n",
      "|Alex|   Bucket|    5|2018-07-12|                   0.25|\n",
      "|Alex|   Gloves|    5|2018-02-18|                   0.25|\n",
      "|Alex|Sandpaper|   10|2018-09-26|                  0.375|\n",
      "|Alex|   Ladder|   20|2018-04-02|                  0.625|\n",
      "|Alex|    Stool|   20|2018-06-22|                  0.625|\n",
      "|Alex|  Brushes|   30|2018-03-03|                   0.75|\n",
      "|Alex|   Vacuum|   40|2018-12-09|                  0.875|\n",
      "|Alex|    Paint|   80|2018-10-10|                    1.0|\n",
      "| Bob|   Bucket|    5|2018-07-12|                    0.4|\n",
      "| Bob|   Gloves|    5|2018-02-18|                    0.4|\n",
      "| Bob|Sandpaper|   10|2018-09-26|                    0.6|\n",
      "| Bob|  Brushes|   30|2018-03-03|                    0.8|\n",
      "| Bob|   Vacuum|   40|2018-12-09|                    1.0|\n",
      "+----+---------+-----+----------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "# create a new column that shows the cumulative_distribution\n",
    "df_cume_dist1 = df.withColumn(\"cumulative_distribution\", cume_dist().over(win_name_ordered_by_price))\n",
    "df_cume_dist1.printSchema()\n",
    "df_cume_dist1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Window Frame specifiedwindowframe(RangeFrame, unboundedpreceding$(), 1) must match the required frame specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# create a new column that shows the cumulative_distribution\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_cume_dist2 \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcumulative_distribution\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcume_dist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mover\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwin_name_ordered_by_price_range_5\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m df_cume_dist2\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m      4\u001B[0m df_cume_dist2\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:3036\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   3034\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   3035\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol should be Column\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 3036\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: Window Frame specifiedwindowframe(RangeFrame, unboundedpreceding$(), 1) must match the required frame specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())"
     ]
    }
   ],
   "source": [
    "# create a new column that shows the cumulative_distribution\n",
    "df_cume_dist2 = df.withColumn(\"cumulative_distribution\", cume_dist().over(win_name_ordered_by_price_range_5))\n",
    "df_cume_dist2.printSchema()\n",
    "df_cume_dist2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T08:34:13.843049913Z",
     "start_time": "2023-12-20T08:34:13.630383810Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**We can't apply cume_dist on rangeBetween also**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 first\n",
    "\n",
    "The **first()** will return the first element of each partition. The ordering of the partition is defined by the orderBy. If no order by is presented, the order is defined by the partition."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+----------+-----------+\n",
      "|name|  product|price|      date|first_price|\n",
      "+----+---------+-----+----------+-----------+\n",
      "|Alex|   Gloves|    5|2018-02-18|          5|\n",
      "|Alex|  Brushes|   30|2018-03-03|          5|\n",
      "|Alex|   Ladder|   20|2018-04-02|          5|\n",
      "|Alex|    Stool|   20|2018-06-22|          5|\n",
      "|Alex|   Bucket|    5|2018-07-12|          5|\n",
      "|Alex|Sandpaper|   10|2018-09-26|          5|\n",
      "|Alex|    Paint|   80|2018-10-10|          5|\n",
      "|Alex|   Vacuum|   40|2018-12-09|          5|\n",
      "| Bob|   Gloves|    5|2018-02-18|          5|\n",
      "| Bob|  Brushes|   30|2018-03-03|          5|\n",
      "| Bob|   Bucket|    5|2018-07-12|          5|\n",
      "| Bob|Sandpaper|   10|2018-09-26|          5|\n",
      "| Bob|   Vacuum|   40|2018-12-09|          5|\n",
      "+----+---------+-----+----------+-----------+\n"
     ]
    }
   ],
   "source": [
    "df1=df.withColumn(\"first_price\",first(col(\"price\")).over(win_name_ordered_by_date))\n",
    "df1.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-20T09:37:54.285426561Z",
     "start_time": "2023-12-20T09:37:53.734878876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+----------+-------------+\n",
      "|name|  product|price|      date|first_product|\n",
      "+----+---------+-----+----------+-------------+\n",
      "|Alex|   Gloves|    5|2018-02-18|       Gloves|\n",
      "|Alex|  Brushes|   30|2018-03-03|       Gloves|\n",
      "|Alex|   Ladder|   20|2018-04-02|       Gloves|\n",
      "|Alex|    Stool|   20|2018-06-22|       Gloves|\n",
      "|Alex|   Bucket|    5|2018-07-12|       Gloves|\n",
      "|Alex|Sandpaper|   10|2018-09-26|       Gloves|\n",
      "|Alex|    Paint|   80|2018-10-10|       Gloves|\n",
      "|Alex|   Vacuum|   40|2018-12-09|       Gloves|\n",
      "| Bob|    Stool|    5|2018-02-18|        Stool|\n",
      "| Bob|  Brushes|   30|2018-03-03|        Stool|\n",
      "| Bob|   Bucket|    5|2018-07-12|        Stool|\n",
      "| Bob|Sandpaper|   10|2018-09-26|        Stool|\n",
      "| Bob|   Vacuum|   40|2018-12-09|        Stool|\n",
      "+----+---------+-----+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "# In below example, we apply first() on a different column\n",
    "df2=df.withColumn(\"first_product\",first(col(\"product\")).over(win_name_ordered_by_date))\n",
    "df2.show(30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T09:42:06.367179672Z",
     "start_time": "2023-12-20T09:42:05.613400654Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+----------+-------------------+\n",
      "|name|  product|price|      date|first_rolling_price|\n",
      "+----+---------+-----+----------+-------------------+\n",
      "|Alex|   Bucket|    5|2018-07-12|                  5|\n",
      "|Alex|   Gloves|    5|2018-02-18|                  5|\n",
      "|Alex|Sandpaper|   10|2018-09-26|                  5|\n",
      "|Alex|   Ladder|   20|2018-04-02|                  5|\n",
      "|Alex|    Stool|   20|2018-06-22|                  5|\n",
      "|Alex|  Brushes|   30|2018-03-03|                  5|\n",
      "|Alex|   Vacuum|   40|2018-12-09|                  5|\n",
      "|Alex|    Paint|   80|2018-10-10|                  5|\n",
      "| Bob|   Bucket|    3|2018-07-12|                  3|\n",
      "| Bob|    Stool|    5|2018-02-18|                  3|\n",
      "| Bob|Sandpaper|   10|2018-09-26|                  3|\n",
      "| Bob|  Brushes|   30|2018-03-03|                  3|\n",
      "| Bob|   Vacuum|   40|2018-12-09|                  3|\n",
      "+----+---------+-----+----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "df3=df.withColumn(\"first_rolling_price\",first(col(\"price\")).over(win_name_ordered_by_price_range_5))\n",
    "df3.show(30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T09:44:09.492166207Z",
     "start_time": "2023-12-20T09:44:08.925875113Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Last\n",
    "\n",
    "A little surprise for last. It does not return the last element of each partition. It returns the current row value. Check below example\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+----------+----+\n",
      "|name|  product|price|      date|last|\n",
      "+----+---------+-----+----------+----+\n",
      "|Alex|   Gloves|    5|2018-02-18|   5|\n",
      "|Alex|  Brushes|   30|2018-03-03|  30|\n",
      "|Alex|   Ladder|   20|2018-04-02|  20|\n",
      "|Alex|    Stool|   20|2018-06-22|  20|\n",
      "|Alex|   Bucket|    5|2018-07-12|   5|\n",
      "|Alex|Sandpaper|   10|2018-09-26|  10|\n",
      "|Alex|    Paint|   80|2018-10-10|  80|\n",
      "|Alex|   Vacuum|   40|2018-12-09|  40|\n",
      "| Bob|   Gloves|    5|2018-02-18|   5|\n",
      "| Bob|  Brushes|   30|2018-03-03|  30|\n",
      "| Bob|   Bucket|    5|2018-07-12|   5|\n",
      "| Bob|Sandpaper|   10|2018-09-26|  10|\n",
      "| Bob|   Vacuum|   40|2018-12-09|  40|\n",
      "+----+---------+-----+----------+----+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_last=df.withColumn(\"last\",last(col(\"price\")).over(win_name_ordered_by_date))\n",
    "df_last.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-20T08:32:19.289715985Z",
     "start_time": "2023-12-20T08:32:19.030547954Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This is not a bug of spark last() function. This is due to how window frame works in a window function**.\n",
    "\n",
    "Check below example, the output of explain(). The range frame is **specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())**, the first row is the first row of all already read row (so far so good). But the last row is the current row, because it can't know which row is the last row of the partition during iteration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------+----------+\n",
      "|name|      date|  product|price|first_price|last_price|\n",
      "+----+----------+---------+-----+-----------+----------+\n",
      "|Alex|2018-02-18|   Gloves|    5|          5|         5|\n",
      "|Alex|2018-03-03|  Brushes|   30|          5|        30|\n",
      "|Alex|2018-04-02|   Ladder|   20|          5|        20|\n",
      "|Alex|2018-06-22|    Stool|   20|          5|        20|\n",
      "|Alex|2018-07-12|   Bucket|    5|          5|         5|\n",
      "|Alex|2018-09-26|Sandpaper|   10|          5|        10|\n",
      "|Alex|2018-10-10|    Paint|   80|          5|        80|\n",
      "|Alex|2018-12-09|   Vacuum|   40|          5|        40|\n",
      "| Bob|2018-02-18|   Gloves|    5|          5|         5|\n",
      "| Bob|2018-03-03|  Brushes|   30|          5|        30|\n",
      "| Bob|2018-07-12|   Bucket|    5|          5|         5|\n",
      "| Bob|2018-09-26|Sandpaper|   10|          5|        10|\n",
      "| Bob|2018-12-09|   Vacuum|   40|          5|        40|\n",
      "+----+----------+---------+-----+-----------+----------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Window [first(price#3L, false) windowspecdefinition(name#0, date#1 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS first_price#8230L, last(price#3L, false) windowspecdefinition(name#0, date#1 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_price#8232L], [name#0], [date#1 ASC NULLS FIRST]\n",
      "+- *(2) Sort [name#0 ASC NULLS FIRST, date#1 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(name#0, 200), ENSURE_REQUIREMENTS, [id=#667]\n",
      "      +- *(1) Scan ExistingRDD[name#0,date#1,product#2,price#3L]\n"
     ]
    }
   ],
   "source": [
    "df_all=df.select(col(\"*\"),first(\"price\").over(win_name_ordered_by_date).alias(\"first_price\"),last(\"price\").over(win_name_ordered_by_date).alias(\"last_price\"))\n",
    "\n",
    "df_all.show()\n",
    "df_all.explain()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First solution, you can add a custom bound to the window by using **rowsBetween**. Check below example\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----+\n",
      "|name|      date|  product|price|last|\n",
      "+----+----------+---------+-----+----+\n",
      "|Alex|2018-02-18|   Gloves|    5|  40|\n",
      "|Alex|2018-03-03|  Brushes|   30|  40|\n",
      "|Alex|2018-04-02|   Ladder|   20|  40|\n",
      "|Alex|2018-06-22|    Stool|   20|  40|\n",
      "|Alex|2018-07-12|   Bucket|    5|  40|\n",
      "|Alex|2018-09-26|Sandpaper|   10|  40|\n",
      "|Alex|2018-10-10|    Paint|   80|  40|\n",
      "|Alex|2018-12-09|   Vacuum|   40|  40|\n",
      "| Bob|2018-02-18|   Gloves|    5|  40|\n",
      "| Bob|2018-03-03|  Brushes|   30|  40|\n",
      "| Bob|2018-07-12|   Bucket|    5|  40|\n",
      "| Bob|2018-09-26|Sandpaper|   10|  40|\n",
      "| Bob|2018-12-09|   Vacuum|   40|  40|\n",
      "+----+----------+---------+-----+----+\n"
     ]
    }
   ],
   "source": [
    "# this adds a custom bound to the window. start is the Window.unboundedPreceding means the earliest, stop is Window.unboundedFollowing means the last\n",
    "custom_win_name=win_name_ordered_by_date.rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "df_last=df.withColumn(\"last\",last(col(\"price\")).over(custom_win_name))\n",
    "df_last.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The problem with this solution is that, we get last value of all data set. Not for each partition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Solution 2 : Use inverse order and take first\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----+\n",
      "|name|      date|  product|price|last|\n",
      "+----+----------+---------+-----+----+\n",
      "|Alex|2018-12-09|   Vacuum|   40|  40|\n",
      "|Alex|2018-10-10|    Paint|   80|  40|\n",
      "|Alex|2018-09-26|Sandpaper|   10|  40|\n",
      "|Alex|2018-07-12|   Bucket|    5|  40|\n",
      "|Alex|2018-06-22|    Stool|   20|  40|\n",
      "|Alex|2018-04-02|   Ladder|   20|  40|\n",
      "|Alex|2018-03-03|  Brushes|   30|  40|\n",
      "|Alex|2018-02-18|   Gloves|    5|  40|\n",
      "| Bob|2018-12-09|   Vacuum|   40|  40|\n",
      "| Bob|2018-09-26|Sandpaper|   10|  40|\n",
      "| Bob|2018-07-12|   Bucket|    5|  40|\n",
      "| Bob|2018-03-03|  Brushes|   30|  40|\n",
      "| Bob|2018-02-18|   Gloves|    5|  40|\n",
      "+----+----------+---------+-----+----+\n"
     ]
    }
   ],
   "source": [
    "win_name_ordered_by_reverse_date=win_name.orderBy(col(\"date\").desc())\n",
    "df_last=df.withColumn(\"last\",first(\"price\").over(win_name_ordered_by_reverse_date))\n",
    "df_last.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.4 Lag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- lag: long (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+----+\n",
      "|name|      date|  product|price| lag|\n",
      "+----+----------+---------+-----+----+\n",
      "|Alex|2018-07-12|   Bucket|    5|null|\n",
      "|Alex|2018-02-18|   Gloves|    5|null|\n",
      "|Alex|2018-09-26|Sandpaper|   10|null|\n",
      "|Alex|2018-04-02|   Ladder|   20|   5|\n",
      "|Alex|2018-06-22|    Stool|   20|   5|\n",
      "|Alex|2018-03-03|  Brushes|   30|  10|\n",
      "|Alex|2018-12-09|   Vacuum|   40|  20|\n",
      "|Alex|2018-10-10|    Paint|   80|  20|\n",
      "| Bob|2018-07-12|   Bucket|    5|null|\n",
      "| Bob|2018-02-18|   Gloves|    5|null|\n",
      "| Bob|2018-09-26|Sandpaper|   10|null|\n",
      "| Bob|2018-03-03|  Brushes|   30|   5|\n",
      "| Bob|2018-12-09|   Vacuum|   40|   5|\n",
      "+----+----------+---------+-----+----+\n"
     ]
    }
   ],
   "source": [
    "# create a new column that shows the lag value by using price.\n",
    "# The lag function takes a column name (e.g. price) and an offset (3). \n",
    "# note if we set offset as 2, the first two row of lag is null, and the third rows gets the first row value of the\n",
    "# price column. If we set offset as 3, the first three rows will be null, and the fourth rows get the first row\n",
    "# value.\n",
    "df_lag = df.withColumn(\"lag\", lag(\"price\", 3).over(win_name_ordered))\n",
    "df_lag.printSchema()\n",
    "df_lag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.5 Lead"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- lead: long (nullable = true)\n",
      "\n",
      "+----+----------+---------+-----+----+\n",
      "|name|      date|  product|price|lead|\n",
      "+----+----------+---------+-----+----+\n",
      "|Alex|2018-07-12|   Bucket|    5|  20|\n",
      "|Alex|2018-02-18|   Gloves|    5|  20|\n",
      "|Alex|2018-09-26|Sandpaper|   10|  30|\n",
      "|Alex|2018-04-02|   Ladder|   20|  40|\n",
      "|Alex|2018-06-22|    Stool|   20|  80|\n",
      "|Alex|2018-03-03|  Brushes|   30|null|\n",
      "|Alex|2018-12-09|   Vacuum|   40|null|\n",
      "|Alex|2018-10-10|    Paint|   80|null|\n",
      "| Bob|2018-07-12|   Bucket|    5|  30|\n",
      "| Bob|2018-02-18|   Gloves|    5|  40|\n",
      "| Bob|2018-09-26|Sandpaper|   10|null|\n",
      "| Bob|2018-03-03|  Brushes|   30|null|\n",
      "| Bob|2018-12-09|   Vacuum|   40|null|\n",
      "+----+----------+---------+-----+----+\n"
     ]
    }
   ],
   "source": [
    "# create a new column that pushes up 3 row of price column.\n",
    "# note if we set offset as 2, the last two row of lead is null in each partition, and the last third row gets the\n",
    "# value of last row of the price column. If we set offset as 3, the last three rows will be null, and the last\n",
    "# fourth rows get the last row value.\n",
    "df_lead = df.withColumn(\"lead\", lead(\"price\", 3).over(win_name_ordered))\n",
    "df_lead.printSchema()\n",
    "df_lead.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 1.\n",
    "Could you show the days from the previous purchase? Or the days before next purchase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price|  lag_date|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-02-18|   Gloves|    5|      null|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-02-18|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-03-03|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-04-02|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-06-22|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-07-12|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-09-26|\n",
      "|Alex|2018-12-09|   Vacuum|   40|2018-10-10|\n",
      "| Bob|2018-02-18|   Gloves|    5|      null|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-02-18|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-03-03|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-07-12|\n",
      "| Bob|2018-12-09|   Vacuum|   40|2018-09-26|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n",
      "+----+----------+---------+-----+----------+---------------------------+\n",
      "|name|      date|  product|price|  lag_date|days_from_previous_purchase|\n",
      "+----+----------+---------+-----+----------+---------------------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|      null|                       null|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-02-18|                         13|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-03-03|                         30|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-04-02|                         81|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-06-22|                         20|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-07-12|                         76|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-09-26|                         14|\n",
      "|Alex|2018-12-09|   Vacuum|   40|2018-10-10|                         60|\n",
      "| Bob|2018-02-18|   Gloves|    5|      null|                       null|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-02-18|                         13|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-03-03|                        131|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-07-12|                         76|\n",
      "| Bob|2018-12-09|   Vacuum|   40|2018-09-26|                         74|\n",
      "+----+----------+---------+-----+----------+---------------------------+\n",
      "\n",
      "+----+----------+---------+-----+----------+\n",
      "|name|      date|  product|price| lead_date|\n",
      "+----+----------+---------+-----+----------+\n",
      "|Alex|2018-02-18|   Gloves|    5|2018-03-03|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-04-02|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-06-22|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-07-12|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-09-26|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-10-10|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-12-09|\n",
      "|Alex|2018-12-09|   Vacuum|   40|      null|\n",
      "| Bob|2018-02-18|   Gloves|    5|2018-03-03|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-07-12|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-09-26|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-12-09|\n",
      "| Bob|2018-12-09|   Vacuum|   40|      null|\n",
      "+----+----------+---------+-----+----------+\n",
      "\n",
      "+----+----------+---------+-----+----------+-------------------------+\n",
      "|name|      date|  product|price| lead_date|days_before_next_purchase|\n",
      "+----+----------+---------+-----+----------+-------------------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|2018-03-03|                       13|\n",
      "|Alex|2018-03-03|  Brushes|   30|2018-04-02|                       30|\n",
      "|Alex|2018-04-02|   Ladder|   20|2018-06-22|                       81|\n",
      "|Alex|2018-06-22|    Stool|   20|2018-07-12|                       20|\n",
      "|Alex|2018-07-12|   Bucket|    5|2018-09-26|                       76|\n",
      "|Alex|2018-09-26|Sandpaper|   10|2018-10-10|                       14|\n",
      "|Alex|2018-10-10|    Paint|   80|2018-12-09|                       60|\n",
      "|Alex|2018-12-09|   Vacuum|   40|      null|                     null|\n",
      "| Bob|2018-02-18|   Gloves|    5|2018-03-03|                       13|\n",
      "| Bob|2018-03-03|  Brushes|   30|2018-07-12|                      131|\n",
      "| Bob|2018-07-12|   Bucket|    5|2018-09-26|                       76|\n",
      "| Bob|2018-09-26|Sandpaper|   10|2018-12-09|                       74|\n",
      "| Bob|2018-12-09|   Vacuum|   40|      null|                     null|\n",
      "+----+----------+---------+-----+----------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "# here we set lag on date column with offset 1, it means the second row will have the value of first row, then\n",
    "# apply the datediff function on this value with the current row date value, then we get days from the last\n",
    "# purchase.\n",
    "# Use the same logic by using lead, we get the days before next purchase, if we set offset as 2, we will get\n",
    "# the days before next 2 purchase\n",
    "\n",
    "# Step1 we create a new column called lag_date which is the date of previous purchase\n",
    "df_lag_day = df.withColumn(\"lag_date\",lag('date',1).over(win_name.orderBy(col('date'))))\n",
    "df_lag_day.show()\n",
    "\n",
    "# Step2, we create a column \"days\" by doing a date diff on \"date\" and \"lag_date\" \n",
    "df_diff=df_lag_day.withColumn(\"days_from_previous_purchase\",datediff(\"date\",\"lag_date\"))\n",
    "df_diff.show()\n",
    "\n",
    "# If you use lead\n",
    "# Step1 we create a new column called lead_date which is the date of previous purchase\n",
    "df_lead_day = df.withColumn(\"lead_date\",lead('date',1).over(win_name.orderBy(col('date'))))\n",
    "df_lead_day.show()\n",
    "\n",
    "# Step2, we create a column \"days\" by doing a date diff on \"date\" and \"lag_date\" \n",
    "df_diff=df_lead_day.withColumn(\"days_before_next_purchase\",datediff(\"lead_date\",\"date\"))\n",
    "df_diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "|name|      date|  product|price|days_from_last_purchase|days_before_next_purchase|\n",
      "+----+----------+---------+-----+-----------------------+-------------------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|                   null|                       13|\n",
      "|Alex|2018-03-03|  Brushes|   30|                     13|                       30|\n",
      "|Alex|2018-04-02|   Ladder|   20|                     30|                       81|\n",
      "|Alex|2018-06-22|    Stool|   20|                     81|                       20|\n",
      "|Alex|2018-07-12|   Bucket|    5|                     20|                       76|\n",
      "|Alex|2018-09-26|Sandpaper|   10|                     76|                       14|\n",
      "|Alex|2018-10-10|    Paint|   80|                     14|                       60|\n",
      "|Alex|2018-12-09|   Vacuum|   40|                     60|                     null|\n",
      "| Bob|2018-02-18|   Gloves|    5|                   null|                       13|\n",
      "| Bob|2018-03-03|  Brushes|   30|                     13|                      131|\n",
      "| Bob|2018-07-12|   Bucket|    5|                    131|                       76|\n",
      "| Bob|2018-09-26|Sandpaper|   10|                     76|                       74|\n",
      "| Bob|2018-12-09|   Vacuum|   40|                     74|                     null|\n",
      "+----+----------+---------+-----+-----------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "# You can do it in one line by putting lag function as parameter in datediff\n",
    "df_final=df.withColumn('days_from_last_purchase', datediff('date', lag('date', 1).over(win_name.orderBy(col('date'))))) \\\n",
    "        .withColumn('days_before_next_purchase', datediff(lead('date', 1).over(win_name.orderBy(col('date'))), 'date'))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Aggregation function example \n",
    "\n",
    "Show aggregation functions on ordered frame and basic partitionBy frame\n",
    "- avg/mean\n",
    "- min\n",
    "- max\n",
    "- sum\n",
    "\n",
    "In df1, we use a partition window specification, so the result is the same for all rows that are in the same partition.\n",
    "\n",
    "In df2, we use an ordered window specification, the result is different for each rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----+---+---+---+-----------+------------------------------------------------------------------+\n",
      "|name|date      |product  |price|avg  |sum|min|max|item_number|item_list                                                         |\n",
      "+----+----------+---------+-----+-----+---+---+---+-----------+------------------------------------------------------------------+\n",
      "|Alex|2018-10-10|Paint    |80   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-04-02|Ladder   |20   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-06-22|Stool    |20   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-12-09|Vacuum   |40   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-07-12|Bucket   |5    |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-02-18|Gloves   |5    |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-03-03|Brushes  |30   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Alex|2018-09-26|Sandpaper|10   |26.25|210|5  |80 |8          |[Paint, Ladder, Stool, Vacuum, Bucket, Gloves, Brushes, Sandpaper]|\n",
      "|Bob |2018-12-09|Vacuum   |40   |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-07-12|Bucket   |5    |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-02-18|Gloves   |5    |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-03-03|Brushes  |30   |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "|Bob |2018-09-26|Sandpaper|10   |18.0 |90 |5  |40 |5          |[Vacuum, Bucket, Gloves, Brushes, Sandpaper]                      |\n",
      "+----+----------+---------+-----+-----+---+---+---+-----------+------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+-----------+-----------+-----------+-----------+-------------------+------------------------------------------------------------------+\n",
      "|name|date      |product  |price|avg_to_date|sum_to_date|max_to_date|min_to_date|item_number_to_date|item_list_to_date                                                 |\n",
      "+----+----------+---------+-----+-----------+-----------+-----------+-----------+-------------------+------------------------------------------------------------------+\n",
      "|Alex|2018-02-18|Gloves   |5    |5.0        |5          |5          |5          |1                  |[Gloves]                                                          |\n",
      "|Alex|2018-03-03|Brushes  |30   |17.5       |35         |30         |30         |2                  |[Gloves, Brushes]                                                 |\n",
      "|Alex|2018-04-02|Ladder   |20   |18.33      |55         |30         |30         |3                  |[Gloves, Brushes, Ladder]                                         |\n",
      "|Alex|2018-06-22|Stool    |20   |18.75      |75         |30         |30         |4                  |[Gloves, Brushes, Ladder, Stool]                                  |\n",
      "|Alex|2018-07-12|Bucket   |5    |16.0       |80         |30         |30         |5                  |[Gloves, Brushes, Ladder, Stool, Bucket]                          |\n",
      "|Alex|2018-09-26|Sandpaper|10   |15.0       |90         |30         |30         |6                  |[Gloves, Brushes, Ladder, Stool, Bucket, Sandpaper]               |\n",
      "|Alex|2018-10-10|Paint    |80   |24.29      |170        |80         |80         |7                  |[Gloves, Brushes, Ladder, Stool, Bucket, Sandpaper, Paint]        |\n",
      "|Alex|2018-12-09|Vacuum   |40   |26.25      |210        |80         |80         |8                  |[Gloves, Brushes, Ladder, Stool, Bucket, Sandpaper, Paint, Vacuum]|\n",
      "|Bob |2018-02-18|Gloves   |5    |5.0        |5          |5          |5          |1                  |[Gloves]                                                          |\n",
      "|Bob |2018-03-03|Brushes  |30   |17.5       |35         |30         |30         |2                  |[Gloves, Brushes]                                                 |\n",
      "|Bob |2018-07-12|Bucket   |5    |13.33      |40         |30         |30         |3                  |[Gloves, Brushes, Bucket]                                         |\n",
      "|Bob |2018-09-26|Sandpaper|10   |12.5       |50         |30         |30         |4                  |[Gloves, Brushes, Bucket, Sandpaper]                              |\n",
      "|Bob |2018-12-09|Vacuum   |40   |18.0       |90         |40         |40         |5                  |[Gloves, Brushes, Bucket, Sandpaper, Vacuum]                      |\n",
      "+----+----------+---------+-----+-----------+-----------+-----------+-----------+-------------------+------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We apply the aggregation function on the window specification that only has partition, so the \n",
    "# result is the same for all rows that are in the same \n",
    "df1 = df.withColumn(\"avg\", avg(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"sum\", sum(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"min\", min(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"max\", max(col(\"price\")).over(win_name)) \\\n",
    "        .withColumn(\"item_number\", count(\"*\").over(win_name)) \\\n",
    "        .withColumn(\"item_list\", collect_list(col(\"product\")).over(win_name))\n",
    "   \n",
    "df1.show(truncate=False)\n",
    "\n",
    "# if we apply aggregation function on a windows spec with order, you will get a cumulative result for each rows\n",
    "df2 = df.withColumn('avg_to_date', round(avg('price').over(win_name_ordered_by_date), 2)) \\\n",
    "        .withColumn('sum_to_date', sum('price').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn('max_to_date', max('price').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn('min_to_date', max('price').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn('item_number_to_date', count('*').over(win_name_ordered_by_date)) \\\n",
    "        .withColumn(\"item_list_to_date\", collect_list(col(\"product\")).over(win_name_ordered_by_date))\n",
    "\n",
    "   \n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Range window specifications example\n",
    "\n",
    "Range window specification will create a sub window inside the main window (created by Window.partitionBy). **Only aggregation functions can apply over range window. Rank or analytic function will raise errors**. To build range window specifications, we need to use the two following functions \n",
    "- rowsBetween(start:Long,end:Long)-> WindowSpec : **Here start, end are the `index` of rows relative to current rows**, -1 means 1 row before current row, 1 mean 1 row after current row\n",
    "- rangeBetween(start:Long, end:Long)-> WindowSpec : **The start, end boundary in rangeBetween is based on `row value` relative to currentRow**. The value definition of the constant values used in range functions:\n",
    "   - Window.currentRow = 0\n",
    "   - Window.unboundedPreceding = Long.MinValue\n",
    "   - Window.unboundedFollowing = Long.MaxValue\n",
    "\n",
    "The (start, end) index are all-inclusive. Their value can be \n",
    "- Window.unboundedPreceding\n",
    "- Window.unboundedFollowing\n",
    "- Window.currentRow. \n",
    "- Or a value relative to Window.currentRow, either negative or positive.\n",
    "\n",
    "Some examples of rowsBetween:\n",
    "- rowsBetween(Window.currentRow, 2): From current row to the next 2 rows \n",
    "- rowsBetween(-3, Window.currentRow): From the previous 3 rows to the current row. \n",
    "- rowsBetween(-1, 2): Frame contains previous row, current row and the next 2 rows \n",
    "- rowsBetween(Window.currentRow, Window.unboundedFollowing): From current row to all next rows \n",
    "- rowsBetween(Window.unboundedPreceding, Window.currentRow): From all previous rows to the current row. \n",
    "- rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing): all rows in the window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 86400 seconds in a day\n",
    "def day_to_seconds(day_num: int):\n",
    "    return day_num * 86400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1  rowsBetween example\n",
    "\n",
    "rowsBetween uses current row as base index (i.e. 0), and offset to specify start or end.\n",
    "- -1: one row before current row\n",
    "- 0: current row\n",
    "- 1: one row after current row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------+\n",
      "|name|date      |product  |price|max_of_last2|\n",
      "+----+----------+---------+-----+------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |5           |\n",
      "|Alex|2018-02-18|Gloves   |5    |5           |\n",
      "|Alex|2018-09-26|Sandpaper|10   |10          |\n",
      "|Alex|2018-04-02|Ladder   |20   |20          |\n",
      "|Alex|2018-06-22|Stool    |20   |20          |\n",
      "|Alex|2018-03-03|Brushes  |30   |30          |\n",
      "|Alex|2018-12-09|Vacuum   |40   |40          |\n",
      "|Alex|2018-10-10|Paint    |80   |80          |\n",
      "|Bob |2018-07-12|Bucket   |5    |5           |\n",
      "|Bob |2018-02-18|Gloves   |5    |5           |\n",
      "|Bob |2018-09-26|Sandpaper|10   |10          |\n",
      "|Bob |2018-03-03|Brushes  |30   |30          |\n",
      "|Bob |2018-12-09|Vacuum   |40   |40          |\n",
      "+----+----------+---------+-----+------------+\n",
      "\n",
      "+----+----------+---------+-----+-------------------------+\n",
      "|name|date      |product  |price|max_from_previous_to_next|\n",
      "+----+----------+---------+-----+-------------------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |5                        |\n",
      "|Alex|2018-02-18|Gloves   |5    |10                       |\n",
      "|Alex|2018-09-26|Sandpaper|10   |20                       |\n",
      "|Alex|2018-04-02|Ladder   |20   |20                       |\n",
      "|Alex|2018-06-22|Stool    |20   |30                       |\n",
      "|Alex|2018-03-03|Brushes  |30   |40                       |\n",
      "|Alex|2018-12-09|Vacuum   |40   |80                       |\n",
      "|Alex|2018-10-10|Paint    |80   |80                       |\n",
      "|Bob |2018-07-12|Bucket   |5    |5                        |\n",
      "|Bob |2018-02-18|Gloves   |5    |10                       |\n",
      "|Bob |2018-09-26|Sandpaper|10   |30                       |\n",
      "|Bob |2018-03-03|Brushes  |30   |40                       |\n",
      "|Bob |2018-12-09|Vacuum   |40   |40                       |\n",
      "+----+----------+---------+-----+-------------------------+\n",
      "\n",
      "+----+----------+---------+-----+----------------+\n",
      "|name|date      |product  |price|max_of_following|\n",
      "+----+----------+---------+-----+----------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |80              |\n",
      "|Alex|2018-02-18|Gloves   |5    |80              |\n",
      "|Alex|2018-09-26|Sandpaper|10   |80              |\n",
      "|Alex|2018-04-02|Ladder   |20   |80              |\n",
      "|Alex|2018-06-22|Stool    |20   |80              |\n",
      "|Alex|2018-03-03|Brushes  |30   |80              |\n",
      "|Alex|2018-12-09|Vacuum   |40   |80              |\n",
      "|Alex|2018-10-10|Paint    |80   |80              |\n",
      "|Bob |2018-07-12|Bucket   |5    |40              |\n",
      "|Bob |2018-02-18|Gloves   |5    |40              |\n",
      "|Bob |2018-09-26|Sandpaper|10   |40              |\n",
      "|Bob |2018-03-03|Brushes  |30   |40              |\n",
      "|Bob |2018-12-09|Vacuum   |40   |40              |\n",
      "+----+----------+---------+-----+----------------+\n"
     ]
    }
   ],
   "source": [
    "# last 2 row(current and the row before it) range window specification\n",
    "# Below range window takes the current row and the row before it. So it's the last 2. \n",
    "last2 = win_name_ordered.rowsBetween(-1, Window.currentRow)\n",
    "df.withColumn(\"max_of_last2\", max(\"price\").over(last2)).show(truncate=False)\n",
    "\n",
    "# Below range window takes the row before, current row, and the row after it\n",
    "privious_to_next = win_name_ordered.rowsBetween(-1,1)\n",
    "df.withColumn(\"max_from_previous_to_next\", max(\"price\").over(privious_to_next)).show(truncate=False)\n",
    "\n",
    "# max of all following row\n",
    "# Below range takes the current row and all the row behind it that are in the same partition.\n",
    "following = win_name_ordered.rowsBetween(Window.currentRow, Window.unboundedFollowing),\n",
    "df.withColumn(\"max_of_following\", max(\"price\").over(following)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2  rangeBetween example\n",
    "\n",
    "rowsBetween uses current **row value** as base index (i.e. 0), and offset to specify start or end.\n",
    "- -666: all the row that contains value is < current row value and > current_row_value - 666 \n",
    "- 0: current row\n",
    "- 666: all the row that contains value is > current row value and < current_row_value + 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+-----+----------+\n",
      "|name|date      |product|price|unix_date |\n",
      "+----+----------+-------+-----+----------+\n",
      "|Alex|2018-10-10|Paint  |80   |1539122400|\n",
      "|Alex|2018-04-02|Ladder |20   |1522620000|\n",
      "|Alex|2018-06-22|Stool  |20   |1529618400|\n",
      "|Alex|2018-12-09|Vacuum |40   |1544310000|\n",
      "|Alex|2018-07-12|Bucket |5    |1531346400|\n",
      "+----+----------+-------+-----+----------+\n"
     ]
    }
   ],
   "source": [
    "# convert string date column to unix timestamp\n",
    "\n",
    "df1 = df.withColumn(\"unix_date\", unix_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "    \n",
    "df1.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz2, how to get the avg  price of sold proucts of the last 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp4 create a column that shows last 30 day avg before current row date\n",
      "+----+----------+---------+-----+----------+----------------+\n",
      "|name|date      |product  |price|unix_date |30day_moving_avg|\n",
      "+----+----------+---------+-----+----------+----------------+\n",
      "|Alex|2018-02-18|Gloves   |5    |1518908400|5.0             |\n",
      "|Alex|2018-03-03|Brushes  |30   |1520031600|17.5            |\n",
      "|Alex|2018-04-02|Ladder   |20   |1522620000|25.0            |\n",
      "|Alex|2018-06-22|Stool    |20   |1529618400|20.0            |\n",
      "|Alex|2018-07-12|Bucket   |5    |1531346400|12.5            |\n",
      "|Alex|2018-09-26|Sandpaper|10   |1537912800|10.0            |\n",
      "|Alex|2018-10-10|Paint    |80   |1539122400|45.0            |\n",
      "|Alex|2018-12-09|Vacuum   |40   |1544310000|40.0            |\n",
      "|Bob |2018-02-18|Gloves   |5    |1518908400|5.0             |\n",
      "|Bob |2018-03-03|Brushes  |30   |1520031600|17.5            |\n",
      "+----+----------+---------+-----+----------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# We create a specific range window, the end is the date of current row, the start is the date 30 days \n",
    "# before current row\n",
    "# Here 0 is the relative unix_date of current row, the frame boundary of rangeBetween(-day_to_seconds(30), 0)\n",
    "# For example, row \"Alex|2018-02-18|Gloves |5 |1518908400|\" will be (1518908400-(30*86400),1518908400). All rows that\n",
    "# have unix_date column value in this frame boundary will be included in the frame.\n",
    "range_30 = win_name.orderBy(col(\"unix_date\")).rangeBetween(-day_to_seconds(30), 0)\n",
    "df2 = df1.withColumn(\"30day_moving_avg\", avg(\"price\").over(range_30))\n",
    "print(\"Exp4 create a column that shows last 30 day avg before current row date\")\n",
    "df2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz3, how to get the avg of 30 day before and 15 days after the current row date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+----------------+------------------+\n",
      "|name|date      |product  |price|unix_date |45day_moving_avg|45day_moving_std  |\n",
      "+----+----------+---------+-----+----------+----------------+------------------+\n",
      "|Alex|2018-02-18|Gloves   |5    |1518908400|17.5            |17.67766952966369 |\n",
      "|Alex|2018-03-03|Brushes  |30   |1520031600|17.5            |17.67766952966369 |\n",
      "|Alex|2018-04-02|Ladder   |20   |1522620000|25.0            |7.0710678118654755|\n",
      "|Alex|2018-06-22|Stool    |20   |1529618400|20.0            |null              |\n",
      "|Alex|2018-07-12|Bucket   |5    |1531346400|12.5            |10.606601717798213|\n",
      "|Alex|2018-09-26|Sandpaper|10   |1537912800|45.0            |49.49747468305833 |\n",
      "|Alex|2018-10-10|Paint    |80   |1539122400|45.0            |49.49747468305833 |\n",
      "|Alex|2018-12-09|Vacuum   |40   |1544310000|40.0            |null              |\n",
      "|Bob |2018-02-18|Gloves   |5    |1518908400|17.5            |17.67766952966369 |\n",
      "|Bob |2018-03-03|Brushes  |30   |1520031600|17.5            |17.67766952966369 |\n",
      "+----+----------+---------+-----+----------+----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "# Note that stddev of some row will return null. Because it requires at least two\n",
    "# observations to calculate standard deviation.\n",
    "range_45 = win_name.orderBy(\"unix_date\").rangeBetween(-day_to_seconds(30), day_to_seconds(15))\n",
    "df3 = df1.withColumn(\"45day_moving_avg\", avg(\"price\").over(range_45)) \\\n",
    "        .withColumn(\"45day_moving_std\", stddev(\"price\").over(range_45))\n",
    "df3.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz4, how to get the median of a window\n",
    "\n",
    "mean(avg) and median are commonly used in statistics. \n",
    "\n",
    "- mean is cheap to calculate, but outliers can have large effect. For example, the income of population, if we have 9 people has 10 dollar, and 1 person has 1010 dollar. The mean is 1100/10= 110. It does not represent any group's income. \n",
    "- Median is expansive to calculate. But in certain cases median are more robust comparing to mean, since it will filter out outlier values. If we retake the previous example, the median will be 10 dollar, which represent a group's income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------------------------+--------------+\n",
      "|name|date      |product  |price|price_list                    |rolling_median|\n",
      "+----+----------+---------+-----+------------------------------+--------------+\n",
      "|Alex|2018-07-12|Bucket   |5    |[5, 5]                        |5             |\n",
      "|Alex|2018-02-18|Gloves   |5    |[5, 5]                        |5             |\n",
      "|Alex|2018-09-26|Sandpaper|10   |[5, 5, 10]                    |5             |\n",
      "|Alex|2018-04-02|Ladder   |20   |[5, 5, 10, 20, 20]            |10            |\n",
      "|Alex|2018-06-22|Stool    |20   |[5, 5, 10, 20, 20]            |10            |\n",
      "|Alex|2018-03-03|Brushes  |30   |[5, 5, 10, 20, 20, 30]        |20            |\n",
      "|Alex|2018-12-09|Vacuum   |40   |[5, 5, 10, 20, 20, 30, 40]    |20            |\n",
      "|Alex|2018-10-10|Paint    |80   |[5, 5, 10, 20, 20, 30, 40, 80]|20            |\n",
      "|Bob |2018-07-12|Bucket   |5    |[5, 5]                        |5             |\n",
      "|Bob |2018-02-18|Gloves   |5    |[5, 5]                        |5             |\n",
      "|Bob |2018-09-26|Sandpaper|10   |[5, 5, 10]                    |5             |\n",
      "|Bob |2018-03-03|Brushes  |30   |[5, 5, 10, 30]                |10            |\n",
      "|Bob |2018-12-09|Vacuum   |40   |[5, 5, 10, 30, 40]            |10            |\n",
      "+----+----------+---------+-----+------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "win1=Window.partitionBy('name')\n",
    "win2=Window.partitionBy('name').orderBy('price')\n",
    "\n",
    "\n",
    "# In this example, we calculate a rolling median, because for each row, the price_list grows a little bit.\n",
    "df1=df.withColumn(\"price_list\",collect_list('price').over(win2)) \\\n",
    "      .withColumn(\"rolling_median\",element_at(\"price_list\",(size(\"price_list\")/2+1).cast(\"int\")))\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+--------------------+-------------+\n",
      "|name|      date|  product|price|          price_list|global_median|\n",
      "+----+----------+---------+-----+--------------------+-------------+\n",
      "|Alex|2018-10-10|    Paint|   80|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-04-02|   Ladder|   20|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-06-22|    Stool|   20|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-12-09|   Vacuum|   40|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-07-12|   Bucket|    5|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-02-18|   Gloves|    5|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-03-03|  Brushes|   30|[5, 5, 10, 20, 20...|           20|\n",
      "|Alex|2018-09-26|Sandpaper|   10|[5, 5, 10, 20, 20...|           20|\n",
      "| Bob|2018-12-09|   Vacuum|   40|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-07-12|   Bucket|    5|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-02-18|   Gloves|    5|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-03-03|  Brushes|   30|  [5, 5, 10, 30, 40]|           10|\n",
      "| Bob|2018-09-26|Sandpaper|   10|  [5, 5, 10, 30, 40]|           10|\n",
      "+----+----------+---------+-----+--------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "# In this example, we calculate a global median for each window\n",
    "# note, as the window specification only has partition, does not have order, the collect list need to be sorted\n",
    "\n",
    "df2=df.withColumn(\"price_list\",sort_array(collect_list('price').over(win1))) \\\n",
    "      .withColumn(\"global_median\",element_at(\"price_list\",(size(\"price_list\")/2+1).cast(\"int\")))\n",
    "  \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|name|          price_list|\n",
      "+----+--------------------+\n",
      "|Alex|[5, 5, 10, 20, 20...|\n",
      "| Bob|  [5, 5, 10, 30, 40]|\n",
      "+----+--------------------+\n",
      "\n",
      "+----+--------------------+------+\n",
      "|name|          price_list|median|\n",
      "+----+--------------------+------+\n",
      "|Alex|[5, 5, 10, 20, 20...|    20|\n",
      "| Bob|  [5, 5, 10, 30, 40]|    10|\n",
      "+----+--------------------+------+\n"
     ]
    }
   ],
   "source": [
    "# We can also use groupBy\n",
    "\n",
    "df3=df.groupBy(\"name\").agg(sort_array(collect_list(\"price\")).alias(\"price_list\"))\n",
    "df3.show()\n",
    "\n",
    "df4=df3.select(\"name\",\"price_list\",(element_at(\"price_list\", (size(\"price_list\")/2+1).cast(\"int\")).alias(\"median\")))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------------------------+------+\n",
      "|name|date      |product  |price|price_list                    |median|\n",
      "+----+----------+---------+-----+------------------------------+------+\n",
      "|Alex|2018-10-10|Paint    |80   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-04-02|Ladder   |20   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-06-22|Stool    |20   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-12-09|Vacuum   |40   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-07-12|Bucket   |5    |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-02-18|Gloves   |5    |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-03-03|Brushes  |30   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Alex|2018-09-26|Sandpaper|10   |[5, 5, 10, 20, 20, 30, 40, 80]|20    |\n",
      "|Bob |2018-12-09|Vacuum   |40   |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-07-12|Bucket   |5    |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-02-18|Gloves   |5    |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-03-03|Brushes  |30   |[5, 5, 10, 30, 40]            |10    |\n",
      "|Bob |2018-09-26|Sandpaper|10   |[5, 5, 10, 30, 40]            |10    |\n",
      "+----+----------+---------+-----+------------------------------+------+\n"
     ]
    }
   ],
   "source": [
    "# The pyspark.sql.functions.broadcast(df) marks a DataFrame as small enough for use in broadcast joins.\n",
    "df.join(broadcast(df4), \"name\", \"inner\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
