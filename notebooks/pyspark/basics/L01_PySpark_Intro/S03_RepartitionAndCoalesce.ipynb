{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3 Partitions of dataframe and dataset\n",
    "Spark is a distributed calculation framework. So data such as rdd, dataframe and dataset are divided into parts. All\n",
    "parts are called partitions.\n",
    "\n",
    "When we create a RDD or data frame, data set. A default partition is given. The number of their partition equals the\n",
    "number of executors in the spark cluster. To improve your computation, you may want to change the default partition\n",
    "number. The number of partition can be modified by two methods:\n",
    "- repartition: is used to increase or decrease the partitions\n",
    "- coalesce: is used only to reduce the number of partitions.\n",
    "\n",
    "Note coalesce is more efficient than repartition which means fewer data movement across the cluster. As a result,\n",
    "if you can use coalesce, do not use repartition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/01 10:21:07 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.184.141 instead (on interface ens33)\n",
      "21/08/01 10:21:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/01 10:21:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.executor.memory\", \"2g\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 RDD repartition and coalesce\n",
    "\n",
    "We can check the partition number by using the getNumPartitions() function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default rdd partition: 4\n",
      "rdd_fix_partition partition: 8\n",
      "After repartition rdd has partition: 12\n",
      "After coalesce rdd has partition: 2\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "# The default partition is 4 because I have 4 executor in my cluster(i.e. local[4])\n",
    "print(\"Default rdd partition: {}\".format(rdd.getNumPartitions()))\n",
    "\n",
    "# if you don't want to use the default partition, you can give a specific partition\n",
    "rdd_fix_partition = spark.sparkContext.parallelize(data, 8)\n",
    "print(\"rdd_fix_partition partition: {}\".format(rdd_fix_partition.getNumPartitions()))\n",
    "\n",
    "# The same patter also works for textFile(), I will not show them here\n",
    "\n",
    "# When you save data to disk, the number of partition is the number of part file. Below will generate four files\n",
    "# rdd.saveAsTextFile(\"/tmp/partition\")\n",
    "# You will find 4 files in /tmp/partition\n",
    "# - part-00000:1,2,3\n",
    "# - part-00001:4,5,6\n",
    "# - part-00002:7,8,9\n",
    "# - part-00003:10,11,12\n",
    "# The skewness of data is 0. Because data is split into 4 partition evenly.\n",
    "\n",
    "# We can change the partition to 12\n",
    "rdd_repart = rdd.repartition(12)\n",
    "print(\"After repartition rdd has partition: {}\".format(rdd_repart.getNumPartitions()))\n",
    "\n",
    "# We can change the partition to 2\n",
    "rdd_coalesce = rdd.coalesce(2)\n",
    "print(\"After coalesce rdd has partition: {}\".format(rdd_coalesce.getNumPartitions()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 DataFrame repartition\n",
    "\n",
    "We can create a data frame by using spark session, or RDD. You can notice, none of them allow us to give a specific\n",
    "partition number. So the spark session will give us a default partition number, which is the number of executor.\n",
    "In our case, it's 4\n",
    "- SPARKSESSION: createDataFrame(rdd)/(dataList)/(rowData,columns)/(dataList,schema)/read()\n",
    "- RDD: toDF()/(*cols)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n",
      "Default data frame partition: 4\n",
      "After repartition(8) data frame partition: 8\n",
      "After coalesce(2) data frame partition: 2\n",
      "+---+----+\n",
      "| id|plus|\n",
      "+---+----+\n",
      "|  0|   2|\n",
      "|  1|   3|\n",
      "|  2|   4|\n",
      "|  3|   5|\n",
      "|  4|   6|\n",
      "|  5|   7|\n",
      "|  6|   8|\n",
      "|  7|   9|\n",
      "|  8|  10|\n",
      "|  9|  11|\n",
      "| 10|  12|\n",
      "| 11|  13|\n",
      "| 12|  14|\n",
      "| 13|  15|\n",
      "| 14|  16|\n",
      "| 15|  17|\n",
      "| 16|  18|\n",
      "| 17|  19|\n",
      "| 18|  20|\n",
      "| 19|  21|\n",
      "+---+----+\n",
      "\n",
      "After withColumn data frame partition: 4\n",
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  0|    1|\n",
      "|  1|    1|\n",
      "|  2|    1|\n",
      "|  3|    1|\n",
      "|  4|    1|\n",
      "|  5|    1|\n",
      "|  6|    1|\n",
      "|  7|    1|\n",
      "|  8|    1|\n",
      "|  9|    1|\n",
      "| 10|    1|\n",
      "| 11|    1|\n",
      "| 12|    1|\n",
      "| 13|    1|\n",
      "| 14|    1|\n",
      "| 15|    1|\n",
      "| 16|    1|\n",
      "| 17|    1|\n",
      "| 18|    1|\n",
      "| 19|    1|\n",
      "+---+-----+\n",
      "\n",
      "After GroupBy data frame partition: 4\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(0, 20)\n",
    "df.show()\n",
    "# note data frame does not provide function to get partition number, we need to convert dataframe to rdd first\n",
    "# data frame is build on top of rdd. So it has the same partition number as the base rdd.\n",
    "print(\"Default data frame partition: {}\".format(df.rdd.getNumPartitions()))\n",
    "df_repart = df.repartition(8)\n",
    "print(\"After repartition(8) data frame partition: {}\".format(df_repart.rdd.getNumPartitions()))\n",
    "df_coalesce = df.coalesce(2)\n",
    "print(\"After coalesce(2) data frame partition: {}\".format(df_coalesce.rdd.getNumPartitions()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 Dataframe auto-repartition after transformation which trigger shuffling.\n",
    "\n",
    "If the transformation(e.g. groupBy, union, join) triggers a shuffle, the data frame will be transferred\n",
    "between multiple executors and even machines. This will lead to an automatic repartition of your dataframe. The\n",
    "default partition number is **200**. You can change this default number by using the **spark.sql.shuffle.partitions**\n",
    "configuration.\n",
    "\n",
    "Note if your sparkSession is local, you will note see the 200 partition number. This only apply to spark session in a\n",
    "cluster.\n",
    "\n",
    "In pyspark, you can add following line\n",
    "``` python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",10)\n",
    "```\n",
    "\n",
    "# In scala, you can add following line\n",
    "``` scala\n",
    "import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS\n",
    "spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 2)\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|plus|\n",
      "+---+----+\n",
      "|  0|   2|\n",
      "|  1|   3|\n",
      "|  2|   4|\n",
      "|  3|   5|\n",
      "|  4|   6|\n",
      "|  5|   7|\n",
      "|  6|   8|\n",
      "|  7|   9|\n",
      "|  8|  10|\n",
      "|  9|  11|\n",
      "| 10|  12|\n",
      "| 11|  13|\n",
      "| 12|  14|\n",
      "| 13|  15|\n",
      "| 14|  16|\n",
      "| 15|  17|\n",
      "| 16|  18|\n",
      "| 17|  19|\n",
      "| 18|  20|\n",
      "| 19|  21|\n",
      "+---+----+\n",
      "\n",
      "After withColumn data frame partition: 4\n",
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  0|    1|\n",
      "|  1|    1|\n",
      "|  2|    1|\n",
      "|  3|    1|\n",
      "|  4|    1|\n",
      "|  5|    1|\n",
      "|  6|    1|\n",
      "|  7|    1|\n",
      "|  8|    1|\n",
      "|  9|    1|\n",
      "| 10|    1|\n",
      "| 11|    1|\n",
      "| 12|    1|\n",
      "| 13|    1|\n",
      "| 14|    1|\n",
      "| 15|    1|\n",
      "| 16|    1|\n",
      "| 17|    1|\n",
      "| 18|    1|\n",
      "| 19|    1|\n",
      "+---+-----+\n",
      "\n",
      "After GroupBy data frame partition: 4\n"
     ]
    }
   ],
   "source": [
    "# Note if you don't do data transformation which triggers shuffling, the partition of the result data frame does\n",
    "# not change.\n",
    "df_no_shuffle = df.withColumn(\"plus\", df.id + 2)\n",
    "df_no_shuffle.show()\n",
    "print(\"After withColumn data frame partition: {}\".format(df_no_shuffle.rdd.getNumPartitions()))\n",
    "\n",
    "\n",
    "# groupBy triggers a shuffle, thus the dataframe will be repartitioned to the default 200 partitions.\n",
    "df_with_shuffle = df.groupBy(\"id\").count()\n",
    "df_with_shuffle.show()\n",
    "print(\"After GroupBy data frame partition: {}\".format(df_with_shuffle.rdd.getNumPartitions()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}