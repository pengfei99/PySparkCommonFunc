{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3.4 Read write json files\n",
    "The official doc can be found [here](https://spark.apache.org/docs/latest/sql-data-sources-json.html). Spark SQL can automatically infer the schema of a JSON dataset and load it as a Dataset[Row]. This conversion can be done using SparkSession.read.json() on either a Dataset[String], or a JSON file.\n",
    "\n",
    "\n",
    "You may encounter three different situations when you read json files:\n",
    "- A text file containing complete JSON objects, one per line. This is typical when you are loading JSON files to Databricks tables.\n",
    "- A text file containing various fields (columns) of data, one of which is a JSON object. This is often seen in computer logs, where there is some plain-text meta-data followed by more detail in a JSON string.\n",
    "- A variation of the above where the JSON field is an array of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/18 08:56:01 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/02/18 08:56:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/18 08:56:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "if local:\n",
    "    spark = SparkSession.builder.master(\"local[4]\") \\\n",
    "        .appName(\"ReadWriteJson\").getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"ReadWriteJson\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", os.environ[\"IMAGE_NAME\"])\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.kubernetes.driver.pod.name\", os.environ[\"POD_NAME\"])\\\n",
    "        .config('spark.jars.packages', 'org.postgresql:postgresql:42.2.24')\\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.1 Read standard json file\n",
    "\n",
    "Standard JSON text files look like this\n",
    "```text\n",
    "{ \"Text1\":\"hello\", \"Text2\":\"goodbye\", \"Num1\":5, \"Array1\":[7,8,9] }\n",
    "{ \"Text1\":\"this\", \"Text2\":\"that\", \"Num1\":6.6, \"Array1\":[77,88,99] }\n",
    "{ \"Text1\":\"yes\", \"Text2\":\"no\", \"Num1\":-0.03, \"Array1\":[555,444,222] }\n",
    "```\n",
    "It contains a set of {}, each {} is a row of your dataframe. The field name will be translated as column name, the value will be row value. Note the value type can be String, Int, Array, or object(embedded structure type).\n",
    "\n",
    "Below shows an example on how to read standard json file. Note unlike read csv, when spark read json by default the inferSchema is activated."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"data/json/adult.json\"\n",
    "\n",
    "df = spark.read.json(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+---------+-------------+------+--------------+------+------------------+--------------+-----------------+-----+-------------+------+----------------+\n",
      "|age|capital-gain|capital-loss|education|education-num|fnlwgt|hours-per-week|income|    marital-status|native-country|       occupation| race| relationship|   sex|       workclass|\n",
      "+---+------------+------------+---------+-------------+------+--------------+------+------------------+--------------+-----------------+-----+-------------+------+----------------+\n",
      "| 39|        2174|           0|Bachelors|           13| 77516|            40| <=50K|     Never-married| United-States|     Adm-clerical|White|Not-in-family|  Male|       State-gov|\n",
      "| 50|           0|           0|Bachelors|           13| 83311|            13| <=50K|Married-civ-spouse| United-States|  Exec-managerial|White|      Husband|  Male|Self-emp-not-inc|\n",
      "| 38|           0|           0|  HS-grad|            9|215646|            40| <=50K|          Divorced| United-States|Handlers-cleaners|White|Not-in-family|  Male|         Private|\n",
      "| 53|           0|           0|     11th|            7|234721|            40| <=50K|Married-civ-spouse| United-States|Handlers-cleaners|Black|      Husband|  Male|         Private|\n",
      "| 28|           0|           0|Bachelors|           13|338409|            40| <=50K|Married-civ-spouse|          Cuba|   Prof-specialty|Black|         Wife|Female|         Private|\n",
      "+---+------------+------------+---------+-------------+------+--------------+------+------------------+--------------+-----------------+-----+-------------+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- capital-gain: long (nullable = true)\n",
      " |-- capital-loss: long (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: long (nullable = true)\n",
      " |-- fnlwgt: long (nullable = true)\n",
      " |-- hours-per-week: long (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.2 Read multi line json file\n",
    "\n",
    "As we mentioned in previous section, A normal json file has the below format\n",
    "```text\n",
    "{record_1}\n",
    "{record_2}\n",
    "{record_3}\n",
    "```\n",
    "And each record shares the same schema.\n",
    "\n",
    "\n",
    "A json file that has multiple lines has the following form:\n",
    "```text\n",
    "[\n",
    "{record_1},\n",
    "{record_2},\n",
    "{record_3}\n",
    "]\n",
    "```\n",
    "\n",
    "The records are located in a list.\n",
    "\n",
    "PySpark JSON reader accept only the normal json format. To read JSON files scattered across multiple lines, we must\n",
    "activate the \"multiline\" option. By default, the \"multiline\" option, is set to false.\n",
    "\n",
    "Try to change multiline option to false, and see what happens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "multi_line_path = \"data/json/zipcode.json\"\n",
    "\n",
    "df1 = spark.read.option(\"multiline\", \"true\").json(multi_line_path)\n",
    "df1.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------+-------+\n",
      "|               City|RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.3 Read multiple files\n",
    "\n",
    "We can also read multiple json file at the same time. Note the file path must be in a list. Below is an example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "file1 = \"data/json/zipcode.json\"\n",
    "file2 = \"data/json/zipcode1.json\"\n",
    "file3 = \"data/json/zipcode2.json\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------+-------+\n",
      "|               City|RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"multiline\", \"true\").json([file1, file2, file3])\n",
    "df2.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.4. Read Text file with json field\n",
    "\n",
    "You may encounter a csv file that contains a json column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------------------------+\n",
      "|Text1|Text2  |size |user                    |\n",
      "+-----+-------+-----+------------------------+\n",
      "|hello|goodbye|5.0  |{\"name\":\"john\",\"age\":3} |\n",
      "|this |that   |6.6  |{\"name\":\"betty\",\"age\":4}|\n",
      "|yes  |no     |-0.03|{\"name\":\"bobby\",\"age\":5}|\n",
      "+-----+-------+-----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_file_path = \"data/json/message.csv\"\n",
    "df3 = spark.read.csv(message_file_path, sep=\"|\", header=True, inferSchema=True)\n",
    "df3.show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Text1: string (nullable = true)\n",
      " |-- Text2: string (nullable = true)\n",
      " |-- size: double (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note the schema, the json field is taken as string. So we can't access the name, age field of the user column. We need to convert the json field into structured column. To do so, we can use the from_json function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------------------------+-----------+\n",
      "|Text1|Text2  |size |user                    |struct_user|\n",
      "+-----+-------+-----+------------------------+-----------+\n",
      "|hello|goodbye|5.0  |{\"name\":\"john\",\"age\":3} |{john, 3}  |\n",
      "|this |that   |6.6  |{\"name\":\"betty\",\"age\":4}|{betty, 4} |\n",
      "|yes  |no     |-0.03|{\"name\":\"bobby\",\"age\":5}|{bobby, 5} |\n",
      "+-----+-------+-----+------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"age\", IntegerType(), nullable=True),\n",
    "\n",
    "])\n",
    "df4 = df3.withColumn(\"struct_user\", from_json(col(\"user\"), schema=schema))\n",
    "df4.show(5, truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Text1: string (nullable = true)\n",
      " |-- Text2: string (nullable = true)\n",
      " |-- size: double (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- struct_user: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the column struct_user is a struc column. We can access its field directly. Below query is an example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|user_name|user_age|\n",
      "+---------+--------+\n",
      "|     john|       3|\n",
      "|    betty|       4|\n",
      "|    bobby|       5|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.select(col(\"struct_user.name\").alias(\"user_name\"), col(\"struct_user.age\").alias(\"user_age\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.4.1 Multiple line in json field\n",
    "\n",
    "In below example, the column user contains a list of json record. Unlike read.json has multiline option. The from_json() function does not have this option."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------------------------------------------------------+\n",
      "|Text1|Text2  |size |user                                                  |\n",
      "+-----+-------+-----+------------------------------------------------------+\n",
      "|hello|goodbye|5.0  |[{\"name\":\"stop\", \"age\":3}, {\"name\":\"go\", \"age\":6}]    |\n",
      "|this |that   |6.6  |[{\"name\":\"eggs\", \"age\":4}, {\"name\":\"bacon\", \"age\":8}] |\n",
      "|yes  |no     |-0.03|[{\"name\":\"apple\", \"age\":5}, {\"name\":\"pear\", \"age\":10}]|\n",
      "+-----+-------+-----+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_line_field_json = \"data/json/message1.csv\"\n",
    "df_multi_line_field = spark.read.csv(multi_line_field_json, header=True, sep=\"|\", inferSchema=True)\n",
    "df_multi_line_field.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+--------------------+------------+\n",
      "|Text1|  Text2| size|                user| struct_user|\n",
      "+-----+-------+-----+--------------------+------------+\n",
      "|hello|goodbye|  5.0|[{\"name\":\"stop\", ...|{null, null}|\n",
      "| this|   that|  6.6|[{\"name\":\"eggs\", ...|{null, null}|\n",
      "|  yes|     no|-0.03|[{\"name\":\"apple\",...|{null, null}|\n",
      "+-----+-------+-----+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multi_line_field.withColumn(\"struct_user\", from_json(col(\"user\"), schema=schema)).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice the from_json function can't get the value at all. So we need to write a little UDF which will do this correctly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+------------------------------------------------------+------------------------+\n",
      "|Text1|Text2  |size |user                                                  |struct_user             |\n",
      "+-----+-------+-----+------------------------------------------------------+------------------------+\n",
      "|hello|goodbye|5.0  |[{\"name\":\"stop\", \"age\":3}, {\"name\":\"go\", \"age\":6}]    |[{stop, 3}, {go, 6}]    |\n",
      "|this |that   |6.6  |[{\"name\":\"eggs\", \"age\":4}, {\"name\":\"bacon\", \"age\":8}] |[{eggs, 4}, {bacon, 8}] |\n",
      "|yes  |no     |-0.03|[{\"name\":\"apple\", \"age\":5}, {\"name\":\"pear\", \"age\":10}]|[{apple, 5}, {pear, 10}]|\n",
      "+-----+-------+-----+------------------------------------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "import json\n",
    "\n",
    "# Schema for the array of JSON objects.\n",
    "json_array_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField('name', StringType(), nullable=False),\n",
    "        StructField('age', IntegerType(), nullable=False)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# Create function to parse JSON using standard Python json library.\n",
    "def parse_json(array_str):\n",
    "    json_obj = json.loads(array_str)\n",
    "    for item in json_obj:\n",
    "        yield (item['name'], item['age'])\n",
    "\n",
    "\n",
    "# Create a UDF, whose return type is the JSON schema defined above.\n",
    "parse_json_udf = udf(lambda str: parse_json(str), json_array_schema)\n",
    "\n",
    "# Use the UDF to change the JSON string into a true array of structs.\n",
    "df_success=df_multi_line_field.withColumn(\"struct_user\", parse_json_udf((col(\"user\"))))\n",
    "df_success.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Text1: string (nullable = true)\n",
      " |-- Text2: string (nullable = true)\n",
      " |-- size: double (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- struct_user: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = false)\n",
      " |    |    |-- age: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_success.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.5 Infer json schema\n",
    "\n",
    "In the above example, we give a schema to the json column explicitly. We can also ask spark to infer schema for us by using schema_of_json() function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"john\",\"age\":3}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import schema_of_json, lit\n",
    "\n",
    "# get the first row\n",
    "user_row = df3.select(\"user\").first()\n",
    "# get the value of column user\n",
    "json_str = user_row.user\n",
    "print(json_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(json='STRUCT<`age`: BIGINT, `name`: STRING>')]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_schema = spark.range(1)\n",
    "df_schema.select(schema_of_json(lit(json_str)).alias(\"json\")).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.6 Convert struct column to json string\n",
    "\n",
    "We have seen how to convert json string to struct column, we can also convert struct column back to json string by using to_json() function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+--------------------+-----------+--------------------+\n",
      "|Text1|  Text2| size|                user|struct_user|            json_str|\n",
      "+-----+-------+-----+--------------------+-----------+--------------------+\n",
      "|hello|goodbye|  5.0|{\"name\":\"john\",\"a...|  {john, 3}|{\"name\":\"john\",\"a...|\n",
      "| this|   that|  6.6|{\"name\":\"betty\",\"...| {betty, 4}|{\"name\":\"betty\",\"...|\n",
      "|  yes|     no|-0.03|{\"name\":\"bobby\",\"...| {bobby, 5}|{\"name\":\"bobby\",\"...|\n",
      "+-----+-------+-----+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df4.withColumn(\"json_str\", to_json(\"struct_user\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.7 Convert json string column to flat column\n",
    "\n",
    "We have seen how to convert json to struct column, we can also convert json filed to flat column. For example, we can create two columns name and age directly by using json_tuple(). Note the generated column name needs to be renamed (e.g. toDF())."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-----+---+\n",
      "|Text1|Text2  |size |c0   |c1 |\n",
      "+-----+-------+-----+-----+---+\n",
      "|hello|goodbye|5.0  |john |3  |\n",
      "|this |that   |6.6  |betty|4  |\n",
      "|yes  |no     |-0.03|bobby|5  |\n",
      "+-----+-------+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "df_flat_field = df3.select(\"Text1\", \"Text2\", \"size\", json_tuple(col(\"user\"), \"name\", \"age\"))\n",
    "df_flat_field.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also use get_json_object()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+--------+\n",
      "|Text1|  Text2| size|user_age|\n",
      "+-----+-------+-----+--------+\n",
      "|hello|goodbye|  5.0|       3|\n",
      "| this|   that|  6.6|       4|\n",
      "|  yes|     no|-0.03|       5|\n",
      "+-----+-------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df_age = df3.select(\"Text1\", \"Text2\", \"size\", get_json_object(col(\"user\"), \"$.age\").alias(\"user_age\"))\n",
    "df_age.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4.8 Write json file\n",
    "\n",
    "Write json file is quite simple. Below is an example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "out_path=\"/tmp/output_test\"\n",
    "df_success.write.mode(\"overwrite\").json(out_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}