{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cache and Persist\n",
    "\n",
    "The **cache()** and **persist()** are methods used to store RDD, DataFrames and Datasets in memory to improve their re-usability across multiple Spark operations. When a dataset is `cached or persisted`, each `worker node` stores its partitioned data in memory. And `Sparkâ€™s persisted data on nodes are fault-tolerant` meaning if any partition of a Dataset is lost, it will automatically be recomputed using the original transformations that created it.\n",
    "\n",
    "## Advantage\n",
    "\n",
    "Computations Cost efficient - Spark computations are very expensive hence reusing the computations are used to save cost.\n",
    "\n",
    "Computations Time efficient - Reusing the repeated computations saves lots of time.\n",
    "\n",
    "## Dis-advantage\n",
    "\n",
    "Large Storage Cost - As we know the memory in the worker node is shared by the computation and storage. If we persist large dataset on a worker node, the memory left for computation will be reduced. If we store the dataset on Disk, the performence will be impacted. So `don't cache/persist unless a dataset will be reused`.\n",
    "\n",
    "## Cache vs Persist\n",
    "\n",
    "**Cache** is the simplified version of **Persist** method. You can't specify the storage level (e.g. MEMORY_ONLY, MEMORY_ONLY_SER, MEMORY_AND_DISK). It uses the default storage level in your spark cluster config. For RDD cache() the default storage level is `MEMORY_ONLY`, for DataFrame and Dataset cache(), default is `MEMORY_AND_DISK`\n",
    "\n",
    "```python\n",
    "\n",
    "df.cache()\n",
    "```\n",
    "\n",
    "**persist** allows you to specify the storage level. Below is an example.\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "df.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "```\n",
    "\n",
    "## Storage Level\n",
    "\n",
    "All different storage level Spark supports are available at `org.apache.spark.storage.StorageLevel` class. The storage level specifies how and where to persist or cache a Spark DataFrame and Dataset.\n",
    "\n",
    "- **MEMORY_ONLY**:  This is the default behavior of the RDD cache() method and stores the RDD or DataFrame as deserialized objects to JVM memory. When there is no enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required. This takes more memory. but unlike RDD, this would be slower than MEMORY_AND_DISK level as it recomputes the unsaved partitions and recomputing the in-memory columnar representation of the underlying table is expensive\n",
    "\n",
    "\n",
    "- **MEMORY_ONLY_SER**:  This is the same as MEMORY_ONLY but the difference being it stores RDD as serialized objects to JVM memory. It takes lesser memory (space-efficient) then MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "- **MEMORY_ONLY_2**:  Same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_ONLY_SER_2**:  Same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_AND_DISK**:  This is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will be stored in JVM memory as a deserialized object. When required storage is greater than available memory, it stores some of the excess partitions into the disk and reads the data from the disk when required. It is slower as there is I/O involved.\n",
    "\n",
    "\n",
    "- **MEMORY_AND_DISK_SER**:  This is the same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available.\n",
    "\n",
    "- **MEMORY_AND_DISK_2**:  Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_AND_DISK_SER_2**:  Same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **DISK_ONLY**:  In this storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O is involved.\n",
    "\n",
    "- **DISK_ONLY_2**:  Same as DISK_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "```text\n",
    "Storage Level    Space used  CPU time  In memory  On-disk  Serialized   Recompute some partitions\n",
    "----------------------------------------------------------------------------------------------------\n",
    "MEMORY_ONLY          High        Low       Y          N        N         Y    \n",
    "MEMORY_ONLY_SER      Low         High      Y          N        Y         Y\n",
    "MEMORY_AND_DISK      High        Medium    Some       Some     Some      N\n",
    "MEMORY_AND_DISK_SER  Low         High      Some       Some     Y         N\n",
    "DISK_ONLY            Low         High      N          Y        Y         N\n",
    "\n",
    "```\n",
    "## Other important point\n",
    "\n",
    "- Spark automatically monitors every persist() and cache() calls you make and `it checks usage on each node and drops persisted data if not used or using least-recently-used (LRU) algorithm`. The manually clean action **unpersist()** method can be used to.\n",
    "- On Spark UI, the Storage tab shows where partitions exist in memory or disk across the cluster.\n",
    "- Dataset cache() is an alias for persist(StorageLevel.MEMORY_AND_DISK)\n",
    "- Caching of Spark DataFrame or Dataset is a **lazy operation**, meaning a DataFrame will not be cached until you trigger an action. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f02c08a097c15"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/07 14:38:54 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/09/07 14:38:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/07 14:38:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"CacheAndPersist\")\\\n",
    "        .config(\"spark.driver.memory\", \"6g\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"CacheAndPersist\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:38:58.980408275Z",
     "start_time": "2023-09-07T12:38:51.350215366Z"
    }
   },
   "id": "5ff697b42dacf879"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below are a list of usefully functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffeefd8020a82e18"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def showPersistedRdd():\n",
    "    \"\"\"\n",
    "    This function shows the persistent rdd information\n",
    "    :return: \n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    rdds = spark.sparkContext._jsc.getPersistentRDDs()\n",
    "    print(f\"Persisted rdd numbers: {len(rdds)}\")\n",
    "    for id, rdd in rdds.items():\n",
    "        print(f\"id: {id}\\ndescription: {rdd}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:39:01.894438791Z",
     "start_time": "2023-09-07T12:39:01.867967791Z"
    }
   },
   "id": "eaf106f3f9b805fc"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def showViews(globalView:bool = False):\n",
    "    \"\"\"\n",
    "    This function shows all the available view in the spark context. If globalView is true, it will show global view, otherwise it only shows the temp view\n",
    "    :return: \n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    base = \"global_temp\" if globalView else None\n",
    "    for view in spark.catalog.listTables(base):\n",
    "        print(view.name)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:47:05.615799609Z",
     "start_time": "2023-09-07T12:47:05.590446739Z"
    }
   },
   "id": "41e7d6d00ccaabc0"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def dropView(viewName:str, globalView:bool=False):\n",
    "    \"\"\"\n",
    "    This function can drop a view with the given view name. The no existing exception is handled by spark. If exist and delete success, return True, otherwise return False\n",
    "    :param viewName: \n",
    "    :type viewName: \n",
    "    :param globalView: \n",
    "    :type globalView: \n",
    "    :return: \n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    if globalView:\n",
    "        result = spark.catalog.dropGlobalTempView(viewName)\n",
    "        print(f\"Global view '{viewName}' has been deleted with return result: {result}.\")\n",
    "    else:\n",
    "        # Drop the temporary view\n",
    "        result = spark.catalog.dropTempView(viewName)\n",
    "        print(f\"Temporary view '{viewName}' has been deleted with return result: {result}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:50:32.060817151Z",
     "start_time": "2023-09-07T12:50:31.997645012Z"
    }
   },
   "id": "c608cd63a2984702"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filePath = \"/home/pengfei/data_set/kaggle/data_format/netflix.parquet\"\n",
    "\n",
    "df = spark.read.parquet(filePath)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:43:02.703893419Z",
     "start_time": "2023-09-07T12:42:58.006871687Z"
    }
   },
   "id": "30bfa43a6f50445d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|      date|\n",
      "+-------+------+----------+\n",
      "|1488844|     3|2005-09-06|\n",
      "| 822109|     5|2005-05-13|\n",
      "| 885013|     4|2005-10-19|\n",
      "|  30878|     4|2005-12-26|\n",
      "| 823519|     3|2004-05-03|\n",
      "| 893988|     3|2005-11-17|\n",
      "| 124105|     4|2004-08-05|\n",
      "|1248029|     3|2004-04-22|\n",
      "|1842128|     4|2004-05-09|\n",
      "|2238063|     3|2005-05-11|\n",
      "|1503895|     4|2005-05-19|\n",
      "|2207774|     5|2005-06-06|\n",
      "|2590061|     3|2004-08-12|\n",
      "|   2442|     3|2004-04-14|\n",
      "| 543865|     4|2004-05-28|\n",
      "|1209119|     4|2004-03-23|\n",
      "| 804919|     4|2004-06-10|\n",
      "|1086807|     3|2004-12-28|\n",
      "|1711859|     4|2005-05-08|\n",
      "| 372233|     5|2005-11-23|\n",
      "+-------+------+----------+\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:43:06.376290220Z",
     "start_time": "2023-09-07T12:43:05.824514432Z"
    }
   },
   "id": "6d7541ce35537246"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|      date|\n",
      "+-------+------+----------+\n",
      "|1488844|     3|2005-09-06|\n",
      "| 822109|     5|2005-05-13|\n",
      "| 885013|     4|2005-10-19|\n",
      "|  30878|     4|2005-12-26|\n",
      "| 823519|     3|2004-05-03|\n",
      "| 893988|     3|2005-11-17|\n",
      "| 124105|     4|2004-08-05|\n",
      "|1248029|     3|2004-04-22|\n",
      "|1842128|     4|2004-05-09|\n",
      "|2238063|     3|2005-05-11|\n",
      "|1503895|     4|2005-05-19|\n",
      "|2207774|     5|2005-06-06|\n",
      "|2590061|     3|2004-08-12|\n",
      "|   2442|     3|2004-04-14|\n",
      "| 543865|     4|2004-05-28|\n",
      "|1209119|     4|2004-03-23|\n",
      "| 804919|     4|2004-06-10|\n",
      "|1086807|     3|2004-12-28|\n",
      "|1711859|     4|2005-05-08|\n",
      "| 372233|     5|2005-11-23|\n",
      "+-------+------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# now we try to cache the dataframe\n",
    "# note that the cache() method is lazy transformation, if no action, it will not be executed.\n",
    "cachedDf = df.cache()\n",
    "cachedDf.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:43:56.554923775Z",
     "start_time": "2023-09-07T12:43:12.969319542Z"
    }
   },
   "id": "713557bb4a1a541b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can check the cached data in spark UI (http://localhost:4040/storage). As we mentioned before, the default storage level for dataframe is `memory and disk`.\n",
    "\n",
    "Base on the config of your cluster, the above operation failed, because we only have 4 GB memory for the worker and half of it is reserved for the calculation. So if the cached data is too big, it's normal we have a out of memory error. When we encounter this kind of situation, we have two solution:\n",
    "- we can increase the worker memory\n",
    "- change the storage level.\n",
    "\n",
    "The below code use the `persist` method to cache the data. It's equivalent of `df.cache()`.\n",
    "\n",
    "```python\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "persistDf = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "persistDf.show()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f2ae419c74ecab"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted rdd numbers: 1\n",
      "id: 18\n",
      "description: *(1) ColumnarToRow\n",
      "+- FileScan parquet [user_id#0,rating#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/pengfei/data_set/kaggle/data_format/netflix.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:string,rating:string,date:string>\n",
      " MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "# We can also get the persisted rdd information by using the below function \n",
    "showPersistedRdd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:44:01.755237995Z",
     "start_time": "2023-09-07T12:44:01.695504011Z"
    }
   },
   "id": "d7bdf0877f0bc0df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance test Dataframe vs Cached dataframe\n",
    "\n",
    "Now let's check if the cached data frame has better performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9830f6410cf2b005"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     3|6904181|\n",
      "|  null|   4498|\n",
      "|     5|5506583|\n",
      "|     1|1118186|\n",
      "|     4|8085741|\n",
      "|     2|2439073|\n",
      "+------+-------+\n",
      "\n",
      "CPU times: user 6.79 ms, sys: 771 Âµs, total: 7.57 ms\n",
      "Wall time: 5.34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "dfResu = df.groupBy(\"rating\").agg(count(\"*\").alias(\"count\"))\n",
    "dfResu.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:44:14.851004080Z",
     "start_time": "2023-09-07T12:44:09.518361680Z"
    }
   },
   "id": "c6cbbd28ff1bae92"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     3|6904181|\n",
      "|  null|   4498|\n",
      "|     5|5506583|\n",
      "|     1|1118186|\n",
      "|     4|8085741|\n",
      "|     2|2439073|\n",
      "+------+-------+\n",
      "\n",
      "CPU times: user 16.4 ms, sys: 0 ns, total: 16.4 ms\n",
      "Wall time: 3.51 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cachedDfResu = cachedDf.groupBy(\"rating\").agg(count(\"*\").alias(\"count\"))\n",
    "cachedDfResu.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:44:22.327857600Z",
     "start_time": "2023-09-07T12:44:18.725951409Z"
    }
   },
   "id": "e218603f0aeac237"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unpersist to clear the memory\n",
    "\n",
    "We can also unpersist the persistence DataFrame or Dataset to remove it from the memory or storage.\n",
    "The function signature is `unpersist(blocking : scala.Boolean) : Dataset.this.type` \n",
    "\n",
    "The default value of the `blocking` parameter is `False`. That means, it doesn't block the spark operation until all the blocks are deleted, and runs asynchronously. If you set it to `True`, it means all spark operation of the dataframe will be blocked until all the persisted block are deleted"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84fe98e4f3e563b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "normalDf = cachedDf.unpersist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:44:37.097548047Z",
     "start_time": "2023-09-07T12:44:37.040349675Z"
    }
   },
   "id": "62ed2b3e7b192294"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     3|6904181|\n",
      "|  null|   4498|\n",
      "|     5|5506583|\n",
      "|     1|1118186|\n",
      "|     4|8085741|\n",
      "|     2|2439073|\n",
      "+------+-------+\n",
      "\n",
      "CPU times: user 10.4 ms, sys: 1.71 ms, total: 12.1 ms\n",
      "Wall time: 2.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "normalDfResu = normalDf.groupBy(\"rating\").agg(count(\"*\").alias(\"count\"))\n",
    "normalDfResu.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:44:46.068491990Z",
     "start_time": "2023-09-07T12:44:43.781771505Z"
    }
   },
   "id": "49f99dd751cc9ad6"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted rdd numbers: 0\n"
     ]
    }
   ],
   "source": [
    "# after unpersist, we can check the number of persisted rdd\n",
    "showPersistedRdd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:44:57.858157276Z",
     "start_time": "2023-09-07T12:44:57.776354541Z"
    }
   },
   "id": "e45caa28268684af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can notice that after unpersist, we no-longer have persisted rdd "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be43b014521dbaca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What happens when we create a view?\n",
    "\n",
    "We also wants to know if we persist data when we create temp view and global view in spark. \n",
    "\n",
    "### A temp view example\n",
    "\n",
    "In below code, we create a temp view."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37f1729d812d1d5d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tempTableName = \"netflix\"\n",
    "df.createOrReplaceTempView(tempTableName)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:46:27.754087362Z",
     "start_time": "2023-09-07T12:46:27.666532378Z"
    }
   },
   "id": "e635beac5dac71"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netflix\n"
     ]
    }
   ],
   "source": [
    "showViews()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:47:14.038284073Z",
     "start_time": "2023-09-07T12:47:13.479208709Z"
    }
   },
   "id": "237c291807bf571"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 223:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  count|rating|\n",
      "+-------+------+\n",
      "|6904181|     3|\n",
      "|   4498|  null|\n",
      "|5506583|     5|\n",
      "|1118186|     1|\n",
      "|8085741|     4|\n",
      "|2439073|     2|\n",
      "+-------+------+\n",
      "\n",
      "CPU times: user 1.23 ms, sys: 3.48 ms, total: 4.71 ms\n",
      "Wall time: 1.97 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = spark.sql(f\"Select count(*) as count, rating from {tempTableName} group by rating\")\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:49:31.818478803Z",
     "start_time": "2023-09-07T12:49:29.833993433Z"
    }
   },
   "id": "fcc7ab4da9a8a161"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted rdd numbers: 0\n"
     ]
    }
   ],
   "source": [
    "showPersistedRdd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:47:41.031775087Z",
     "start_time": "2023-09-07T12:47:41.006284914Z"
    }
   },
   "id": "a330d6942156b00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the above operations, we can conclude that the temp view will not persist any Rdd.\n",
    "\n",
    "Now let's clean the view"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95c40833ad57e6e5"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary view 'netflix' has been deleted with return result: True.\n"
     ]
    }
   ],
   "source": [
    "# clean the table\n",
    "dropView(tempTableName)\n",
    "# show available view\n",
    "showViews()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:50:43.804242573Z",
     "start_time": "2023-09-07T12:50:43.532760985Z"
    }
   },
   "id": "cfbadccd48e489f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A global view example\n",
    "\n",
    "Now let's check what happens when we create a global view. \n",
    "\n",
    "> Global views are designed for long-term sharing of DataFrames across different Spark applications or sessions "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48575af1a747ae3"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "globalViewName = \"gnetflix\"\n",
    "df.createOrReplaceGlobalTempView(globalViewName)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:50:55.810341543Z",
     "start_time": "2023-09-07T12:50:55.764048121Z"
    }
   },
   "id": "29797b936a8c45cb"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "showViews()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:58:51.868650463Z",
     "start_time": "2023-09-07T12:58:51.764258309Z"
    }
   },
   "id": "8ca8f9c946246062"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnetflix\n"
     ]
    }
   ],
   "source": [
    "showViews(globalView=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:59:01.210648576Z",
     "start_time": "2023-09-07T12:59:01.086159076Z"
    }
   },
   "id": "ffdd25a09e83d756"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 374:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  count|rating|\n",
      "+-------+------+\n",
      "|6904181|     3|\n",
      "|   4498|  null|\n",
      "|5506583|     5|\n",
      "|1118186|     1|\n",
      "|8085741|     4|\n",
      "|2439073|     2|\n",
      "+-------+------+\n",
      "\n",
      "CPU times: user 3.64 ms, sys: 542 Âµs, total: 4.18 ms\n",
      "Wall time: 2.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = spark.sql(f\"Select count(*) as count, rating from global_temp.{globalViewName} group by rating\")\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:58:33.919256927Z",
     "start_time": "2023-09-07T12:58:31.897141130Z"
    }
   },
   "id": "d5c5c9bf0612c35a"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted rdd numbers: 0\n"
     ]
    }
   ],
   "source": [
    "# after the creation of the global view, let's check if there are any persisted rdd\n",
    "showPersistedRdd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T12:59:10.695699152Z",
     "start_time": "2023-09-07T12:59:10.677160017Z"
    }
   },
   "id": "a7dbaf22ec66c96d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**We can conclude that after the creation of the global view, no RDDs are persisted. After running an operation on the view, no RDDs are persisted**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccc90dc4562cc058"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global view 'gnetflix' has been deleted with return result: True.\n"
     ]
    }
   ],
   "source": [
    "dropView(globalViewName,globalView=True)\n",
    "showViews(globalView=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:16:08.742907593Z",
     "start_time": "2023-09-07T10:16:08.735966941Z"
    }
   },
   "id": "5ab6df13923b63e8"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3da2b0501b7c85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
