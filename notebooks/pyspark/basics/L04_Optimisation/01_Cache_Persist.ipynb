{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cache and Persist\n",
    "\n",
    "The **cache()** and **persist()** are methods used to store RDD, DataFrames and Datasets in memory to improve their re-usability across multiple Spark operations. When a dataset is `cached or persisted`, each `worker node` stores its partitioned data in memory. And `Sparkâ€™s persisted data on nodes are fault-tolerant` meaning if any partition of a Dataset is lost, it will automatically be recomputed using the original transformations that created it.\n",
    "\n",
    "## Advantage\n",
    "\n",
    "Computations Cost efficient - Spark computations are very expensive hence reusing the computations are used to save cost.\n",
    "\n",
    "Computations Time efficient - Reusing the repeated computations saves lots of time.\n",
    "\n",
    "## Dis-advantage\n",
    "\n",
    "Large Storage Cost - As we know the memory in the worker node is shared by the computation and storage. If we persist large dataset on a worker node, the memory left for computation will be reduced. If we store the dataset on Disk, the performence will be impacted. So `don't cache/persist unless a dataset will be reused`.\n",
    "\n",
    "## Cache vs Persist\n",
    "\n",
    "**Cache** is the simplified version of **Persist** method. You can't specify the storage level (e.g. MEMORY_ONLY, MEMORY_ONLY_SER, MEMORY_AND_DISK). It uses the default storage level in your spark cluster config. For RDD cache() the default storage level is `MEMORY_ONLY`, for DataFrame and Dataset cache(), default is `MEMORY_AND_DISK`\n",
    "\n",
    "```python\n",
    "\n",
    "df.cache()\n",
    "```\n",
    "\n",
    "**persist** allows you to specify the storage level. Below is an example.\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "df.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "```\n",
    "\n",
    "## Storage Level\n",
    "\n",
    "All different storage level Spark supports are available at `org.apache.spark.storage.StorageLevel` class. The storage level specifies how and where to persist or cache a Spark DataFrame and Dataset.\n",
    "\n",
    "- **MEMORY_ONLY**:  This is the default behavior of the RDD cache() method and stores the RDD or DataFrame as deserialized objects to JVM memory. When there is no enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required. This takes more memory. but unlike RDD, this would be slower than MEMORY_AND_DISK level as it recomputes the unsaved partitions and recomputing the in-memory columnar representation of the underlying table is expensive\n",
    "\n",
    "\n",
    "- **MEMORY_ONLY_SER**:  This is the same as MEMORY_ONLY but the difference being it stores RDD as serialized objects to JVM memory. It takes lesser memory (space-efficient) then MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "- **MEMORY_ONLY_2**:  Same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_ONLY_SER_2**:  Same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_AND_DISK**:  This is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will be stored in JVM memory as a deserialized object. When required storage is greater than available memory, it stores some of the excess partitions into the disk and reads the data from the disk when required. It is slower as there is I/O involved.\n",
    "\n",
    "\n",
    "- **MEMORY_AND_DISK_SER**:  This is the same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available.\n",
    "\n",
    "- **MEMORY_AND_DISK_2**:  Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_AND_DISK_SER_2**:  Same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **DISK_ONLY**:  In this storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O is involved.\n",
    "\n",
    "- **DISK_ONLY_2**:  Same as DISK_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "\n",
    "## Other important point\n",
    "\n",
    "- Spark automatically monitors every persist() and cache() calls you make and `it checks usage on each node and drops persisted data if not used or using least-recently-used (LRU) algorithm`. The manually clean action **unpersist()** method can be used to.\n",
    "- On Spark UI, the Storage tab shows where partitions exist in memory or disk across the cluster.\n",
    "- Dataset cache() is an alias for persist(StorageLevel.MEMORY_AND_DISK)\n",
    "- Caching of Spark DataFrame or Dataset is a **lazy operation**, meaning a DataFrame will not be cached until you trigger an action. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f02c08a097c15"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/07 10:56:55 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/09/07 10:56:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/07 10:56:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"CacheAndPersist\")\\\n",
    "        .config(\"spark.driver.memory\", \"6g\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"CacheAndPersist\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:57:00.694378822Z",
     "start_time": "2023-09-07T08:56:50.414281610Z"
    }
   },
   "id": "5ff697b42dacf879"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below are a list of usefully functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffeefd8020a82e18"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "\n",
    "def showPersistedRdd():\n",
    "    \"\"\"\n",
    "    This function shows the persistent rdd information\n",
    "    :return: \n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    rdds = spark.sparkContext._jsc.getPersistentRDDs()\n",
    "    print(f\"Persisted rdd numbers: {len(rdds)}\")\n",
    "    for id, rdd in rdds.items():\n",
    "        print(f\"id: {id}\\ndescription: {rdd}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:39:13.430443574Z",
     "start_time": "2023-09-07T09:39:13.324840107Z"
    }
   },
   "id": "eaf106f3f9b805fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def showGlobalViews():\n",
    "    \"\"\"\n",
    "    This function shows all the available global view\n",
    "    :return: \n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    for view in spark.catalog.listTables(\"global_temp\"):\n",
    "        print(view.name)\n",
    "        \n",
    "# get the temp view list\n",
    "views = spark.catalog.listTables()\n",
    "print(len(views))\n",
    "\n",
    "for view in views:\n",
    "    print(view.name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41e7d6d00ccaabc0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dropView(viewName:str, globalView:bool=False):\n",
    "    if globalView:\n",
    "        result = spark.catalog.dropGlobalTempView(viewName)\n",
    "        print(f\"Global view '{viewName}' has been deleted with return result: {result}.\")\n",
    "    else:\n",
    "        # Check if the temporary view exists before deleting\n",
    "        if spark.catalog.isCached(viewName):\n",
    "            # Drop the temporary view\n",
    "            result = spark.catalog.dropTempView(viewName)\n",
    "            print(f\"Temporary view '{viewName}' has been deleted with return result: {result}.\")\n",
    "        else:\n",
    "            print(f\"Temporary view '{viewName}' does not exist.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c608cd63a2984702"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filePath = \"/home/pengfei/data_set/kaggle/data_format/netflix.parquet\"\n",
    "\n",
    "df = spark.read.parquet(filePath)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:57:27.138649238Z",
     "start_time": "2023-09-07T08:57:21.269469801Z"
    }
   },
   "id": "30bfa43a6f50445d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|      date|\n",
      "+-------+------+----------+\n",
      "|1488844|     3|2005-09-06|\n",
      "| 822109|     5|2005-05-13|\n",
      "| 885013|     4|2005-10-19|\n",
      "|  30878|     4|2005-12-26|\n",
      "| 823519|     3|2004-05-03|\n",
      "| 893988|     3|2005-11-17|\n",
      "| 124105|     4|2004-08-05|\n",
      "|1248029|     3|2004-04-22|\n",
      "|1842128|     4|2004-05-09|\n",
      "|2238063|     3|2005-05-11|\n",
      "|1503895|     4|2005-05-19|\n",
      "|2207774|     5|2005-06-06|\n",
      "|2590061|     3|2004-08-12|\n",
      "|   2442|     3|2004-04-14|\n",
      "| 543865|     4|2004-05-28|\n",
      "|1209119|     4|2004-03-23|\n",
      "| 804919|     4|2004-06-10|\n",
      "|1086807|     3|2004-12-28|\n",
      "|1711859|     4|2005-05-08|\n",
      "| 372233|     5|2005-11-23|\n",
      "+-------+------+----------+\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T08:58:16.691724167Z",
     "start_time": "2023-09-07T08:58:15.996153946Z"
    }
   },
   "id": "6d7541ce35537246"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 395:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|user_id|rating|      date|\n",
      "+-------+------+----------+\n",
      "|1488844|     3|2005-09-06|\n",
      "| 822109|     5|2005-05-13|\n",
      "| 885013|     4|2005-10-19|\n",
      "|  30878|     4|2005-12-26|\n",
      "| 823519|     3|2004-05-03|\n",
      "| 893988|     3|2005-11-17|\n",
      "| 124105|     4|2004-08-05|\n",
      "|1248029|     3|2004-04-22|\n",
      "|1842128|     4|2004-05-09|\n",
      "|2238063|     3|2005-05-11|\n",
      "|1503895|     4|2005-05-19|\n",
      "|2207774|     5|2005-06-06|\n",
      "|2590061|     3|2004-08-12|\n",
      "|   2442|     3|2004-04-14|\n",
      "| 543865|     4|2004-05-28|\n",
      "|1209119|     4|2004-03-23|\n",
      "| 804919|     4|2004-06-10|\n",
      "|1086807|     3|2004-12-28|\n",
      "|1711859|     4|2005-05-08|\n",
      "| 372233|     5|2005-11-23|\n",
      "+-------+------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# now we try to cache the dataframe\n",
    "# note that the cache() method is lazy transformation, if no action, it will not be executed.\n",
    "cachedDf = df.cache()\n",
    "cachedDf.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:37:03.140387804Z",
     "start_time": "2023-09-07T09:36:10.407661652Z"
    }
   },
   "id": "713557bb4a1a541b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can check the cached data in spark UI (http://localhost:4040/storage). As we mentioned before, the default storage level for dataframe is `memory and disk`.\n",
    "\n",
    "Base on the config of your cluster, the above operation failed, because we only have 4 GB memory for the worker and half of it is reserved for the calculation. So if the cached data is too big, it's normal we have a out of memory error. When we encounter this kind of situation, we have two solution:\n",
    "- we can increase the worker memory\n",
    "- change the storage level.\n",
    "\n",
    "The below code use the `persist` method to cache the data. It's equivalent of `df.cache()`.\n",
    "\n",
    "```python\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "persistDf = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "persistDf.show()\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f2ae419c74ecab"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted rdd numbers: 1\n",
      "id: 896\n",
      "description: *(1) ColumnarToRow\n",
      "+- FileScan parquet [user_id#0,rating#1,date#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/pengfei/data_set/kaggle/data_format/netflix.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:string,rating:string,date:string>\n",
      " MapPartitionsRDD[896] at showString at <unknown>:0\n"
     ]
    }
   ],
   "source": [
    "# We can also get the persisted rdd information by using the below function \n",
    "showPersistedRdd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:39:27.192694056Z",
     "start_time": "2023-09-07T09:39:27.161058527Z"
    }
   },
   "id": "d7bdf0877f0bc0df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance test Dataframe vs Cached dataframe\n",
    "\n",
    "Now let's check if the cached data frame has better performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9830f6410cf2b005"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     3|6904181|\n",
      "|  null|   4498|\n",
      "|     5|5506583|\n",
      "|     1|1118186|\n",
      "|     4|8085741|\n",
      "|     2|2439073|\n",
      "+------+-------+\n",
      "\n",
      "CPU times: user 6.05 ms, sys: 0 ns, total: 6.05 ms\n",
      "Wall time: 4.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "dfResu = df.groupBy(\"rating\").agg(count(\"*\").alias(\"count\"))\n",
    "dfResu.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:00:08.699315954Z",
     "start_time": "2023-09-07T09:00:04.416516248Z"
    }
   },
   "id": "c6cbbd28ff1bae92"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     3|6904181|\n",
      "|  null|   4498|\n",
      "|     5|5506583|\n",
      "|     1|1118186|\n",
      "|     4|8085741|\n",
      "|     2|2439073|\n",
      "+------+-------+\n",
      "\n",
      "CPU times: user 3.85 ms, sys: 8.92 ms, total: 12.8 ms\n",
      "Wall time: 3.68 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cachedDfResu = cachedDf.groupBy(\"rating\").agg(count(\"*\").alias(\"count\"))\n",
    "cachedDfResu.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:00:20.499162960Z",
     "start_time": "2023-09-07T09:00:16.774505176Z"
    }
   },
   "id": "e218603f0aeac237"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unpersist to clear the memory\n",
    "\n",
    "We can also unpersist the persistence DataFrame or Dataset to remove it from the memory or storage.\n",
    "The function signature is `unpersist(blocking : scala.Boolean) : Dataset.this.type` \n",
    "\n",
    "The default value of the `blocking` parameter is `False`. That means, it doesn't block the spark operation until all the blocks are deleted, and runs asynchronously. If you set it to `True`, it means all spark operation of the dataframe will be blocked until all the persisted block are deleted"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84fe98e4f3e563b"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "normalDf = cachedDf.unpersist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:39:45.034533669Z",
     "start_time": "2023-09-07T09:39:44.999765516Z"
    }
   },
   "id": "62ed2b3e7b192294"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|     3|6904181|\n",
      "|  null|   4498|\n",
      "|     5|5506583|\n",
      "|     1|1118186|\n",
      "|     4|8085741|\n",
      "|     2|2439073|\n",
      "+------+-------+\n",
      "\n",
      "CPU times: user 5.3 ms, sys: 0 ns, total: 5.3 ms\n",
      "Wall time: 2.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "normalDfResu = normalDf.groupBy(\"rating\").agg(count(\"*\").alias(\"count\"))\n",
    "normalDfResu.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:11:58.275747683Z",
     "start_time": "2023-09-07T09:11:55.580881788Z"
    }
   },
   "id": "49f99dd751cc9ad6"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted rdd numbers: 0\n"
     ]
    }
   ],
   "source": [
    "# after unpersist, we can check the number of persisted rdd\n",
    "showPersistedRdd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:40:52.875933266Z",
     "start_time": "2023-09-07T09:40:52.822653277Z"
    }
   },
   "id": "e45caa28268684af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What happens when we create a view?\n",
    "\n",
    "We also wants to know if we persist data when we create temp view and global view in spark. \n",
    "\n",
    "### A temp view example\n",
    "\n",
    "In below code, we create a temp view."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37f1729d812d1d5d"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "tempTableName = \"netflix\"\n",
    "df.createOrReplaceTempView(tempTableName)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:54:18.592452114Z",
     "start_time": "2023-09-07T09:54:18.546445281Z"
    }
   },
   "id": "e635beac5dac71"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:23:03.568077654Z",
     "start_time": "2023-09-07T09:23:03.351224089Z"
    }
   },
   "id": "a03cdef9b6965944"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "netflix\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:25:10.770840780Z",
     "start_time": "2023-09-07T09:25:10.727475996Z"
    }
   },
   "id": "237c291807bf571"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 615:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  count|rating|\n",
      "+-------+------+\n",
      "|6904181|     3|\n",
      "|   4498|  null|\n",
      "|5506583|     5|\n",
      "|1118186|     1|\n",
      "|8085741|     4|\n",
      "|2439073|     2|\n",
      "+-------+------+\n",
      "\n",
      "CPU times: user 2.27 ms, sys: 643 Âµs, total: 2.91 ms\n",
      "Wall time: 2.25 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result = spark.sql(f\"Select count(*) as count, rating from {tempTableName} group by rating\")\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:54:41.916351317Z",
     "start_time": "2023-09-07T09:54:39.656507407Z"
    }
   },
   "id": "fcc7ab4da9a8a161"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted rdd numbers: 0\n"
     ]
    }
   ],
   "source": [
    "showPersistedRdd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:39:57.270105580Z",
     "start_time": "2023-09-07T09:39:57.213403068Z"
    }
   },
   "id": "a330d6942156b00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the above operations, we can conclude that the temp view will not persist any Rdd.\n",
    "\n",
    "Now let's clean the "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95c40833ad57e6e5"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# clean the table\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:15:42.546581064Z",
     "start_time": "2023-09-07T10:15:42.529837140Z"
    }
   },
   "id": "cfbadccd48e489f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A global view example\n",
    "\n",
    "Now let's check what happens when we create a global view. \n",
    "\n",
    "> Global views are designed for long-term sharing of DataFrames across different Spark applications or sessions "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48575af1a747ae3"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "globalViewName = \"gnetflix\"\n",
    "df.createOrReplaceGlobalTempView(globalViewName)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:16:00.100647379Z",
     "start_time": "2023-09-07T10:16:00.085303366Z"
    }
   },
   "id": "29797b936a8c45cb"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:10:33.670959160Z",
     "start_time": "2023-09-07T10:10:33.569178608Z"
    }
   },
   "id": "8ca8f9c946246062"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnetflix\n"
     ]
    }
   ],
   "source": [
    "showGlobalViews()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:10:48.896932704Z",
     "start_time": "2023-09-07T10:10:48.694166807Z"
    }
   },
   "id": "ffdd25a09e83d756"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Catalog' object has no attribute 'dropGlobalView'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcatalog\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropGlobalView\u001B[49m(tempTableName)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Catalog' object has no attribute 'dropGlobalView'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:04:09.657501371Z",
     "start_time": "2023-09-07T10:04:09.414548083Z"
    }
   },
   "id": "d5c5c9bf0612c35a"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global view 'gnetflix' has been deleted with return result: True.\n"
     ]
    }
   ],
   "source": [
    "dropView(globalViewName,globalView=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T10:16:08.742907593Z",
     "start_time": "2023-09-07T10:16:08.735966941Z"
    }
   },
   "id": "5ab6df13923b63e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dfa8e65b7a885b50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
