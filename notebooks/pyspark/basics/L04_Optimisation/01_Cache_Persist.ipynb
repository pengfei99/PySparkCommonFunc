{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cache and Persist\n",
    "\n",
    "The **cache()** and **persist()** are methods used to store RDD, DataFrames and Datasets in memory to improve their re-usability across multiple Spark operations. When a dataset is `cached or persisted`, each `worker node` stores its partitioned data in memory. And `Sparkâ€™s persisted data on nodes are fault-tolerant` meaning if any partition of a Dataset is lost, it will automatically be recomputed using the original transformations that created it.\n",
    "\n",
    "## Advantage\n",
    "\n",
    "Computations Cost efficient - Spark computations are very expensive hence reusing the computations are used to save cost.\n",
    "\n",
    "Computations Time efficient - Reusing the repeated computations saves lots of time.\n",
    "\n",
    "## Dis-advantage\n",
    "\n",
    "Large Storage Cost - As we know the memory in the worker node is shared by the computation and storage. If we persist large dataset on a worker node, the memory left for computation will be reduced. If we store the dataset on Disk, the performence will be impacted. So `don't cache/persist unless a dataset will be reused`.\n",
    "\n",
    "## Cache vs Persist\n",
    "\n",
    "**Cache** is the simplified version of **Persist** method. You can't specify the storage level (e.g. MEMORY_ONLY, MEMORY_ONLY_SER, MEMORY_AND_DISK). It uses the default storage level in your spark cluster config. For RDD cache() the default storage level is `MEMORY_ONLY`, for DataFrame and Dataset cache(), default is `MEMORY_AND_DISK`\n",
    "\n",
    "```python\n",
    "\n",
    "df.cache()\n",
    "```\n",
    "\n",
    "**persist** allows you to specify the storage level. Below is an example.\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "df.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "```\n",
    "\n",
    "## Storage Level\n",
    "\n",
    "All different storage level Spark supports are available at `org.apache.spark.storage.StorageLevel` class. The storage level specifies how and where to persist or cache a Spark DataFrame and Dataset.\n",
    "\n",
    "- **MEMORY_ONLY**:  This is the default behavior of the RDD cache() method and stores the RDD or DataFrame as deserialized objects to JVM memory. When there is no enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required. This takes more memory. but unlike RDD, this would be slower than MEMORY_AND_DISK level as it recomputes the unsaved partitions and recomputing the in-memory columnar representation of the underlying table is expensive\n",
    "\n",
    "\n",
    "- **MEMORY_ONLY_SER**:  This is the same as MEMORY_ONLY but the difference being it stores RDD as serialized objects to JVM memory. It takes lesser memory (space-efficient) then MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
    "\n",
    "- **MEMORY_ONLY_2**:  Same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_ONLY_SER_2**:  Same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_AND_DISK**:  This is the default behavior of the DataFrame or Dataset. In this Storage Level, The DataFrame will be stored in JVM memory as a deserialized object. When required storage is greater than available memory, it stores some of the excess partitions into the disk and reads the data from the disk when required. It is slower as there is I/O involved.\n",
    "\n",
    "\n",
    "- **MEMORY_AND_DISK_SER**:  This is the same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available.\n",
    "\n",
    "- **MEMORY_AND_DISK_2**:  Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **MEMORY_AND_DISK_SER_2**:  Same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "- **DISK_ONLY**:  In this storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O is involved.\n",
    "\n",
    "- **DISK_ONLY_2**:  Same as DISK_ONLY storage level but replicate each partition to two cluster nodes.\n",
    "\n",
    "\n",
    "## Other important point\n",
    "\n",
    "- Spark automatically monitors every persist() and cache() calls you make and `it checks usage on each node and drops persisted data if not used or using least-recently-used (LRU) algorithm`. The manually clean action **unpersist()** method can be used to.\n",
    "- On Spark UI, the Storage tab shows where partitions exist in memory or disk across the cluster.\n",
    "- Dataset cache() is an alias for persist(StorageLevel.MEMORY_AND_DISK)\n",
    "- Caching of Spark DataFrame or Dataset is a **lazy operation**, meaning a DataFrame will not be cached until you trigger an action. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f02c08a097c15"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/06 15:34:47 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/09/06 15:34:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/06 15:34:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"CacheAndPersist\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"CacheAndPersist\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T13:34:52.210986069Z",
     "start_time": "2023-09-06T13:34:44.687133316Z"
    }
   },
   "id": "5ff697b42dacf879"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filePath = \"/home/pengfei/data_set/sf_fire/sf_fire_snappy.parquet\"\n",
    "\n",
    "df = spark.read.parquet(filePath)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T14:01:02.936104206Z",
     "start_time": "2023-09-06T14:00:57.749361825Z"
    }
   },
   "id": "30bfa43a6f50445d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+--------------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+-----------+----+----------------+--------+-------------+-------+--------------------+--------------+--------------+--------------------------+----------------------+------------------+--------------------+----------------+--------------------+\n",
      "|CallNumber|UnitID|IncidentNumber|            CallType|  CallDate| WatchDate|        ReceivedDtTm|           EntryDtTm|        DispatchDtTm|        ResponseDtTm|         OnSceneDtTm|       TransportDtTm|        HospitalDtTm|CallFinalDisposition|       AvailableDtTm|             Address|         City|ZipcodeofIncident|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|       CallTypeGroup|NumberofAlarms|      UnitType|Unitsequenceincalldispatch|FirePreventionDistrict|SupervisorDistrict|NeighborhoodDistrict|        Location|               RowID|\n",
      "+----------+------+--------------+--------------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+-----------+----+----------------+--------+-------------+-------+--------------------+--------------+--------------+--------------------------+----------------------+------------------+--------------------+----------------+--------------------+\n",
      "| 210391607|   E19|      21017645|              Alarms|02/08/2021|02/08/2021|02/08/2021 01:00:...|02/08/2021 01:01:...|02/08/2021 01:01:...|02/08/2021 01:03:...|02/08/2021 01:05:...|                null|                null|                Fire|02/08/2021 01:18:...|400 Block of SERR...|San Francisco|            94132|      B08|         19|8581|               3|       3|            3|   true|               Alarm|             1|        ENGINE|                         1|                     8|                 7|           Lakeshore|   210391607-E19|POINT (-122.48045...|\n",
      "| 210391164|   T04|      21017596|              Alarms|02/08/2021|02/08/2021|02/08/2021 10:54:...|02/08/2021 10:56:...|02/08/2021 10:56:...|02/08/2021 10:57:...|02/08/2021 10:59:...|                null|                null|                Fire|02/08/2021 11:06:...|600 Block of LONG...|San Francisco|            94158|      B03|         04|2264|               3|       3|            3|  false|               Alarm|             1|         TRUCK|                         1|                     3|                 6|         Mission Bay|   210391164-T04|POINT (-122.39227...|\n",
      "| 210391034|   E16|      21017578|Citizen Assist / ...|02/08/2021|02/08/2021|02/08/2021 10:18:...|02/08/2021 10:19:...|02/08/2021 10:19:...|02/08/2021 10:20:...|02/08/2021 10:27:...|                null|                null|                Fire|02/08/2021 10:53:...|FRANKLIN ST/FILBE...|San Francisco|            94123|      B04|         16|3233|               3|       3|            3|   true|               Alarm|             1|        ENGINE|                         1|                     4|                 2|              Marina|   210391034-E16|POINT (-122.42581...|\n",
      "| 210390767|   T19|      21017552|               Other|02/08/2021|02/08/2021|02/08/2021 08:50:...|02/08/2021 08:54:...|02/08/2021 08:55:...|02/08/2021 08:57:...|                null|                null|                null|                Fire|02/08/2021 09:02:...|CALL BOX: JOHN DA...|    Daly City|             null|      B09|         33|9922|               3|       3|            3|   true|                Fire|             1|         TRUCK|                         9|                  None|              None|                None|   210390767-T19|POINT (-122.46239...|\n",
      "| 210382984|   B05|      21017398|              Alarms|02/07/2021|02/07/2021|02/07/2021 09:18:...|02/07/2021 09:20:...|02/07/2021 09:21:...|02/07/2021 09:21:...|02/07/2021 09:25:...|                null|                null|                Fire|02/07/2021 09:35:...|2100 Block of FEL...|San Francisco|            94117|      B05|         12|4554|               3|       3|            3|  false|               Alarm|             1|         CHIEF|                         2|                     5|                 5|   Lone Mountain/USF|   210382984-B05|POINT (-122.45328...|\n",
      "| 210382403|   T05|      21017307|              Alarms|02/07/2021|02/07/2021|02/07/2021 05:24:...|02/07/2021 05:26:...|02/07/2021 05:26:...|02/07/2021 05:27:...|02/07/2021 05:30:...|                null|                null|                Fire|02/07/2021 05:50:...|1400 Block of GEA...|San Francisco|            94109|      B04|         38|3323|               3|       3|            3|  false|               Alarm|             1|         TRUCK|                         2|                     4|                 5|           Japantown|   210382403-T05|POINT (-122.42636...|\n",
      "| 210381978|   B10|      21017263|        Outside Fire|02/07/2021|02/07/2021|02/07/2021 03:09:...|02/07/2021 03:09:...|02/07/2021 03:11:...|02/07/2021 03:11:...|02/07/2021 03:11:...|                null|                null|                Fire|02/07/2021 03:39:...|500 Block of AMAD...|San Francisco|            94124|      B10|         25|6463|               3|       3|            3|  false|                Fire|             1|         CHIEF|                         5|                  10.0|                10|Bayview Hunters P...|   210381978-B10|POINT (-122.38299...|\n",
      "| 210381601|   T06|      21017206|              Alarms|02/07/2021|02/07/2021|02/07/2021 01:07:...|02/07/2021 01:09:...|02/07/2021 01:09:...|02/07/2021 01:10:...|02/07/2021 01:11:...|                null|                null|                Fire|02/07/2021 01:16:...|0 Block of SANCHE...|San Francisco|            94114|      B05|         06|5131|               3|       3|            3|  false|               Alarm|             1|         TRUCK|                         2|                     2|                 8| Castro/Upper Market|   210381601-T06|POINT (-122.43120...|\n",
      "| 210381356|  KM12|      21017173|    Medical Incident|02/07/2021|02/07/2021|02/07/2021 12:00:...|02/07/2021 12:00:...|02/07/2021 12:01:...|02/07/2021 12:02:...|02/07/2021 12:12:...|02/07/2021 12:27:...|02/07/2021 12:33:...|    Code 2 Transport|02/07/2021 01:05:...|2300 Block of 16T...|San Francisco|            94103|      B02|         29|5241|               2|       2|            2|  false|Non Life-threatening|             1|       PRIVATE|                         3|                     2|                10|             Mission|  210381356-KM12|POINT (-122.40952...|\n",
      "| 210380503|   E10|      21017065|              Alarms|02/07/2021|02/06/2021|02/07/2021 05:41:...|02/07/2021 05:42:...|02/07/2021 05:42:...|02/07/2021 05:44:...|02/07/2021 05:46:...|                null|                null|                Fire|02/07/2021 06:15:...|1400 Block of BAK...|San Francisco|            94115|      B05|         10|4262|               3|       3|            3|   true|               Alarm|             1|        ENGINE|                         2|                     5|                 5|           Japantown|   210380503-E10|POINT (-122.44301...|\n",
      "| 210380252|   E19|      21017021|              Alarms|02/07/2021|02/06/2021|02/07/2021 02:11:...|02/07/2021 02:12:...|02/07/2021 02:12:...|02/07/2021 02:14:...|02/07/2021 02:17:...|                null|                null|                Fire|02/07/2021 02:24:...|300 Block of ARBA...|San Francisco|            94132|      B08|         19|8582|               3|       3|            3|   true|               Alarm|             1|        ENGINE|                         1|                     8|                 7|           Lakeshore|   210380252-E19|POINT (-122.48306...|\n",
      "| 210372754|   E08|      21016911|              Alarms|02/06/2021|02/06/2021|02/06/2021 06:50:...|02/06/2021 06:51:...|02/06/2021 06:52:...|02/06/2021 06:53:...|02/06/2021 06:56:...|                null|                null|                Fire|02/06/2021 06:59:...|   0 Block of 6TH ST|San Francisco|            94103|      B03|         01|2251|               3|       3|            3|   true|               Alarm|             1|        ENGINE|                         2|                     3|                 6|     South of Market|   210372754-E08|POINT (-122.40902...|\n",
      "| 210370471|   B06|      21016646|      Structure Fire|02/06/2021|02/05/2021|02/06/2021 06:44:...|02/06/2021 06:46:...|02/06/2021 06:46:...|02/06/2021 06:48:...|02/06/2021 06:57:...|                null|                null|                Fire|02/06/2021 06:57:...|0 Block of DAKOTA ST|San Francisco|            94107|      B10|         37|2614|               3|       3|            3|  false|               Alarm|             1|         CHIEF|                         4|                    10|                10|        Potrero Hill|   210370471-B06|POINT (-122.39563...|\n",
      "| 210370398|   T07|      21016635|              Alarms|02/06/2021|02/05/2021|02/06/2021 05:33:...|02/06/2021 05:33:...|02/06/2021 05:33:...|02/06/2021 05:36:...|02/06/2021 05:40:...|                null|                null|                Fire|02/06/2021 05:42:...|800 Block of POTR...|San Francisco|            94110|      B10|         07|2553|               3|       3|            3|  false|               Alarm|             1|         TRUCK|                         2|                    10|                10|        Potrero Hill|   210370398-T07|POINT (-122.40672...|\n",
      "| 210370261|   T12|      21016610|              Alarms|02/06/2021|02/05/2021|02/06/2021 03:24:...|02/06/2021 03:27:...|02/06/2021 03:27:...|02/06/2021 03:29:...|02/06/2021 03:36:...|                null|                null|                Fire|02/06/2021 03:40:...|200 Block of UPPE...|San Francisco|            94117|      B05|         12|5165|               3|       3|            3|  false|               Alarm|             1|         TRUCK|                         3|                     5|                 8|      Haight Ashbury|   210370261-T12|POINT (-122.44517...|\n",
      "| 210362631|   T17|      21016433|              Alarms|02/05/2021|02/05/2021|02/05/2021 05:04:...|02/05/2021 05:06:...|02/05/2021 05:06:...|                null|                null|                null|                null|                Fire|02/05/2021 05:10:...|5800 Block of 3RD ST|San Francisco|            94124|      B10|         17|6537|               3|       3|            3|  false|               Alarm|             1|         TRUCK|                         3|                  10.0|                10|Bayview Hunters P...|   210362631-T17|POINT (-122.39472...|\n",
      "| 210360668|   E17|      21016151|        Outside Fire|02/05/2021|02/05/2021|02/05/2021 08:01:...|02/05/2021 08:05:...|02/05/2021 08:10:...|02/05/2021 08:10:...|02/05/2021 08:13:...|                null|                null|                Fire|02/05/2021 08:15:...|600 Block of INNE...|Hunters Point|            94124|      B10|         17|6663|               3|       3|            3|   true|                Fire|             1|        ENGINE|                         1|                    10|                10|Bayview Hunters P...|   210360668-E17|POINT (-122.37111...|\n",
      "| 210360399|    77|      21016122|    Medical Incident|02/05/2021|02/04/2021|02/05/2021 05:22:...|02/05/2021 05:24:...|02/05/2021 05:24:...|02/05/2021 05:25:...|                null|                null|                null|    Code 2 Transport|02/05/2021 05:28:...| 100 Block of 6TH ST|San Francisco|            94103|      B03|         01|2251|               3|       3|            3|   true|Potentially Life-...|             1|         MEDIC|                         3|                     3|                 6|     South of Market|    210360399-77|POINT (-122.40848...|\n",
      "| 210363081|   RC1|      21016489|    Medical Incident|02/05/2021|02/05/2021|02/05/2021 07:29:...|02/05/2021 07:29:...|02/05/2021 07:33:...|02/05/2021 07:34:...|                null|                null|                null|    Code 2 Transport|02/05/2021 07:35:...|  MISSION ST/10TH ST|San Francisco|            94103|      B02|         36|2341|               3|       E|            3|   true|Potentially Life-...|             1|RESCUE CAPTAIN|                         3|                     2|                 6|     South of Market|   210363081-RC1|POINT (-122.41590...|\n",
      "| 210390635|BLS841|      21017540|    Medical Incident|02/08/2021|02/08/2021|02/08/2021 08:09:...|02/08/2021 08:09:...|02/08/2021 08:09:...|02/08/2021 08:09:...|02/08/2021 08:49:...|                null|                null|               Other|02/08/2021 09:17:...| 0 Block of GROVE ST|San Francisco|            94102|      B02|         36|1552|               2|       2|            2|  false|                null|             1|         CHIEF|                         1|                     2|                 6|          Tenderloin|210390635-BLS841|POINT (-122.41739...|\n",
      "+----------+------+--------------+--------------------+----------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----------------+---------+-----------+----+----------------+--------+-------------+-------+--------------------+--------------+--------------+--------------------------+----------------------+------------------+--------------------+----------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T14:01:11.312625065Z",
     "start_time": "2023-09-06T14:01:09.293768214Z"
    }
   },
   "id": "6d7541ce35537246"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/06 16:08:10 WARN BlockManager: Block rdd_18_2 could not be removed as it was not found on disk or in memory\n",
      "23/09/06 16:08:10 ERROR Executor: Exception in task 1.0 in stage 8.0 (TID 18)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.buildNonNulls(NullableColumnBuilder.scala:86)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.buildNonNulls$(NullableColumnBuilder.scala:84)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.buildNonNulls(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:85)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$2999/0x0000000101306040.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$131/0x00000001002d2040.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:113)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1456/0x0000000100b8e840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "23/09/06 16:08:10 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 8.0 (TID 18),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.buildNonNulls(NullableColumnBuilder.scala:86)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.buildNonNulls$(NullableColumnBuilder.scala:84)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.buildNonNulls(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:85)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$2999/0x0000000101306040.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$131/0x00000001002d2040.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:113)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1456/0x0000000100b8e840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "23/09/06 16:08:10 WARN TaskSetManager: Lost task 1.0 in stage 8.0 (TID 18) (10.50.2.80 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.buildNonNulls(NullableColumnBuilder.scala:86)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.buildNonNulls$(NullableColumnBuilder.scala:84)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.buildNonNulls(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build(CompressibleColumnBuilder.scala:85)\n",
      "\tat org.apache.spark.sql.execution.columnar.compression.CompressibleColumnBuilder.build$(CompressibleColumnBuilder.scala:84)\n",
      "\tat org.apache.spark.sql.execution.columnar.NativeColumnBuilder.build(ColumnBuilder.scala:98)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$2999/0x0000000101306040.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$131/0x00000001002d2040.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:113)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1456/0x0000000100b8e840.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\n",
      "23/09/06 16:08:10 ERROR TaskSetManager: Task 1 in stage 8.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_56230/2699477237.py\", line 4, in <module>\n",
      "    cachedDf.show()\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py\", line 606, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      3\u001B[0m cachedDf \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mcache()\n\u001B[0;32m----> 4\u001B[0m \u001B[43mcachedDf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:606\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m--> 606\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31m<class 'str'>\u001B[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2116\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2113\u001B[0m     traceback\u001B[38;5;241m.\u001B[39mprint_exc()\n\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2116\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_showtraceback\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_pdb:\n\u001B[1;32m   2118\u001B[0m     \u001B[38;5;66;03m# drop into debugger\u001B[39;00m\n\u001B[1;32m   2119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebugger(force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/ipykernel/zmqshell.py:550\u001B[0m, in \u001B[0;36mZMQInteractiveShell._showtraceback\u001B[0;34m(self, etype, evalue, stb)\u001B[0m\n\u001B[1;32m    544\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    545\u001B[0m sys\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mflush()\n\u001B[1;32m    547\u001B[0m exc_content \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    548\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraceback\u001B[39m\u001B[38;5;124m\"\u001B[39m: stb,\n\u001B[1;32m    549\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mename\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mstr\u001B[39m(etype\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m),\n\u001B[0;32m--> 550\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevalue\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    551\u001B[0m }\n\u001B[1;32m    553\u001B[0m dh \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplayhook\n\u001B[1;32m    554\u001B[0m \u001B[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;66;03m# to pick up\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/protocol.py:471\u001B[0m, in \u001B[0;36mPy4JJavaError.__str__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__str__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    470\u001B[0m     gateway_client \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_exception\u001B[38;5;241m.\u001B[39m_gateway_client\n\u001B[0;32m--> 471\u001B[0m     answer \u001B[38;5;241m=\u001B[39m \u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexception_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    472\u001B[0m     return_value \u001B[38;5;241m=\u001B[39m get_return_value(answer, gateway_client, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001B[39;00m\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001B[39;00m\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# now we try to cache the dataframe\n",
    "# note that the cache() method is lazy transformation, if no action, it will not be executed.\n",
    "cachedDf = df.cache()\n",
    "cachedDf.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T14:08:11.714355598Z",
     "start_time": "2023-09-06T14:07:09.105433772Z"
    }
   },
   "id": "713557bb4a1a541b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above operation failed, because we only have 4 GB memory for the worker and half of it is reserved for the "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6f2ae419c74ecab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performence test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9830f6410cf2b005"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
