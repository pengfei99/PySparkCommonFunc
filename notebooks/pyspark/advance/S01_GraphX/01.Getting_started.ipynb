{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro of GraphX\n",
    "\n",
    "The official doc can be found [here](https://spark.apache.org/docs/latest/graphx-programming-guide.html#getting-started)\n",
    "\n",
    "We will use a new data structure called graph frames. The official doc can be found [here](https://graphframes.github.io/graphframes/docs/_site/user-guide.html)\n",
    "\n",
    "The jar file can be downloaded [here](http://spark-packages.org/package/graphframes/graphframes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import *\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/06 16:17:05 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/07/06 16:17:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/pengfei/opt/spark-3.3.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pengfei/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pengfei/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-53775aa4-4595-47fb-925c-1ea65b3bd159;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 477ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-53775aa4-4595-47fb-925c-1ea65b3bd159\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/25ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/06 16:17:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/06 16:17:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12') \\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12') \\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "vertices = spark.createDataFrame([('1', 'Carter', 'Derrick', 50),\n",
    "                                  ('2', 'May', 'Derrick', 26),\n",
    "                                 ('3', 'Mills', 'Jeff', 80),\n",
    "                                  ('4', 'Hood', 'Robert', 65),\n",
    "                                  ('5', 'Banks', 'Mike', 93),\n",
    "                                 ('98', 'Berg', 'Tim', 28),\n",
    "                                 ('99', 'Page', 'Allan', 16)],\n",
    "                                 ['id', 'name', 'firstname', 'age'])\n",
    "\n",
    "edges = spark.createDataFrame([('1', '2', 'friend'),\n",
    "                               ('2', '1', 'friend'),\n",
    "                              ('3', '1', 'friend'),\n",
    "                              ('1', '3', 'friend'),\n",
    "                               ('2', '3', 'follows'),\n",
    "                               ('3', '4', 'friend'),\n",
    "                               ('4', '3', 'friend'),\n",
    "                               ('5', '3', 'friend'),\n",
    "                               ('3', '5', 'friend'),\n",
    "                               ('4', '5', 'follows'),\n",
    "                              ('98', '99', 'friend'),\n",
    "                              ('99', '98', 'friend')],\n",
    "                              ['src', 'dst', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(vertices, edges)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+\n",
      "| id|  name|firstname|age|\n",
      "+---+------+---------+---+\n",
      "|  1|Carter|  Derrick| 50|\n",
      "|  2|   May|  Derrick| 26|\n",
      "|  3| Mills|     Jeff| 80|\n",
      "|  4|  Hood|   Robert| 65|\n",
      "|  5| Banks|     Mike| 93|\n",
      "| 98|  Berg|      Tim| 28|\n",
      "| 99|  Page|    Allan| 16|\n",
      "+---+------+---------+---+\n",
      "\n",
      "+---+---+-------+\n",
      "|src|dst|   type|\n",
      "+---+---+-------+\n",
      "|  1|  2| friend|\n",
      "|  2|  1| friend|\n",
      "|  3|  1| friend|\n",
      "|  1|  3| friend|\n",
      "|  2|  3|follows|\n",
      "|  3|  4| friend|\n",
      "|  4|  3| friend|\n",
      "|  5|  3| friend|\n",
      "|  3|  5| friend|\n",
      "|  4|  5|follows|\n",
      "| 98| 99| friend|\n",
      "| 99| 98| friend|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Take a look at the DataFrames\n",
    "g.vertices.show()\n",
    "g.edges.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|degree|\n",
      "+---+------+\n",
      "|  3|     7|\n",
      "|  1|     4|\n",
      "|  2|     3|\n",
      "|  4|     3|\n",
      "|  5|     3|\n",
      "| 98|     2|\n",
      "| 99|     2|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "The GraphFrame we just created is a directed one, and can be visualized as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
