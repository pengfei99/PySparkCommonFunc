{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Getting started with GraphX\n",
    "\n",
    "## 1.1 Intro of GraphX\n",
    "\n",
    "**GraphX is a component in Spark for graphs and graph-parallel computation**. At a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: **a directed multigraph with properties attached to each vertex and edge**. To support graph computation, GraphX exposes a set of fundamental operators (e.g., `subgraph, joinVertices, and aggregateMessages`) as well as an `optimized variant` of the [Pregel](https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel) API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.\n",
    "\n",
    "The official doc can be found [here](https://spark.apache.org/docs/latest/graphx-programming-guide.html#getting-started)\n",
    "\n",
    "\n",
    "## 1.2 Installation\n",
    "Although GraphX is a component of Spark (no need to install), but it uses a special data structure (**graphframes**) which is not included in Spark by default.\n",
    "We need to import the lib `graphframes` which allows us to create dataframe for graphe.\n",
    "\n",
    "The official doc of the data structure can be found [here](https://graphframes.github.io/graphframes/docs/_site/user-guide.html)\n",
    "\n",
    "The jar file can be downloaded [here](http://spark-packages.org/package/graphframes/graphframes)\n",
    "\n",
    "For scala api, it's quite simple, you can download the jar file, add the jar file to the context, Or ask the spark session to download it automatically by using below\n",
    "`.config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12')`.\n",
    "\n",
    "For python api, you need to do the above steps, and you need to install a python wrapper in your virtual env.\n",
    "\n",
    "```shell\n",
    "pip install graphframes\n",
    "```\n",
    "\n",
    "> If you visit the pypi page of this package, it's a little outdated. But it's only a wrapper, for now I don't encounter any compatibility issues."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 A simple example\n",
    "\n",
    "In below example, we will create two classical dataframes:\n",
    "1. store all vertices (nodes) of the graphe\n",
    "2. store all edges (relations between nodes) of the graphe\n",
    "\n",
    "Then we will create a graph frame by using the two dataframes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import *\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/07 16:28:16 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/07/07 16:28:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/pengfei/opt/spark-3.3.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pengfei/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pengfei/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6b10c96d-256a-4354-8001-ebdb6817a7fc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 372ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6b10c96d-256a-4354-8001-ebdb6817a7fc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/07 16:28:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12') \\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12') \\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "vertices = spark.createDataFrame([('1', 'Carter', 'Derrick', 50),\n",
    "                                  ('2', 'May', 'Derrick', 26),\n",
    "                                 ('3', 'Mills', 'Jeff', 80),\n",
    "                                  ('4', 'Hood', 'Robert', 65),\n",
    "                                  ('5', 'Banks', 'Mike', 93),\n",
    "                                 ('98', 'Berg', 'Tim', 28),\n",
    "                                 ('99', 'Page', 'Allan', 16)],\n",
    "                                 ['id', 'name', 'firstname', 'age'])\n",
    "\n",
    "edges = spark.createDataFrame([('1', '2', 'friend'),\n",
    "                               ('2', '1', 'friend'),\n",
    "                              ('3', '1', 'friend'),\n",
    "                              ('1', '3', 'friend'),\n",
    "                               ('2', '3', 'follows'),\n",
    "                               ('3', '4', 'friend'),\n",
    "                               ('4', '3', 'friend'),\n",
    "                               ('5', '3', 'friend'),\n",
    "                               ('3', '5', 'friend'),\n",
    "                               ('4', '5', 'follows'),\n",
    "                              ('98', '99', 'friend'),\n",
    "                              ('99', '98', 'friend')],\n",
    "                              ['src', 'dst', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(vertices, edges)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+\n",
      "| id|  name|firstname|age|\n",
      "+---+------+---------+---+\n",
      "|  1|Carter|  Derrick| 50|\n",
      "|  2|   May|  Derrick| 26|\n",
      "|  3| Mills|     Jeff| 80|\n",
      "|  4|  Hood|   Robert| 65|\n",
      "|  5| Banks|     Mike| 93|\n",
      "| 98|  Berg|      Tim| 28|\n",
      "| 99|  Page|    Allan| 16|\n",
      "+---+------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the vertices (nodes)\n",
    "g.vertices.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|src|dst|   type|\n",
      "+---+---+-------+\n",
      "|  1|  2| friend|\n",
      "|  2|  1| friend|\n",
      "|  3|  1| friend|\n",
      "|  1|  3| friend|\n",
      "|  2|  3|follows|\n",
      "|  3|  4| friend|\n",
      "|  4|  3| friend|\n",
      "|  5|  3| friend|\n",
      "|  3|  5| friend|\n",
      "|  4|  5|follows|\n",
      "| 98| 99| friend|\n",
      "| 99| 98| friend|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the edges (relations between nodes)\n",
    "g.edges.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|degree|\n",
      "+---+------+\n",
      "|  3|     7|\n",
      "|  1|     4|\n",
      "|  2|     3|\n",
      "|  4|     3|\n",
      "|  5|     3|\n",
      "| 98|     2|\n",
      "| 99|     2|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The GraphFrame we just created is a **directed graph**, and can be visualized as follows:\n",
    "![graphx_graph_exp1.webp](../../../../images/graphx_graph_exp1.webp)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Filtering the graph\n",
    "\n",
    "A GraphFrame itself can’t be filtered, but DataFrames deducted from a Graph can. Consequently, the filter-function (or any other function) can be used just as you would use it with DataFrames. The only trap-hole might be the correct use of quotation marks: the whole condition should be quoted. The examples below should clarify this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+\n",
      "| id|  name|firstname|age|\n",
      "+---+------+---------+---+\n",
      "|  1|Carter|  Derrick| 50|\n",
      "|  3| Mills|     Jeff| 80|\n",
      "|  4|  Hood|   Robert| 65|\n",
      "|  5| Banks|     Mike| 93|\n",
      "+---+------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter all the nodes which has age > 30\n",
    "filteredNode= g.vertices.filter(\"age > 30\")\n",
    "filteredNode.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  3|       4|\n",
      "|  1|       2|\n",
      "|  5|       2|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get all the nodes which has at least 2 relations\n",
    "filteredByDegree=g.inDegrees.filter(\"inDegree >= 2\").sort(\"inDegree\", ascending=False)\n",
    "filteredByDegree.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|src|dst|  type|\n",
      "+---+---+------+\n",
      "|  1|  2|friend|\n",
      "|  2|  1|friend|\n",
      "|  3|  1|friend|\n",
      "|  1|  3|friend|\n",
      "|  3|  4|friend|\n",
      "|  4|  3|friend|\n",
      "|  5|  3|friend|\n",
      "|  3|  5|friend|\n",
      "| 98| 99|friend|\n",
      "| 99| 98|friend|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get all relation which is friend\n",
    "filteredEdge=g.edges.filter('type == \"friend\"')\n",
    "filteredEdge.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Connected components\n",
    "\n",
    "A connected component of a graph is a subgraph in which any two vertices are connected to each other by one or more edges, and which is connected to no additional vertices in the supergraph. In the (undirected) example below there are three connected components. Connected components detection can be interesting for clustering, but also to make your computations more efficient.\n",
    "\n",
    "> Practically, GraphFrames requires you to set a directory where it can save checkpoints. Create such a folder in your working directory and drop the following line (where graphframes_cps is your new folder) in Jupyter to set the checkpoint directory.\n",
    "\n",
    "Then, the connected components can easily be computed with the connectedComponents-function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"/tmp/graphframes_cps\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+------------+\n",
      "| id|  name|firstname|age|   component|\n",
      "+---+------+---------+---+------------+\n",
      "|  1|Carter|  Derrick| 50|154618822656|\n",
      "|  2|   May|  Derrick| 26|154618822656|\n",
      "|  3| Mills|     Jeff| 80|154618822656|\n",
      "|  4|  Hood|   Robert| 65|154618822656|\n",
      "|  5| Banks|     Mike| 93|154618822656|\n",
      "| 98|  Berg|      Tim| 28|317827579904|\n",
      "| 99|  Page|    Allan| 16|317827579904|\n",
      "+---+------+---------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the connected components\n",
    "g.connectedComponents().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our mini-graph has two connected components, which are described for each vertex in the **component column**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Motif finding\n",
    "\n",
    "Finding motifs helps to execute queries to discover structural patterns in graphs. Network motifs are patterns that occur repeatedly in the graph and represent the relationships between the vertices. GraphFrames motif finding uses a declarative Domain Specific Language (DSL) for expressing structural queries.\n",
    "\n",
    "The query can be invoked by using the find-function, where the motif (in quotation marks) is expressed as the first parameter of the function.\n",
    "\n",
    "The following example will search for pairs of vertices a,b connected by edge e and pairs of vertices b,c connected by edge e2. It will return a DataFrame of all such structures in the graph, with columns for each of the named elements (vertices or edges) in the motif."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+----------------+\n",
      "|                   a|               e|                   b|              e2|\n",
      "+--------------------+----------------+--------------------+----------------+\n",
      "|{2, May, Derrick,...|  {2, 1, friend}|{1, Carter, Derri...|  {1, 2, friend}|\n",
      "|{3, Mills, Jeff, 80}|  {3, 1, friend}|{1, Carter, Derri...|  {1, 3, friend}|\n",
      "|{1, Carter, Derri...|  {1, 2, friend}|{2, May, Derrick,...|  {2, 1, friend}|\n",
      "|{1, Carter, Derri...|  {1, 3, friend}|{3, Mills, Jeff, 80}|  {3, 1, friend}|\n",
      "|{4, Hood, Robert,...|  {4, 3, friend}|{3, Mills, Jeff, 80}|  {3, 4, friend}|\n",
      "|{5, Banks, Mike, 93}|  {5, 3, friend}|{3, Mills, Jeff, 80}|  {3, 5, friend}|\n",
      "|{3, Mills, Jeff, 80}|  {3, 4, friend}|{4, Hood, Robert,...|  {4, 3, friend}|\n",
      "|{3, Mills, Jeff, 80}|  {3, 5, friend}|{5, Banks, Mike, 93}|  {5, 3, friend}|\n",
      "|{99, Page, Allan,...|{99, 98, friend}| {98, Berg, Tim, 28}|{98, 99, friend}|\n",
      "| {98, Berg, Tim, 28}|{98, 99, friend}|{99, Page, Allan,...|{99, 98, friend}|\n",
      "+--------------------+----------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If edges and/or vertices are anonymous, they won’t be displayed in the resulting DataFrame. Motifs can be joined by a semicolon and can be negated with a exclamation mark. More details about the Domain Specific Language can be found in the [documentation](https://graphframes.github.io/graphframes/docs/_site/user-guide.html).\n",
    "\n",
    "As an example we can try to find the mutual friends for any pair of users a and c. In order to be a mutual friend b, b must be a friend with both a and c (and not just followed by c, for example)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   a|                   b|                   c|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{4, Hood, Robert,...|{3, Mills, Jeff, 80}|{4, Hood, Robert,...|\n",
      "|{3, Mills, Jeff, 80}|{1, Carter, Derri...|{2, May, Derrick,...|\n",
      "|{4, Hood, Robert,...|{3, Mills, Jeff, 80}|{1, Carter, Derri...|\n",
      "|{5, Banks, Mike, 93}|{3, Mills, Jeff, 80}|{1, Carter, Derri...|\n",
      "|{3, Mills, Jeff, 80}|{1, Carter, Derri...|{3, Mills, Jeff, 80}|\n",
      "|{5, Banks, Mike, 93}|{3, Mills, Jeff, 80}|{5, Banks, Mike, 93}|\n",
      "|{5, Banks, Mike, 93}|{3, Mills, Jeff, 80}|{4, Hood, Robert,...|\n",
      "|{1, Carter, Derri...|{2, May, Derrick,...|{1, Carter, Derri...|\n",
      "|{1, Carter, Derri...|{3, Mills, Jeff, 80}|{4, Hood, Robert,...|\n",
      "|{2, May, Derrick,...|{1, Carter, Derri...|{3, Mills, Jeff, 80}|\n",
      "|{1, Carter, Derri...|{3, Mills, Jeff, 80}|{5, Banks, Mike, 93}|\n",
      "|{1, Carter, Derri...|{3, Mills, Jeff, 80}|{1, Carter, Derri...|\n",
      "|{99, Page, Allan,...| {98, Berg, Tim, 28}|{99, Page, Allan,...|\n",
      "| {98, Berg, Tim, 28}|{99, Page, Allan,...| {98, Berg, Tim, 28}|\n",
      "|{3, Mills, Jeff, 80}|{5, Banks, Mike, 93}|{3, Mills, Jeff, 80}|\n",
      "|{4, Hood, Robert,...|{3, Mills, Jeff, 80}|{5, Banks, Mike, 93}|\n",
      "|{2, May, Derrick,...|{1, Carter, Derri...|{2, May, Derrick,...|\n",
      "|{3, Mills, Jeff, 80}|{4, Hood, Robert,...|{3, Mills, Jeff, 80}|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mutualFriends = g.find(\"(a)-[]->(b); (b)-[]->(c); (c)-[]->(b); (b)-[]->(a)\").dropDuplicates()\n",
    "mutualFriends.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------------+--------------------+\n",
      "|a                    |b                       |c                   |\n",
      "+---------------------+------------------------+--------------------+\n",
      "|{2, May, Derrick, 26}|{1, Carter, Derrick, 50}|{3, Mills, Jeff, 80}|\n",
      "+---------------------+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To query all the mutual friends between 2 and 3 we can filter the DataFrame.\n",
    "\n",
    "mutualFriends.filter('a.id == 2 and c.id == 3').show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. TriangleCount and PageRank\n",
    "\n",
    "To finish up, we’ll discover two additional built-in algorithms. TriangleCount counts the number of triangles passing through each vertex in this graph. A triangle can be defined as a group of three vertices that is interrelated, i.e. a has an edge to b, b has an edge to c, and c has an edge to a. The example below shows a graph with two triangles.\n",
    "\n",
    "![](../../../../images/graphx_graph_exp2.webp)\n",
    "\n",
    "In the GraphFrames package you can count the number of triangles passing through each vertex by invoking the triangleCount-function. Note that our simple example has only two triangles in total. Triangles are used for various tasks for real‐life networks, including community discovery, link prediction, and spam filtering."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+---------+---+\n",
      "|count| id|  name|firstname|age|\n",
      "+-----+---+------+---------+---+\n",
      "|    1|  1|Carter|  Derrick| 50|\n",
      "|    2|  3| Mills|     Jeff| 80|\n",
      "|    1|  2|   May|  Derrick| 26|\n",
      "|    1|  5| Banks|     Mike| 93|\n",
      "|    1|  4|  Hood|   Robert| 65|\n",
      "|    0| 98|  Berg|      Tim| 28|\n",
      "|    0| 99|  Page|    Allan| 16|\n",
      "+-----+---+------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.triangleCount().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last function we discuss is PageRank. PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.\n",
    "\n",
    "The PageRank algorithm holds that an imaginary surfer who is randomly clicking on links will eventually stop clicking. The probability, at any step, that the person will continue is a damping factor. The damping factor can be be set by changing the resetProbability parameter. Other important parameters are the tolerance (tol) and the maximum number of iterations (maxIter)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+------------------+\n",
      "| id|  name|firstname|age|          pagerank|\n",
      "+---+------+---------+---+------------------+\n",
      "|  3| Mills|     Jeff| 80| 1.853919642738813|\n",
      "| 98|  Berg|      Tim| 28|1.0225331112091938|\n",
      "| 99|  Page|    Allan| 16|1.0225331112091938|\n",
      "|  5| Banks|     Mike| 93|0.9703579134677663|\n",
      "|  1|Carter|  Derrick| 50|0.9055074972891308|\n",
      "|  4|  Hood|   Robert| 65|0.6873519241384106|\n",
      "|  2|   May|  Derrick| 26|0.5377967999474921|\n",
      "+---+------+---------+---+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+------------------+\n",
      "|src|dst|   type|            weight|\n",
      "+---+---+-------+------------------+\n",
      "|  3|  5| friend|0.3333333333333333|\n",
      "|  3|  1| friend|0.3333333333333333|\n",
      "|  3|  4| friend|0.3333333333333333|\n",
      "| 98| 99| friend|               1.0|\n",
      "| 99| 98| friend|               1.0|\n",
      "|  5|  3| friend|               1.0|\n",
      "|  1|  3| friend|               0.5|\n",
      "|  1|  2| friend|               0.5|\n",
      "|  4|  3| friend|               0.5|\n",
      "|  4|  5|follows|               0.5|\n",
      "|  2|  3|follows|               0.5|\n",
      "|  2|  1| friend|               0.5|\n",
      "+---+---+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = g.pageRank(resetProbability=0.15, tol=0.01)\n",
    "## look at the pagerank score for every vertex\n",
    "pr.vertices.show()\n",
    "## look at the weight of every edge\n",
    "pr.edges.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 6 Directed vs un directed edges\n",
    "\n",
    "Undirected graphs have edges that do not have a direction. The edges indicate a two-way relationship, in that each edge can be traversed in both directions. If your DataFrame only consist of two-way directed edges, you may be interested in analyzing undirected edges. You can convert your graph by mapping a function over the edges DataFrame that deletes the row if src ≥ dst (or the other way around). In GraphX you could use to_undirected() to create a deep, undirected copy of the Graph, unfortunately GraphFrames does not support this functionality.\n",
    "\n",
    "An easy example to work around this missing functionality can be found in the following code snippet. Please note that the ‘follows’ edge doesn’t really make sense in an undirected graph, since doesn’t represent a two-way relationship.\n",
    "\n",
    "```python\n",
    "copy = edges\n",
    "from pyspark.sql.functions import udf\n",
    "@udf(\"string\")\n",
    "def to_undir(src, dst):\n",
    "    if src >= dst:\n",
    "        return 'Delete'\n",
    "    else :\n",
    "        return 'Keep'\n",
    "copy.withColumn('undir', to_undir(copy.src, copy.dst))\\\n",
    ".filter('undir == \"Keep\"').drop('undir').show()\n",
    "## for efficiency, it's better to avoid udf functions where possible ## and use built-in pyspark.sql.functions instead.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
