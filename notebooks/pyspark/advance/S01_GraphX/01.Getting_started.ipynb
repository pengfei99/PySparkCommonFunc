{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Getting started with GraphX\n",
    "\n",
    "## 1.1 Intro of GraphX\n",
    "\n",
    "**GraphX is a component in Spark for graphs and graph-parallel computation**. At a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: **a directed multigraph with properties attached to each vertex and edge**. To support graph computation, GraphX exposes a set of fundamental operators (e.g., `subgraph, joinVertices, and aggregateMessages`) as well as an `optimized variant` of the [Pregel](https://spark.apache.org/docs/latest/graphx-programming-guide.html#pregel) API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.\n",
    "\n",
    "The official doc can be found [here](https://spark.apache.org/docs/latest/graphx-programming-guide.html#getting-started)\n",
    "\n",
    "\n",
    "## 1.2 Installation\n",
    "Although GraphX is a component of Spark (no need to install), but it uses a special data structure (**graphframes**) which is not included in Spark by default.\n",
    "We need to import the lib `graphframes` which allows us to create dataframe for graphe.\n",
    "\n",
    "The official doc of the data structure can be found [here](https://graphframes.github.io/graphframes/docs/_site/user-guide.html)\n",
    "\n",
    "The jar file can be downloaded [here](http://spark-packages.org/package/graphframes/graphframes)\n",
    "\n",
    "For scala api, it's quite simple, you can download the jar file, add the jar file to the context, Or ask the spark session to download it automatically by using below\n",
    "`.config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12')`.\n",
    "\n",
    "For python api, you need to do the above steps, and you need to install a python wrapper in your virtual env.\n",
    "\n",
    "```shell\n",
    "pip install graphframes\n",
    "```\n",
    "\n",
    "> If you visit the pypi page of this package, it's a little outdated. But it's only a wrapper, for now I don't encounter any compatibility issues."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 A simple example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import *\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/07 13:36:43 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/07/07 13:36:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/pengfei/opt/spark-3.3.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pengfei/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pengfei/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-737adafe-8024-4dc5-b8a7-9a37155c045b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 295ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-737adafe-8024-4dc5-b8a7-9a37155c045b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/07 13:36:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "if local:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12') \\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"RepartitionAndCoalesce\")\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:master\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config('spark.jars.packages','graphframes:graphframes:0.8.2-spark3.2-s_2.12') \\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\",\"2g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# make the large dataframe show pretty\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "vertices = spark.createDataFrame([('1', 'Carter', 'Derrick', 50),\n",
    "                                  ('2', 'May', 'Derrick', 26),\n",
    "                                 ('3', 'Mills', 'Jeff', 80),\n",
    "                                  ('4', 'Hood', 'Robert', 65),\n",
    "                                  ('5', 'Banks', 'Mike', 93),\n",
    "                                 ('98', 'Berg', 'Tim', 28),\n",
    "                                 ('99', 'Page', 'Allan', 16)],\n",
    "                                 ['id', 'name', 'firstname', 'age'])\n",
    "\n",
    "edges = spark.createDataFrame([('1', '2', 'friend'),\n",
    "                               ('2', '1', 'friend'),\n",
    "                              ('3', '1', 'friend'),\n",
    "                              ('1', '3', 'friend'),\n",
    "                               ('2', '3', 'follows'),\n",
    "                               ('3', '4', 'friend'),\n",
    "                               ('4', '3', 'friend'),\n",
    "                               ('5', '3', 'friend'),\n",
    "                               ('3', '5', 'friend'),\n",
    "                               ('4', '5', 'follows'),\n",
    "                              ('98', '99', 'friend'),\n",
    "                              ('99', '98', 'friend')],\n",
    "                              ['src', 'dst', 'type'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(vertices, edges)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---+\n",
      "| id|  name|firstname|age|\n",
      "+---+------+---------+---+\n",
      "|  1|Carter|  Derrick| 50|\n",
      "|  2|   May|  Derrick| 26|\n",
      "|  3| Mills|     Jeff| 80|\n",
      "|  4|  Hood|   Robert| 65|\n",
      "|  5| Banks|     Mike| 93|\n",
      "| 98|  Berg|      Tim| 28|\n",
      "| 99|  Page|    Allan| 16|\n",
      "+---+------+---------+---+\n",
      "\n",
      "+---+---+-------+\n",
      "|src|dst|   type|\n",
      "+---+---+-------+\n",
      "|  1|  2| friend|\n",
      "|  2|  1| friend|\n",
      "|  3|  1| friend|\n",
      "|  1|  3| friend|\n",
      "|  2|  3|follows|\n",
      "|  3|  4| friend|\n",
      "|  4|  3| friend|\n",
      "|  5|  3| friend|\n",
      "|  3|  5| friend|\n",
      "|  4|  5|follows|\n",
      "| 98| 99| friend|\n",
      "| 99| 98| friend|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the vertices (nodes)\n",
    "g.vertices.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show the edges (relations between nodes)\n",
    "g.edges.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|degree|\n",
      "+---+------+\n",
      "|  3|     7|\n",
      "|  1|     4|\n",
      "|  2|     3|\n",
      "|  4|     3|\n",
      "|  5|     3|\n",
      "| 98|     2|\n",
      "| 99|     2|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The GraphFrame we just created is a **directed graph**, and can be visualized as follows:\n",
    "![graphx_graph_exp1.webp](../../../../images/graphx_graph_exp1.webp)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
