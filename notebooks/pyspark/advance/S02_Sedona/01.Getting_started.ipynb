{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Apache Sedona\n",
    "\n",
    "**Apache Sedona (formerly known as GeoSpark) is a cluster computing system for processing large-scale spatial data**. Sedona extends existing cluster computing systems, such as Apache Spark and Apache Flink, with a set of out-of-the-box distributed Spatial Datasets and Spatial SQL that efficiently load, process, and analyze large-scale spatial data across machines.\n",
    "\n",
    "You may say we already have `GeoPandas`, why we need Sedona. GeoPandas is greate when your dataset is small (< 2 GB), it can't host large dataset.\n",
    "\n",
    "The official Sedona site is [here](https://sedona.apache.org/1.4.1/)\n",
    "\n",
    "## 1. What Sedona offers?\n",
    "\n",
    "- Distributed spatial datasets\n",
    "   * Spatial RDD on Spark\n",
    "   * Spatial DataFrame/SQL on Spark\n",
    "   * Spatial DataStream on Flink\n",
    "   * Spatial Table/SQL on Flink\n",
    "\n",
    "- Complex spatial objects\n",
    "   * Vector geometries / trajectories\n",
    "   * Raster images with Map Algebra\n",
    "   * Various input formats: CSV, TSV, WKT, WKB, GeoJSON, Shapefile, GeoTIFF, NetCDF/HDF\n",
    "\n",
    "- Distributed spatial queries\n",
    "   * Spatial query: range query, range join query, distance join query, K Nearest Neighbor query\n",
    "   * Spatial index: R-Tree, Quad-Tree\n",
    "\n",
    "- Rich spatial analytics toolsÂ¶\n",
    "   * Coordinate Reference System / Spatial Reference System Transformation\n",
    "   * High resolution map generation: Visualize Spatial DataFrame/RDD\n",
    "   * Apache Zeppelin integration\n",
    "   * Support Scala, Java, Python, R\n",
    "\n",
    "\n",
    "## 2. Install and configure Sedona\n",
    "Sedona can work on top of a `Spark cluster or a Flink cluster`, and it provides many language as API:\n",
    "- Scala/Java\n",
    "- Python\n",
    "- R\n",
    "\n",
    "In this tutorial, I only show how to install it on Spark with Scala and Python API.\n",
    "\n",
    "For more details, you can visit the official [doc](https://sedona.apache.org/1.4.1/setup/install-python/)\n",
    "\n",
    "### 2.1 Get the Sedona jar file\n",
    "\n",
    "You can find all Sedona release jar file in the [maven central repo](https://mvnrepository.com/artifact/org.apache.sedona?p=1).\n",
    "\n",
    "#### Use shaded jar files\n",
    "\n",
    "To facilitate the installation, Sedona provides `shaded jars` (We only need to import two jars files).\n",
    "\n",
    "For example if your spark env is `3.4> spark > 3.0 with Scala 2.12`, you will need the below mvn conf\n",
    "\n",
    "```xml\n",
    "<dependencies>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-spark-shaded-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-viz-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper -->\n",
    "<dependency>\n",
    "    <groupId>org.datasyslab</groupId>\n",
    "    <artifactId>geotools-wrapper</artifactId>\n",
    "    <version>1.4.0-28.2</version>\n",
    "</dependency>\n",
    "</dependencies>\n",
    "```\n",
    "\n",
    "The optional **GeoTools library** is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars.\n",
    "\n",
    "> For other spark env, you can find the full doc [here](https://sedona.apache.org/1.4.1/setup/maven-coordinates/)\n",
    "\n",
    "\n",
    "#### Use unshaded jars\n",
    "\n",
    "If you use unshaded jars, your mvn config file will become longer. Below is an example `3.4> spark > 3.0 with Scala 2.12`.\n",
    "\n",
    "```xml\n",
    "<dependencies>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-core-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-sql-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-viz-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<!-- Required if you use Sedona Python -->\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-python-adapter-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "    <groupId>org.datasyslab</groupId>\n",
    "    <artifactId>geotools-wrapper</artifactId>\n",
    "    <version>1.4.0-28.2</version>\n",
    "</dependency>\n",
    "</dependencies>\n",
    "```\n",
    "\n",
    "> You can notice we have much more jar to import.\n",
    "\n",
    "### 2.2 Import the sedona Jar files into your spark session. (Create sedona config)\n",
    "\n",
    "There are two ways to import Jar files into your spark session.\n",
    "\n",
    "1. Put the jar files directly into the $SPARK_HOME/jars/ (In cluster mode, make sure all the worker nodes also have the jar file in place)\n",
    "2. Ask spark session to download the jar file by using the `spark.jars.packages` config\n",
    "\n",
    "Check the below example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T09:04:31.492173236Z",
     "start_time": "2023-10-04T09:04:31.447929311Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build a spark session with sedona (sedona < 1.4.1)\n",
    "# spark = SparkSession. \\\n",
    "#     builder. \\\n",
    "#     appName('appName'). \\\n",
    "#     config(\"spark.serializer\", KryoSerializer). \\\n",
    "#     config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "#     config('spark.jars.packages',\n",
    "#            'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n",
    "#            'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "#     getOrCreate()\n",
    "# SedonaRegistrator.registerAll(spark)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.1 Create sedona context from scratch\n",
    "\n",
    "The below example shows how to create a sedona context from scratch.\n",
    "\n",
    "We need extra jars for the sedona classes. To load these jars into the spark context, you can use the two below config:\n",
    "- \"spark.jars.packages\", \"package id\" : It will download the jar for both driver node and worker node. But you need to have internet connection\n",
    "- \"spark.jars\", \"jar path\": This config requires you to download jar manually on the local file system of the driver and worker node. You don't need internet connection anymore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/04 11:03:09 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/10/04 11:03:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/10/04 11:03:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# build a sedona session offline\n",
    "jar_folder = Path(r\"/home/pengfei/git/PySparkCommonFunc/jars\")\n",
    "jar_list = [str(jar) for jar in jar_folder.iterdir() if jar.is_file()]\n",
    "jar_path = \",\".join(jar_list)\n",
    "\n",
    "config = SedonaContext.builder(). \\\n",
    "    config('spark.jars', jar_path). \\\n",
    "    getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T09:03:16.135736302Z",
     "start_time": "2023-10-04T09:03:02.276902678Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/03 15:26:55 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/10/03 15:26:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/10/03 15:26:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# build a sedona session (sedona >= 1.4.1) online\n",
    "config = SedonaContext.builder(). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "    getOrCreate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T13:27:07.973448250Z",
     "start_time": "2023-10-03T13:26:51.449846845Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create a sedona context\n",
    "sedona = SedonaContext.create(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T09:05:17.499201323Z",
     "start_time": "2023-10-04T09:05:06.362811495Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.2 Create sedona context from an existing spark session\n",
    "\n",
    "In some case, you don't have the right to create a spark session from scratch (e.g. in Wherobots/AWS EMR/Databricks). The platform provides already a spark session.\n",
    "\n",
    "You can use below command to create a sedona context from an existing spark session.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "# suppose you have a spark session called spark\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('MySedona'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "     getOrCreate()\n",
    "\n",
    "# you can create a sedona context from it\n",
    "sedona = SedonaContext.create(spark)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Use sedona to read datasets\n",
    "\n",
    "Sedona provides a sedona SQL api, which allows you to read, transform and write geospatial data\n",
    "\n",
    "You can find the full doc [here](https://sedona.apache.org/1.4.1/api/sql/Overview/)\n",
    "\n",
    "In this tutorial, we will read a tsv(tab-separated values) file that represents some country coordinates (polygons) in the USA."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "file_path=\"../../../../data/county_small.tsv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T09:05:23.576356114Z",
     "start_time": "2023-10-04T09:05:23.509317077Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n",
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n",
      "|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n",
      "|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n",
      "|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n",
      "|POLYGON ((-98.273...| 31|129|00835886|31129|   Nuckolls|     Nuckolls County| 06| H1|G4020|null| null|null|   A|1489645187| 1718484|+40.1764918|-098.0468422|\n",
      "|POLYGON ((-65.910...| 72|085|01804523|72085|Las Piedras|Las Piedras Munic...| 13| H1|G4020| 490|41980|null|   A|  87748363|   32509|+18.1871483|-065.8711890|\n",
      "|POLYGON ((-97.129...| 46|099|01265772|46099|  Minnehaha|    Minnehaha County| 06| H1|G4020|null|43620|null|   A|2090540341|17349847|+43.6674723|-096.7957261|\n",
      "|POLYGON ((-99.821...| 48|327|01383949|48327|     Menard|       Menard County| 06| H1|G4020|null| null|null|   A|2336245914|  613559|+30.8843655|-099.8539896|\n",
      "|POLYGON ((-120.65...| 06|091|00277310|06091|     Sierra|       Sierra County| 06| H1|G4020|null| null|null|   A|2468686374|23299110|+39.5769252|-120.5219926|\n",
      "|POLYGON ((-85.239...| 21|053|00516873|21053|    Clinton|      Clinton County| 06| H1|G4020|null| null|null|   A| 510864252|21164150|+36.7288647|-085.1534262|\n",
      "|POLYGON ((-83.880...| 39|063|01074044|39063|    Hancock|      Hancock County| 06| H1|G4020| 248|22300|null|   A|1376210232| 5959837|+41.0004711|-083.6660335|\n",
      "|POLYGON ((-102.08...| 48|189|01383880|48189|       Hale|         Hale County| 06| H1|G4020|null|38380|null|   A|2602115649|  246678|+34.0684364|-101.8228879|\n",
      "|POLYGON ((-85.978...| 01|027|00161539|01027|       Clay|         Clay County| 06| H1|G4020|null| null|null|   A|1564252367| 5284573|+33.2703999|-085.8635254|\n",
      "|POLYGON ((-101.62...| 48|011|01383791|48011|  Armstrong|    Armstrong County| 06| H1|G4020| 108|11100|null|   A|2354581764|12219587|+34.9641790|-101.3566363|\n",
      "|POLYGON ((-84.397...| 39|003|01074015|39003|      Allen|        Allen County| 06| H1|G4020| 338|30620|null|   A|1042470093|11266164|+40.7716274|-084.1061032|\n",
      "|POLYGON ((-82.449...| 13|189|00348794|13189|   McDuffie|     McDuffie County| 06| H1|G4020|null|12260|null|   A| 666816637|23116292|+33.4824637|-082.4731880|\n",
      "|POLYGON ((-90.191...| 55|111|01581115|55111|       Sauk|         Sauk County| 06| H1|G4020| 357|12660|null|   A|2152007753|45296336|+43.4279976|-089.9433290|\n",
      "|POLYGON ((-92.415...| 05|137|00069902|05137|      Stone|        Stone County| 06| H1|G4020|null| null|null|   A|1570579427| 7841929|+35.8570312|-092.1405728|\n",
      "|POLYGON ((-117.74...| 41|063|01155135|41063|    Wallowa|      Wallowa County| 06| H1|G4020|null| null|null|   A|8148602810|14199330|+45.5937530|-117.1855796|\n",
      "|POLYGON ((-80.518...| 42|007|01214112|42007|     Beaver|       Beaver County| 06| H1|G4020| 430|38300|null|   A|1125901160|24165972|+40.6841401|-080.3507209|\n",
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n"
     ]
    }
   ],
   "source": [
    "rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(file_path)\n",
    "rawDf.createOrReplaceTempView(\"rawdf\")\n",
    "rawDf.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T09:05:27.911915724Z",
     "start_time": "2023-10-04T09:05:24.426135190Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "rawDf.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T09:05:29.869512967Z",
     "start_time": "2023-10-04T09:05:29.614913287Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Create a Geometry type column\n",
    "\n",
    "You can notice the `_c0` column has type string, even-though its value is a polygon of GPS coordinates. So first step is to convert the string column into a `Geometry type column`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        county_shape|         county_name|\n",
      "+--------------------+--------------------+\n",
      "|POLYGON ((-97.019...|       Cuming County|\n",
      "|POLYGON ((-123.43...|    Wahkiakum County|\n",
      "|POLYGON ((-104.56...|      De Baca County|\n",
      "|POLYGON ((-96.910...|    Lancaster County|\n",
      "|POLYGON ((-98.273...|     Nuckolls County|\n",
      "|POLYGON ((-65.910...|Las Piedras Munic...|\n",
      "|POLYGON ((-97.129...|    Minnehaha County|\n",
      "|POLYGON ((-99.821...|       Menard County|\n",
      "|POLYGON ((-120.65...|       Sierra County|\n",
      "|POLYGON ((-85.239...|      Clinton County|\n",
      "|POLYGON ((-83.880...|      Hancock County|\n",
      "|POLYGON ((-102.08...|         Hale County|\n",
      "|POLYGON ((-85.978...|         Clay County|\n",
      "|POLYGON ((-101.62...|    Armstrong County|\n",
      "|POLYGON ((-84.397...|        Allen County|\n",
      "|POLYGON ((-82.449...|     McDuffie County|\n",
      "|POLYGON ((-90.191...|         Sauk County|\n",
      "|POLYGON ((-92.415...|        Stone County|\n",
      "|POLYGON ((-117.74...|      Wallowa County|\n",
      "|POLYGON ((-80.518...|       Beaver County|\n",
      "+--------------------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spatialDf=sedona.sql(\"select ST_GeomFromText(rawdf._c0) as county_shape, rawdf._c6 as county_name from rawdf\")\n",
    "spatialDf.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T13:27:29.271650867Z",
     "start_time": "2023-10-03T13:27:27.443806840Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county_shape: geometry (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "spatialDf.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T13:27:31.242922392Z",
     "start_time": "2023-10-03T13:27:31.158308903Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can notice the `county_shape` column has geometry type now."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Save the geo dataframe\n",
    "\n",
    "To save a Spatial DataFrame to some permanent storage such as Hive tables, S3 and HDFS, you can simply `convert each geometry in the Geometry type column back to a plain String` and save the plain DataFrame to wherever you want.\n",
    "\n",
    "Use the following code to convert the Geometry column in a DataFrame back to a WKT string column:\n",
    "\n",
    "```sql\n",
    "SELECT ST_AsText(county_shape) FROM rawdf\n",
    "```\n",
    "\n",
    "\n",
    "### 4.1 Save GeoParquet\n",
    "\n",
    "Since v1.3.0, Sedona natively supports writing GeoParquet file. GeoParquet can be saved as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"/tmp/county_geo_parquet\"\n",
    "spatialDf.write.format(\"geoparquet\").save(output_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sedona/spark allow us to read and write many geospatial datasource, we won't show all of them in this tutorial. We will have a dedicated section for this"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
