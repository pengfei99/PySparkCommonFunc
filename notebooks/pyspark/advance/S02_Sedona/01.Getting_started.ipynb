{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Apache Sedona\n",
    "\n",
    "**Apache Sedona (formerly known as GeoSpark) is a cluster computing system for processing large-scale spatial data**. Sedona extends existing cluster computing systems, such as Apache Spark and Apache Flink, with a set of out-of-the-box distributed Spatial Datasets and Spatial SQL that efficiently load, process, and analyze large-scale spatial data across machines.\n",
    "\n",
    "You may say we already have `GeoPandas`, why we need Sedona. GeoPandas is greate when your dataset is small (< 2 GB), it can't host large dataset.\n",
    "\n",
    "The official Sedona site is [here](https://sedona.apache.org)\n",
    "\n",
    "Sedona can be connected with various computational framework:\n",
    "- spark\n",
    "- flink\n",
    "- snowflake\n",
    "\n",
    "> In this tutorial, we only focus on how to use sedona in spark\n",
    "\n",
    "## 1. What Sedona offers?\n",
    "\n",
    "- Distributed spatial datasets\n",
    "   * Spatial RDD on Spark\n",
    "   * Spatial DataFrame/SQL on Spark\n",
    "   * Spatial DataStream on Flink\n",
    "   * Spatial Table/SQL on Flink\n",
    "\n",
    "- Complex spatial objects\n",
    "   * Vector geometries / trajectories\n",
    "   * Raster images with Map Algebra\n",
    "   * Various input formats: CSV, TSV, WKT, WKB, GeoJSON, Shapefile, GeoTIFF, NetCDF/HDF\n",
    "\n",
    "- Distributed spatial queries\n",
    "   * Spatial query: range query, range join query, distance join query, K Nearest Neighbor query\n",
    "   * Spatial index: R-Tree, Quad-Tree\n",
    "\n",
    "- Rich spatial analytics toolsÂ¶\n",
    "   * Coordinate Reference System / Spatial Reference System Transformation\n",
    "   * High resolution map generation: Visualize Spatial DataFrame/RDD\n",
    "   * Apache Zeppelin integration\n",
    "   * Support Scala, Java, Python, R\n",
    "\n",
    "\n",
    "## 2. Install and configure Sedona\n",
    "When Sedona works on top of a `Spark cluster`, and it provides the below languages as API:\n",
    "- Scala/Java\n",
    "- Python\n",
    "- R\n",
    "\n",
    "> In this tutorial, I only show how to install it on Spark with Scala and Python API.\n",
    "\n",
    "For more details, you can visit the official [doc](https://sedona.apache.org/1.4.1/setup/install-python/)\n",
    "\n",
    "### 2.1 Get the Sedona jar file\n",
    "\n",
    "You can find all Sedona release jar file in the [maven central repo](https://mvnrepository.com/artifact/org.apache.sedona?p=1).\n",
    "\n",
    "#### Use shaded jar files\n",
    "\n",
    "To facilitate the installation, Sedona provides `shaded jars` (We only need to import two jars files).\n",
    "\n",
    "For example if your spark env is `3.4> spark > 3.0 with Scala 2.12`, you will need the below mvn conf\n",
    "\n",
    "```xml\n",
    "<dependencies>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-spark-shaded-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-viz-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<!-- Optional: https://mvnrepository.com/artifact/org.datasyslab/geotools-wrapper -->\n",
    "<dependency>\n",
    "    <groupId>org.datasyslab</groupId>\n",
    "    <artifactId>geotools-wrapper</artifactId>\n",
    "    <version>1.4.0-28.2</version>\n",
    "</dependency>\n",
    "</dependencies>\n",
    "```\n",
    "\n",
    "The optional **GeoTools library** is required if you want to use CRS transformation, ShapefileReader or GeoTiff reader. This wrapper library is a re-distribution of GeoTools official jars.\n",
    "\n",
    "> For other spark env, you can find the full doc [here](https://sedona.apache.org/1.4.1/setup/maven-coordinates/)\n",
    "\n",
    "\n",
    "#### Use unshaded jars\n",
    "\n",
    "If you use unshaded jars, your mvn config file will become longer. Below is an example `3.4> spark > 3.0 with Scala 2.12`.\n",
    "\n",
    "```xml\n",
    "<dependencies>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-core-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-sql-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-viz-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<!-- Required if you use Sedona Python -->\n",
    "<dependency>\n",
    "  <groupId>org.apache.sedona</groupId>\n",
    "  <artifactId>sedona-python-adapter-3.0_2.12</artifactId>\n",
    "  <version>1.4.1</version>\n",
    "</dependency>\n",
    "<dependency>\n",
    "    <groupId>org.datasyslab</groupId>\n",
    "    <artifactId>geotools-wrapper</artifactId>\n",
    "    <version>1.4.0-28.2</version>\n",
    "</dependency>\n",
    "</dependencies>\n",
    "```\n",
    "\n",
    "> You can notice we have much more jar to import.\n",
    "\n",
    "### 2.2 Use sedona in pyspark\n",
    "\n",
    "You can find the official doc [here](https://sedona.apache.org/latest-snapshot/setup/install-python/)\n",
    "\n",
    "To use sedona in pyspark we need to follow the below three steps:\n",
    "- Install apache-sedona in the target python virtual environments\n",
    "- Download the required jars(check the version dependencies).\n",
    "- Create the spark session with the required jar file\n",
    "\n",
    "\n",
    "#### 2.2.1 Install apache-sedona python package\n",
    "\n",
    "The official package page is [here](https://pypi.org/project/apache-sedona/). You can use below command to install the package\n",
    "\n",
    "```shell\n",
    "# simple install\n",
    "pip install apache-sedona\n",
    "\n",
    "# install sedona with pyspark as dependency\n",
    "# Since Sedona v1.1.0, pyspark is an optional dependency of Sedona Python because spark comes pre-installed on many spark platforms. \n",
    "# To install pyspark along with Sedona Python in one go, use the spark extra\n",
    "pip install apache-sedona[spark]\n",
    "\n",
    "# you need to check the version of apacke-sedona, because the jar version must be compatible \n",
    "pip show apache-sedona\n",
    "```\n",
    "\n",
    "> for example, if the python package version apache-sedona is 1.6.1, then the jar version must be 1.6.1 too.\n",
    "> \n",
    "\n",
    "#### 2.2.2 Download the required jars\n",
    "\n",
    "Before download, determine the right jar version is very important. You can find all required jar in the below urls:\n",
    " - sedona-jars: https://repo.maven.apache.org/maven2/org/apache/sedona/\n",
    " - geotools-wrapper-jars: https://repo.maven.apache.org/maven2/org/datasyslab/geotools-wrapper/1.6.1-28.2/\n",
    "\n",
    "I will show two examples of the shaded jar:\n",
    "  - `sedona-spark-shaded-3.0_2.12-1.4.1.jar`: this jar is built for spark 3.0 compile with scala 2.12. The sedona version is 1.4.1\n",
    "  - `sedona-spark-shaded-3.5_2.13-1.6.1.jar`: this jar is built for spark 3.5 compile with scala 2.13. The sedona version is 1.6.1\n",
    "  \n",
    "For the geotools jar:\n",
    "  - `geotools-wrapper-1.4.0-28.2.jar`: this jar is built for sedona version 1.4.0, the geotools version is 28.2\n",
    "  - `geotools-wrapper-1.6.1-28.2.jar`: this jar is built for sedona version 1.6.1, the geotools version is 28.2 \n",
    "\n",
    "#### 2.2.3 Create the spark session with the required jar file\n",
    "\n",
    "Import the sedona Jar files into your spark session. (Create sedona config)\n",
    "\n",
    "There are two ways to import Jar files into your spark session.\n",
    "\n",
    "1. Put the jar files directly into the $SPARK_HOME/jars/ (In cluster mode, make sure all the worker nodes also have the jar file in place)\n",
    "2. Ask spark session to download the jar file by using the `spark.jars.packages` config\n",
    "\n",
    "Check the below example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sedona.spark import *\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:21:42.836627Z",
     "start_time": "2024-11-12T09:21:42.830128Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# build a spark session with sedona (sedona < 1.4.1)\n",
    "# spark = SparkSession. \\\n",
    "#     builder. \\\n",
    "#     appName('appName'). \\\n",
    "#     config(\"spark.serializer\", KryoSerializer). \\\n",
    "#     config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "#     config('spark.jars.packages',\n",
    "#            'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n",
    "#            'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "#     getOrCreate()\n",
    "# SedonaRegistrator.registerAll(spark)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.1 Create sedona context from scratch\n",
    "\n",
    "The below example shows how to create a sedona context from scratch.\n",
    "\n",
    "We need extra jars for the sedona classes. To load these jars into the spark context, you can use the two below config:\n",
    "- \"spark.jars.packages\", \"package id\" : It will download the jar for both driver node and worker node. But you need to have internet connection\n",
    "- \"spark.jars\", \"jar path\": This config requires you to download jar manually on the local file system of the driver and worker node. You don't need internet connection anymore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# build a sedona session with internet\n",
    "config = SedonaContext.builder(). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-3.5_2.13:1.6.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.6.1-28.2'). \\\n",
    "    config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all'). \\\n",
    "    getOrCreate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:21:56.587833Z",
     "start_time": "2024-11-12T09:21:45.994227Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:19:01.574546Z",
     "start_time": "2024-11-12T09:18:58.038327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build a sedona session offline\n",
    "jar_folder = Path(r\"/home/pengfei/git/PySparkCommonFunc/jars\")\n",
    "jar_list = [\"sedona-spark-shaded-3.5_2.13-1.6.1.jar\",\"geotools-wrapper-1.6.1-28.2.jar\"]\n",
    "jar_path = \",\".join(jar_list)\n",
    "\n",
    "config = SedonaContext.builder(). \\\n",
    "    config('spark.jars', jar_path). \\\n",
    "    getOrCreate()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n"
  },
  {
   "cell_type": "code",
   "source": [
    "# create a sedona context\n",
    "sedona = SedonaContext.create(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:22:05.347235Z",
     "start_time": "2024-11-12T09:22:00.610342Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2.2 Create sedona context from an existing spark session\n",
    "\n",
    "In some case, you don't have the right to create a spark session from scratch (e.g. in Wherobots/AWS EMR/Databricks). The platform provides already a spark session.\n",
    "\n",
    "You can use below command to create a sedona context from an existing spark session.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "# suppose you have a spark session called spark\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('MySedona'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "     getOrCreate()\n",
    "\n",
    "# you can create a sedona context from it\n",
    "sedona = SedonaContext.create(spark)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Use sedona to read datasets\n",
    "\n",
    "Sedona provides a sedona SQL api, which allows you to read, transform and write geospatial data\n",
    "\n",
    "You can find the full doc [here](https://sedona.apache.org/1.4.1/api/sql/Overview/)\n",
    "\n",
    "In this tutorial, we will read a tsv(tab-separated values) file that represents some country coordinates (polygons) in the USA."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_folder_path = \"../../../../data/\"\n",
    "file_path=f\"{data_folder_path}/county_small.tsv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:22:10.529158Z",
     "start_time": "2024-11-12T09:22:10.519138Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "rawDf = sedona.read.format(\"csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"false\").load(file_path)\n",
    "rawDf.createOrReplaceTempView(\"rawdf\")\n",
    "rawDf.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:22:13.775661Z",
     "start_time": "2024-11-12T09:22:12.627296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n",
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|NULL| NULL|NULL|   A|1477895811|10447360|+41.9158651|-096.7885168|\n",
      "|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|NULL| NULL|NULL|   A| 682138871|61658258|+46.2946377|-123.4244583|\n",
      "|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|NULL| NULL|NULL|   A|6015539696|29159492|+34.3592729|-104.3686961|\n",
      "|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|NULL|   A|2169240202|22877180|+40.7835474|-096.6886584|\n",
      "|POLYGON ((-98.273...| 31|129|00835886|31129|   Nuckolls|     Nuckolls County| 06| H1|G4020|NULL| NULL|NULL|   A|1489645187| 1718484|+40.1764918|-098.0468422|\n",
      "|POLYGON ((-65.910...| 72|085|01804523|72085|Las Piedras|Las Piedras Munic...| 13| H1|G4020| 490|41980|NULL|   A|  87748363|   32509|+18.1871483|-065.8711890|\n",
      "|POLYGON ((-97.129...| 46|099|01265772|46099|  Minnehaha|    Minnehaha County| 06| H1|G4020|NULL|43620|NULL|   A|2090540341|17349847|+43.6674723|-096.7957261|\n",
      "|POLYGON ((-99.821...| 48|327|01383949|48327|     Menard|       Menard County| 06| H1|G4020|NULL| NULL|NULL|   A|2336245914|  613559|+30.8843655|-099.8539896|\n",
      "|POLYGON ((-120.65...| 06|091|00277310|06091|     Sierra|       Sierra County| 06| H1|G4020|NULL| NULL|NULL|   A|2468686374|23299110|+39.5769252|-120.5219926|\n",
      "|POLYGON ((-85.239...| 21|053|00516873|21053|    Clinton|      Clinton County| 06| H1|G4020|NULL| NULL|NULL|   A| 510864252|21164150|+36.7288647|-085.1534262|\n",
      "|POLYGON ((-83.880...| 39|063|01074044|39063|    Hancock|      Hancock County| 06| H1|G4020| 248|22300|NULL|   A|1376210232| 5959837|+41.0004711|-083.6660335|\n",
      "|POLYGON ((-102.08...| 48|189|01383880|48189|       Hale|         Hale County| 06| H1|G4020|NULL|38380|NULL|   A|2602115649|  246678|+34.0684364|-101.8228879|\n",
      "|POLYGON ((-85.978...| 01|027|00161539|01027|       Clay|         Clay County| 06| H1|G4020|NULL| NULL|NULL|   A|1564252367| 5284573|+33.2703999|-085.8635254|\n",
      "|POLYGON ((-101.62...| 48|011|01383791|48011|  Armstrong|    Armstrong County| 06| H1|G4020| 108|11100|NULL|   A|2354581764|12219587|+34.9641790|-101.3566363|\n",
      "|POLYGON ((-84.397...| 39|003|01074015|39003|      Allen|        Allen County| 06| H1|G4020| 338|30620|NULL|   A|1042470093|11266164|+40.7716274|-084.1061032|\n",
      "|POLYGON ((-82.449...| 13|189|00348794|13189|   McDuffie|     McDuffie County| 06| H1|G4020|NULL|12260|NULL|   A| 666816637|23116292|+33.4824637|-082.4731880|\n",
      "|POLYGON ((-90.191...| 55|111|01581115|55111|       Sauk|         Sauk County| 06| H1|G4020| 357|12660|NULL|   A|2152007753|45296336|+43.4279976|-089.9433290|\n",
      "|POLYGON ((-92.415...| 05|137|00069902|05137|      Stone|        Stone County| 06| H1|G4020|NULL| NULL|NULL|   A|1570579427| 7841929|+35.8570312|-092.1405728|\n",
      "|POLYGON ((-117.74...| 41|063|01155135|41063|    Wallowa|      Wallowa County| 06| H1|G4020|NULL| NULL|NULL|   A|8148602810|14199330|+45.5937530|-117.1855796|\n",
      "|POLYGON ((-80.518...| 42|007|01214112|42007|     Beaver|       Beaver County| 06| H1|G4020| 430|38300|NULL|   A|1125901160|24165972|+40.6841401|-080.3507209|\n",
      "+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "rawDf.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:22:14.905969Z",
     "start_time": "2024-11-12T09:22:14.893644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Create a Geometry type column\n",
    "\n",
    "You can notice the `_c0` column has type string, even-though its value is a polygon of GPS coordinates. So first step is to convert the string column into a `Geometry type column`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spatialDf=sedona.sql(\"select ST_GeomFromText(rawdf._c0) as county_shape, rawdf._c6 as county_name from rawdf\")\n",
    "spatialDf.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:22:17.431412Z",
     "start_time": "2024-11-12T09:22:16.881719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        county_shape|         county_name|\n",
      "+--------------------+--------------------+\n",
      "|POLYGON ((-97.019...|       Cuming County|\n",
      "|POLYGON ((-123.43...|    Wahkiakum County|\n",
      "|POLYGON ((-104.56...|      De Baca County|\n",
      "|POLYGON ((-96.910...|    Lancaster County|\n",
      "|POLYGON ((-98.273...|     Nuckolls County|\n",
      "|POLYGON ((-65.910...|Las Piedras Munic...|\n",
      "|POLYGON ((-97.129...|    Minnehaha County|\n",
      "|POLYGON ((-99.821...|       Menard County|\n",
      "|POLYGON ((-120.65...|       Sierra County|\n",
      "|POLYGON ((-85.239...|      Clinton County|\n",
      "|POLYGON ((-83.880...|      Hancock County|\n",
      "|POLYGON ((-102.08...|         Hale County|\n",
      "|POLYGON ((-85.978...|         Clay County|\n",
      "|POLYGON ((-101.62...|    Armstrong County|\n",
      "|POLYGON ((-84.397...|        Allen County|\n",
      "|POLYGON ((-82.449...|     McDuffie County|\n",
      "|POLYGON ((-90.191...|         Sauk County|\n",
      "|POLYGON ((-92.415...|        Stone County|\n",
      "|POLYGON ((-117.74...|      Wallowa County|\n",
      "|POLYGON ((-80.518...|       Beaver County|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "spatialDf.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:22:18.308699Z",
     "start_time": "2024-11-12T09:22:18.299749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- county_shape: geometry (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can notice the `county_shape` column has geometry type now."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Save the geo dataframe\n",
    "\n",
    "To save a Spatial DataFrame to some permanent storage such as Hive tables, S3 and HDFS, you can simply `convert each geometry in the Geometry type column back to a plain String` and save the plain DataFrame to wherever you want.\n",
    "\n",
    "Use the following code to convert the Geometry column in a DataFrame back to a WKT string column:\n",
    "\n",
    "```sql\n",
    "SELECT ST_AsText(county_shape) FROM rawdf\n",
    "```\n",
    "\n",
    "\n",
    "### 4.1 Save GeoParquet\n",
    "\n",
    "Since v1.3.0, Sedona natively supports writing GeoParquet file. GeoParquet can be saved as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_path = f\"{data_folder_path}/tmp/county_geo_parquet\"\n",
    "spatialDf.write.format(\"geoparquet\").save(output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-12T09:05:25.111868Z",
     "start_time": "2024-11-12T09:05:23.829041Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sedona/spark allow us to read and write many geospatial datasource, we won't show all of them in this tutorial. We will have a dedicated section for this\n",
    "\n",
    "> From the first check, you can notice the tsv file is 4MB, and geo parquet file is only 3MB. And the geo parquet already has geo-schema(e.g. point, line, polygon, etc.)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
