{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interoperate with GeoPandas\n",
    "\n",
    "**Sedona Python currently only works with Shapely 1.x. If you use GeoPandas, please use <= GeoPandas 0.11.1. GeoPandas > 0.11.1 will automatically install Shapely 2.0. If you use Shapely, please use <= 1.8.4.**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "import geopandas as gpd\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/10 17:30:05 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/07/10 17:30:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/pengfei/opt/spark-3.3.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pengfei/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pengfei/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-shaded-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-16abba46-a2b1-41b5-b9e7-ecb753b12423;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.4.0-28.2 in central\n",
      ":: resolution report :: resolve 236ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.sedona#sedona-spark-shaded-3.0_2.12;1.4.1 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.4.0-28.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-16abba46-a2b1-41b5-b9e7-ecb753b12423\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/9ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/10 17:30:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# build a sedona session (sedona >= 1.4.1)\n",
    "config = SedonaContext.builder(). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "# create a sedona context\n",
    "sedona = SedonaContext.create(config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 from GeoPandas to Sedona\n",
    "\n",
    "Let's convert a geo pandas dataframe to sedona(spark) dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# read shape file with geo pandas\n",
    "large_file_loc = \"/home/pengfei/data_set/australia_free_gis/gis_osm_pois_free_1.shp\"\n",
    "\n",
    "small_file_loc = \"/home/pengfei/data_set/small_shape_file_exp/gis_osm_pois_free_1.shp\"\n",
    "gdf = gpd.read_file(small_file_loc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GeoDataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_1274892/2680481290.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;31m# create a sedona dataframe with geopandas dataframe\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0msdf\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0msedona\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0msdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    887\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m             \u001B[0mhas_pandas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    889\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhas_pandas\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpandas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    890\u001B[0m             \u001B[0;31m# Create a DataFrame from pandas DataFrame.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 891\u001B[0;31m             return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n\u001B[0m\u001B[1;32m    892\u001B[0m                 \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    893\u001B[0m             )\n\u001B[1;32m    894\u001B[0m         return self._create_dataframe(\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    432\u001B[0m                         \u001B[0;34m\"has been set to false.\\n  %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m                     )\n\u001B[1;32m    434\u001B[0m                     \u001B[0mwarn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    435\u001B[0m                     \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 436\u001B[0;31m         \u001B[0mconverted_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_convert_from_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimezone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    437\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconverted_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, pdf, schema, timezone)\u001B[0m\n\u001B[1;32m    470\u001B[0m                                 \u001B[0mcopied\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    471\u001B[0m                             \u001B[0mpdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfield\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    472\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    473\u001B[0m                 \u001B[0mshould_localize\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_timestamp_ntz_preferred\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 474\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mcolumn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseries\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miteritems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    475\u001B[0m                     \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mseries\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    476\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mshould_localize\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mis_datetime64tz_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtz\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    477\u001B[0m                         \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_series_convert_timestamps_tz_local\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseries\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimezone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.cache/pypoetry/virtualenvs/sparkcommonfunc-jby-k8HJ-py3.8/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   5985\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_accessors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5986\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5987\u001B[0m         ):\n\u001B[1;32m   5988\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5989\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'GeoDataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "# create a sedona dataframe with geopandas dataframe\n",
    "sdf =sedona.createDataFrame(gdf)\n",
    "sdf.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
